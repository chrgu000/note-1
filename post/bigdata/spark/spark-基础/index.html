<!DOCTYPE html>
<html lang="zh-cn">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>spark-基础</title>
<meta name="description" content=" John Doe&#39;s Personal blog about everything">
<meta name="generator" content="Hugo 0.17" />
<meta property="og:title" content="spark-基础" />
<meta property="og:description" content="spark-基础1. Spark集群安装1.1. 安装1.1.1. 机器部署1.1.2. 下载Spark安装包1.1.3. 配置Spark1.2. 执行第一个spark程序1.3. 启动Spark Shell1.3.1. 启动spark shell1.3.2. 在spark shell中编写WordCount程序1.4. 在IDEA中编写WordCount程序1. Spark集群安装1.1. 安装1.1.1. 机器部署准备两台以上Linux服务器，安装好JDK1.7
1.1.2. 下载Spark安装包下载
http://www.apache.org/dyn/closer.lua/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz
上传解压安装包
上传 spark-1.5.2-bin-hadoop2.6.tgz 安装包到Linux上
解压安装包到指定位置
tar -zxvf spark-1.5.2-bin-hadoop2.6.tgz -C /usr/local1.1.3. 配置Spark进入到Spark安装目录" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/bigdata/spark/spark-%E5%9F%BA%E7%A1%80/" />


<meta property="og:updated_time" content="2017-04-08T18:42:52&#43;00:00"/>











<link rel="stylesheet" href="/css/google-font.css?family=Open+Sans:400,400italic,700,600" type="text/css" media="all" />
<link rel="stylesheet" href="/css/atom-one-dark.css">
<link rel="stylesheet" href="/css/style.css" type="text/css" media="all" />
<link rel="stylesheet" href="/css/custom.css" type="text/css" media="all" />
<link rel="stylesheet" href="/css/jquery.bigautocomplete.css" type="text/css" media="all" />
<link rel="stylesheet" href="/css/asciinema-player.css" type="text/css" media="all" />
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/fonts/fontawesome-webfont.svg" rel="stylesheet">

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/javascript" src="/js/scripts.js"></script>
<!--[if lt IE 9]>
	<script src="/js/css3-mediaqueries.js"></script>
<![endif]-->

</head>
<body id="mr-mobile" class="home blog mr-right-sb" itemscope="itemscope" itemtype="http://schema.org/WebPage">
	<div class="mr-container mr-container-outer">
		<div class="mr-header-mobile-nav clearfix"></div>
			<header class="mr-header" itemscope="itemscope" itemtype="http://schema.org/WPHeader">
				<div class="mr-container mr-container-inner mr-row clearfix">
					<div class="mr-custom-header clearfix">
						<div class="mr-site-identity">
							<div class="mr-site-logo" role="banner" itemscope="itemscope" itemtype="http://schema.org/Brand">
								<div class="mr-header-text">
									<a class="mr-header-text-link" href="/" title="零零散散" rel="home">
										<h1 class="mr-header-title">零零散散</h1>
										<h2 class="mr-header-tagline">点滴记录</h2>
									</a>
								</div>
							</div>
						</div>
					</div>
				</div>
				<div class="mr-main-nav-wrap">
					<nav class="menu" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
	<ul class="menu__list">
        
            <li class="menu__item"><a class="menu__link" href="/categories/docker">docker</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/hadoop">hadoop</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/scala">scala</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/spark">spark</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/storm">storm</a></li>
        
	</ul>
</nav>
				</div>
			</header>
		<div class="mr-wrapper clearfix">


<div class="mr-content" id="main-content" role="main" itemprop="mainContentOfPage">
	<article class="post">
		<header class="entry-header clearfix">
			<h1 class="entry-title">spark-基础</h1>
			<p class="mr-meta entry-meta">
				<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
				<time class="entry-meta-date updated" datetime="2017-04-08 18:42:52 &#43;0000 UTC">April 08, 2017</time>
				<span class="entry-meta-categories">
					<svg class="icon icon-category" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
					<a class="meta-categories" href="/categories/spark" rel="category">spark</a></span>
			</p>
		</header>
		<div class="entry-content clearfix">
			
			<div id="toc" class="toc">
<div id="toctitle">spark-基础</div>
<ul class="sectlevel1">
<li><a href="#_spark集群安装">1. Spark集群安装</a>
<ul class="sectlevel2">
<li><a href="#_安装">1.1. 安装</a>
<ul class="sectlevel3">
<li><a href="#_机器部署">1.1.1. 机器部署</a></li>
<li><a href="#_下载spark安装包">1.1.2. 下载Spark安装包</a></li>
<li><a href="#_配置spark">1.1.3. 配置Spark</a></li>
</ul>
</li>
<li><a href="#_执行第一个spark程序">1.2. 执行第一个spark程序</a></li>
<li><a href="#_启动spark_shell">1.3. 启动Spark Shell</a>
<ul class="sectlevel3">
<li><a href="#_启动spark_shell_2">1.3.1. 启动spark shell</a></li>
<li><a href="#_在spark_shell中编写wordcount程序">1.3.2. 在spark shell中编写WordCount程序</a></li>
</ul>
</li>
<li><a href="#_在idea中编写wordcount程序">1.4. 在IDEA中编写WordCount程序</a></li>
</ul>
</li>
</ul>
</div>
<div class="sect1">
<h2 id="_spark集群安装">1. Spark集群安装</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_安装">1.1. 安装</h3>
<div class="sect3">
<h4 id="_机器部署">1.1.1. 机器部署</h4>
<div class="paragraph">
<p>准备两台以上Linux服务器，安装好JDK1.7</p>
</div>
</div>
<div class="sect3">
<h4 id="_下载spark安装包">1.1.2. 下载Spark安装包</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>下载<br>
<a href="http://www.apache.org/dyn/closer.lua/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz" class="bare">http://www.apache.org/dyn/closer.lua/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz</a></p>
</li>
<li>
<p>上传解压安装包</p>
<div class="paragraph">
<p>上传 <strong>spark-1.5.2-bin-hadoop2.6.tgz</strong> 安装包到Linux上</p>
</div>
</li>
<li>
<p>解压安装包到指定位置</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>tar -zxvf spark-1.5.2-bin-hadoop2.6.tgz -C /usr/local</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_配置spark">1.1.3. 配置Spark</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>进入到Spark安装目录</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>cd /usr/local/spark-1.5.2-bin-hadoop2.6</code></pre>
</div>
</div>
</li>
<li>
<p>进入conf目录并重命名并修改spark-env.sh.template文件</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>cd conf/
mv spark-env.sh.template spark-env.sh
vi spark-env.sh</code></pre>
</div>
</div>
<div class="paragraph">
<p>在该配置文件中添加如下配置</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>export JAVA_HOME=/usr/java/jdk1.7.0_45
export SPARK_MASTER_IP=node1.itcast.cn
export SPARK_MASTER_PORT=7077</code></pre>
</div>
</div>
</li>
<li>
<p>重命名并修改slaves.template文件</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>mv slaves.template slaves
vi slaves
//在该文件中添加子节点所在的位置（Worker节点）
node2.itcast.cn
node3.itcast.cn
node4.itcast.cn</code></pre>
</div>
</div>
</li>
<li>
<p>将配置好的Spark拷贝到其他节点上</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>scp -r spark-1.5.2-bin-hadoop2.6/ node2:/usr/local/
scp -r spark-1.5.2-bin-hadoop2.6/ node3:/usr/local/
scp -r spark-1.5.2-bin-hadoop2.6/ node4:/usr/local/</code></pre>
</div>
</div>
</li>
<li>
<p>Spark集群配置完毕，目前是1个 <strong>Master</strong> ，3个 <strong>Work</strong> ，在 <strong>node1</strong> 上启动 <strong>Spark</strong> 集群</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>/usr/local/spark-1.5.2-bin-hadoop2.6/sbin/start-all.sh</code></pre>
</div>
</div>
</li>
<li>
<p>启动后执行jps命令</p>
<div class="listingblock">
<div class="content">
<pre>主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：
http://node1:8080/
到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：</pre>
</div>
</div>
</li>
<li>
<p>Spark集群规划</p>
<div class="listingblock">
<div class="content">
<pre>node1，node2是Master；node3，node4，node5是Worker
安装配置zk集群，并启动zk集群
停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark"</code></pre>
</div>
</div>
</li>
<li>
<p>在 <strong>node1</strong> 节点上修改 <strong>slaves</strong> 配置文件内容指定 <strong>worker</strong> 节点</p>
</li>
<li>
<p>在 <strong>node1</strong> 上执行 <strong>sbin/start-all.sh</strong> 脚本，然后在 <strong>node2</strong> 上执行 <strong>sbin/start-master.sh</strong> 启动第二个 <strong>Master</strong></p>
</li>
<li>
<p>执行Spark程序</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_执行第一个spark程序">1.2. 执行第一个spark程序</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://node1.itcast.cn:7077 \
--executor-memory 1G \
--total-executor-cores 2 \
/usr/local/spark-1.5.2-bin-hadoop2.6/lib/spark-examples-1.5.2-hadoop2.6.0.jar \
100</code></pre>
</div>
</div>
<div class="paragraph">
<p>该算法是利用蒙特·卡罗算法求PI</p>
</div>
</div>
<div class="sect2">
<h3 id="_启动spark_shell">1.3. 启动Spark Shell</h3>
<div class="paragraph">
<p>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。</p>
</div>
<div class="sect3">
<h4 id="_启动spark_shell_2">1.3.1. 启动spark shell</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-shell \
--master spark://node1.itcast.cn:7077 \ <i class="conum" data-value="1"></i><b>(1)</b>
--executor-memory 2g \ <i class="conum" data-value="2"></i><b>(2)</b>
--total-executor-cores 2 <i class="conum" data-value="3"></i><b>(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>master spark://node1.itcast.cn:7077 指定Master的地址</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>executor-memory 2g 指定每个worker可用内存为2G</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>total-executor-cores 2 指定整个集群使用的cup核数为2个</td>
</tr>
</table>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可</p>
</div>
</div>
<div class="sect3">
<h4 id="_在spark_shell中编写wordcount程序">1.3.2. 在spark shell中编写WordCount程序</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>首先启动hdfs</p>
</li>
<li>
<p>向hdfs上传一个文件到hdfs://node1.itcast.cn:9000/words.txt</p>
</li>
<li>
<p>在spark shell中用scala语言编写spark程序</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>sc.textFile("hdfs://node1.itcast.cn:9000/words.txt").flatMap(_.split(" "))
.map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://node1.itcast.cn:9000/out")</code></pre>
</div>
</div>
</li>
<li>
<p>使用hdfs命令查看结果</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>hdfs dfs -ls hdfs://node1.itcast.cn:9000/out/p*</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
 <strong>sc</strong> 是 <strong>SparkContext</strong> 对象，该对象时提交 <strong>spark</strong> 程序的入口
<strong>textFile(hdfs://node1.itcast.cn:9000/words.txt)</strong> 是hdfs中读取数据
<strong>flatMap(<em>.split(" "))</strong> 先map在压平
<strong>map</em>,1</strong> 将单词和1构成元组
<strong>reduceByKey(<em>+</em>)</strong> 按照 <strong>key</strong> 进行r <strong>educe</strong> ，并将v <strong>alue</strong> 累加
<strong>saveAsTextFile("hdfs://node1.itcast.cn:9000/out")</strong> 将结果写入到hdfs中
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_在idea中编写wordcount程序">1.4. 在IDEA中编写WordCount程序</h3>
<div class="paragraph">
<p>spark shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理jar包的依赖。</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>创建一个项目</p>
</li>
<li>
<p>选择Maven项目，然后点击next</p>
</li>
<li>
<p>填写maven的GAV，然后点击next</p>
</li>
<li>
<p>填写项目名称，然后点击finish</p>
</li>
<li>
<p>创建好maven项目后，点击Enable Auto-Import</p>
</li>
<li>
<p>配置Maven的pom.xml</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;cn.itcast.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-mvn&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;properties&gt;
        &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt;
        &lt;encoding&gt;UTF-8&lt;/encoding&gt;
        &lt;scala.version&gt;2.10.6&lt;/scala.version&gt;
        &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
            &lt;version&gt;${scala.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
            &lt;version&gt;1.5.2&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;
            &lt;version&gt;1.5.2&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;2.6.2&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;
        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.2.0&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;compile&lt;/goal&gt;
                            &lt;goal&gt;testCompile&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;args&gt;
                                &lt;arg&gt;-make:transitive&lt;/arg&gt;
                                &lt;arg&gt;-dependencyfile&lt;/arg&gt;
                                &lt;arg&gt;${project.build.directory}/.scala_dependencies&lt;/arg&gt;
                            &lt;/args&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.18.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;useFile&gt;false&lt;/useFile&gt;
                    &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt;
                    &lt;includes&gt;
                        &lt;include&gt;**/*Test.*&lt;/include&gt;
                        &lt;include&gt;**/*Suite.*&lt;/include&gt;
                    &lt;/includes&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.3&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;shade&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;filters&gt;
                                &lt;filter&gt;
                                    &lt;artifact&gt;*:*&lt;/artifact&gt;
                                    &lt;excludes&gt;
                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;
                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;
                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;
                                    &lt;/excludes&gt;
                                &lt;/filter&gt;
                            &lt;/filters&gt;
                            &lt;transformers&gt;
                                &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt;
                                    &lt;mainClass&gt;cn.itcast.spark.WordCount&lt;/mainClass&gt;
                                &lt;/transformer&gt;
                            &lt;/transformers&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;</code></pre>
</div>
</div>
</li>
<li>
<p>将src/main/java和src/test/java分别修改成src/main/scala和src/test/scala，与pom.xml中的配置保持一致</p>
</li>
<li>
<p>新建一个scala class，类型为Object</p>
</li>
<li>
<p>编写spark程序</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>package cn.itcast.spark

import org.apache.spark.{SparkContext, SparkConf}

object WordCount {
  def main(args: Array[String]) {
    //创建SparkConf()并设置App名称
    val conf = new SparkConf().setAppName("WC")
    //创建SparkContext，该对象是提交spark App的入口
    val sc = new SparkContext(conf)
    //使用sc创建RDD并执行相应的transformation和action
    sc.textFile(args(0)).flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_, 1).sortBy(_._2, false).saveAsTextFile(args(1))
    //停止sc，结束该任务
    sc.stop()
  }
}</code></pre>
</div>
</div>
</li>
<li>
<p>使用Maven打包：首先修改pom.xml中的main class</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>点击idea右侧的Maven Project选项</p>
</div>
<div class="paragraph">
<p>点击Lifecycle,选择clean和package，然后点击Run Maven Build</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>选择编译成功的jar包，并将该jar上传到Spark集群中的某个节点上</p>
</li>
<li>
<p>首先启动hdfs和Spark集群
启动hdfs</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>/usr/local/hadoop-2.6.1/sbin/start-dfs.sh</code></pre>
</div>
</div>
<div class="paragraph">
<p>启动spark</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>/usr/local/spark-1.5.2-bin-hadoop2.6/sbin/start-all.sh</code></pre>
</div>
</div>
</li>
<li>
<p>使用spark-submit命令提交Spark应用（注意参数的顺序）</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \
--class cn.itcast.spark.WordCount \
--master spark://node1.itcast.cn:7077 \
--executor-memory 2G \
--total-executor-cores 4 \
/root/spark-mvn-1.0-SNAPSHOT.jar \
hdfs://node1.itcast.cn:9000/words.txt \
hdfs://node1.itcast.cn:9000/out</code></pre>
</div>
</div>
</li>
<li>
<p>查看程序执行结果</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>hdfs dfs -cat hdfs://node1.itcast.cn:9000/out/part-00000
(hello,6)
(tom,3)
(kitty,2)
(jerry,1)</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>

		</div>
		
	</article>
	
<div class="mr-author-box clearfix">
	<figure class="mr-author-box-avatar">
		<img alt="dishui avatar" src="/src/img/dishui.png" class="avatar avatar-90 photo" height="90" width="90">
	</figure>
	<div class="mr-author-box-header">
		<span class="mr-author-box-name">About dishui</span>
	</div>
	<div class="mr-author-box-bio">
		辛勤的搬运工!!!
	</div>
</div>

	

	<nav class="mr-post-nav mr-row clearfix" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
		
		<div class="mr-col-1-2 mr-post-nav-item mr-post-nav-prev">
			<a href="/post/bigdata/spark/spark-rdd/" rel="prev"><span>«Previous</span><p>spark-rdd</p></a>
		</div>
		
		
		<div class="mr-col-1-2 mr-post-nav-item mr-post-nav-next">
			<a href="/post/bigdata/spark/spark-%E5%8F%82%E8%80%83/" rel="next"><span>Next»</span><p>spark参考</p></a>
		</div>
		
	</nav>


	
</div>

<aside class="mr-sidebar" itemscope="itemscope" itemtype="http://schema.org/WPSideBar">
<div class="mr-widget widget_search navbar-wrapper" >
    <form class="search-form" role="search" >
        <label>
            <span class="screen-reader-text">Search for:</span>
            <input id="lanren" type="search" class="search-field" placeholder="Search..." value="" name="q">
        </label>
        <div id="list-container" class="bdsug" style="height: auto; display: block;">
        </div>
    </form>
    <div id="side-toc" class="entry-content">
    
    </div>
</div>
</aside>
	</div>
		<div class="mr-copyright-wrap">
			<div class="mr-container mr-container-inner clearfix">
				<p class="mr-copyright">&copy; 2017 零零散散. <a href="https://git.oschina.net/dishui/dishui" target="_blank" rel="nofollow noopener noreferrer">dishui</a>.</p>
			</div>
		</div>
	</div>

<script>
	var navigation = responsiveNav(".menu", {
		navClass: "menu--collapse",
	});
</script>
<script src="/js/asciinema-player.js"></script>
<script data-main="/js/app.js" src="/js/require.js"></script>


</body>
</html>