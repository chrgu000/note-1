a1.sources = r1
a1.channels = c1
a1.sinks = k1


a1.sources.r1.type = TAILDIR
a1.sources.r1.channels = c1
a1.sources.r1.positionFile = ${FLUME_POSITION_FILE}
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = ${FLUME_SOURCE_FILE}
a1.sources.r1.headers.f1.headerKey1 = value1
#a1.sources.r1.filegroups.f2 = /var/log/test2/.*log.*
#a1.sources.r1.headers.f2.headerKey1 = value2
#a1.sources.r1.headers.f2.headerKey2 = value2-2
a1.sources.r1.fileHeader = true

a1.sources.r1.interceptors = i2
a1.sources.r1.interceptors.i2.type = io.dishui.inceptor.KafkaInceptor$KafkaIterceptorBuilder


a1.channels.c1.type = file
a1.channels.c1.checkpointDir = ${FLUME_CHECKPOINTDIR}
a1.channels.c1.dataDirs = ${FLUME_DATADIRS}
a1.channels.c1.maxFileSize = 104857600
a1.channels.c1.capacity = 90000000
a1.channels.c1.keep-alive = 60


#a1.sinks.k1.type = logger
#a1.sinks.k1.channel = c1



a1.sinks.k1.channel = c1
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic = testtopic
a1.sinks.k1.kafka.bootstrap.servers = ${FLUME_KAFKA_BROKERLIST}
a1.sinks.k1.kafka.flumeBatchSize = 1
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1
a1.sinks.k1.kafka.producer.compression.type = snappy