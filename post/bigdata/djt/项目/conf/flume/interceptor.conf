a1.sources = r1
a1.channels = ch1
a1.sinks = k1

a1.sources.r1.channels = ch1
#a1.sources.r1.type = exec
#a1.sources.r1.command = tail -F /home/hadoop/flume/nginxlog/t/1.txt

a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /home/hadoop/flume/nginxlog
a1.sources.r1.ignorePattern = event(_\d{4}\-\d{2}\-\d{2}_\d{2}_\d{2})?\.log(\.COMPLETED)?
a1.sources.r1.deserializer.maxLineLength = 10240

a1.sources.r1.fileHeader = true
a1.sources.r1.basenameHeader = true
a1.sources.r1.basenameHeaderKey = basename

a1.sources.r1.interceptors = i1 i2
a1.sources.r1.interceptors.i1.type = host
a1.sources.r1.interceptors.i1.hostHeader = hostname
a1.sources.r1.interceptors.i2.type = com.djt.flume.iterceptor.SetDayIterceptor$SetDayIterceptorBuilder

a1.channels.ch1.type = file
a1.channels.ch1.checkpointDir = /home/hadoop/flume/flume-collect/checkpoint
a1.channels.ch1.dataDirs = /home/hadoop/flume/flume-collect/dataDir


a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = ch1
a1.sinks.k1.hdfs.path = /flume/events/%{year}-%{month}-%{day}
a1.sinks.k1.hdfs.filePrefix = %{hostname}
a1.sinks.k1.hdfs.inUseSuffix = .tmp
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.rollInterval = 0
a1.sinks.k1.hdfs.rollCount = 0
#128M 滚动一个文件
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.idleTimeout = 900
a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.fileType = SequenceFile



#a1.sinks.k1.type = logger
#a1.sinks.k1.channel = ch1
