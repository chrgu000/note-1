<!DOCTYPE html>
<html lang="zh-cn">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>hive详解</title>
<meta name="description" content=" John Doe&#39;s Personal blog about everything">
<meta name="generator" content="Hugo 0.17" />
<meta property="og:title" content="hive详解" />
<meta property="og:description" content="hive详解1. Hive简介1.1. 什么是Hive1.2. 为什么使用Hive1.3. Hive的特点2. Hive 架构2.1. 基本组成2.2. 各组件的基本功能2.3. Hive与Hadoop的关系2.4. Hive与传统数据库对比2.5. 对比2.6. Hive的数据存储2.7. HIVE 的安装部署2.7.1. 使用方式3. Hive基本操作3.1. DDL操作3.1.1. 创建表具体实例3.1.2. 修改表增加/删除分区重命名表增加/更新列显示命令3.1.3. DML操作LoadInsertSELECT3.1.4. Hive Join4. Hive Shell参数4.1. Hive命令行4." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/bigdata/hadoop/hive%E8%AF%A6%E8%A7%A3/" />


<meta property="og:updated_time" content="2017-03-17T00:00:00&#43;00:00"/>











<link rel="stylesheet" href="/css/google-font.css?family=Open+Sans:400,400italic,700,600" type="text/css" media="all" />
<link rel="stylesheet" href="/css/atom-one-dark.css">
<link rel="stylesheet" href="/css/style.css" type="text/css" media="all" />
<link rel="stylesheet" href="/css/custom.css" type="text/css" media="all" />
<link rel="stylesheet" href="/css/jquery.bigautocomplete.css" type="text/css" media="all" />
<link rel="stylesheet" href="/css/asciinema-player.css" type="text/css" media="all" />
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/fonts/fontawesome-webfont.svg" rel="stylesheet">

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/javascript" src="/js/scripts.js"></script>
<!--[if lt IE 9]>
	<script src="/js/css3-mediaqueries.js"></script>
<![endif]-->

</head>
<body id="mr-mobile" class="home blog mr-right-sb" itemscope="itemscope" itemtype="http://schema.org/WebPage">
	<div class="mr-container mr-container-outer">
		<div class="mr-header-mobile-nav clearfix"></div>
			<header class="mr-header" itemscope="itemscope" itemtype="http://schema.org/WPHeader">
				<div class="mr-container mr-container-inner mr-row clearfix">
					<div class="mr-custom-header clearfix">
						<div class="mr-site-identity">
							<div class="mr-site-logo" role="banner" itemscope="itemscope" itemtype="http://schema.org/Brand">
								<div class="mr-header-text">
									<a class="mr-header-text-link" href="/" title="零零散散" rel="home">
										<h1 class="mr-header-title">零零散散</h1>
										<h2 class="mr-header-tagline">点滴记录</h2>
									</a>
								</div>
							</div>
						</div>
					</div>
				</div>
				<div class="mr-main-nav-wrap">
					<nav class="menu" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
	<ul class="menu__list">
        
            <li class="menu__item"><a class="menu__link" href="/categories/docker">docker</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/hadoop">hadoop</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/scala">scala</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/spark">spark</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/storm">storm</a></li>
        
	</ul>
</nav>
				</div>
			</header>
		<div class="mr-wrapper clearfix">


<div class="mr-content" id="main-content" role="main" itemprop="mainContentOfPage">
	<article class="post">
		<header class="entry-header clearfix">
			<h1 class="entry-title">hive详解</h1>
			<p class="mr-meta entry-meta">
				<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
				<time class="entry-meta-date updated" datetime="2017-03-17 00:00:00 &#43;0000 UTC">March 17, 2017</time>
				<span class="entry-meta-categories">
					<svg class="icon icon-category" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
					<a class="meta-categories" href="/categories/hadoop" rel="category">hadoop</a></span>
			</p>
		</header>
		<div class="entry-content clearfix">
			
			<div id="toc" class="toc">
<div id="toctitle">hive详解</div>
<ul class="sectlevel1">
<li><a href="#_hive简介">1. Hive简介</a>
<ul class="sectlevel2">
<li><a href="#_什么是hive">1.1. 什么是Hive</a></li>
<li><a href="#_为什么使用hive">1.2. 为什么使用Hive</a></li>
<li><a href="#_hive的特点">1.3. Hive的特点</a></li>
</ul>
</li>
<li><a href="#__strong_hive_strong_架构">2. <strong>Hive</strong> 架构</a>
<ul class="sectlevel2">
<li><a href="#_基本组成">2.1. 基本组成</a></li>
<li><a href="#_各组件的基本功能">2.2. 各组件的基本功能</a></li>
<li><a href="#_hive与hadoop的关系">2.3. Hive与Hadoop的关系</a></li>
<li><a href="#_hive与传统数据库对比">2.4. Hive与传统数据库对比</a></li>
<li><a href="#_对比">2.5. 对比</a></li>
<li><a href="#_hive的数据存储">2.6. Hive的数据存储</a></li>
<li><a href="#__strong_hive_strong_的安装部署">2.7. <strong>HIVE</strong> 的安装部署</a>
<ul class="sectlevel3">
<li><a href="#_使用方式">2.7.1. 使用方式</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_hive基本操作">3. Hive基本操作</a>
<ul class="sectlevel2">
<li><a href="#_ddl操作">3.1. DDL操作</a>
<ul class="sectlevel3">
<li><a href="#_创建表">3.1.1. 创建表</a>
<ul class="sectlevel4">
<li><a href="#_具体实例">具体实例</a></li>
</ul>
</li>
<li><a href="#_修改表">3.1.2. 修改表</a>
<ul class="sectlevel4">
<li><a href="#_增加_删除分区">增加/删除分区</a></li>
<li><a href="#_重命名表">重命名表</a></li>
<li><a href="#_增加_更新列">增加/更新列</a></li>
<li><a href="#_显示命令">显示命令</a></li>
</ul>
</li>
<li><a href="#_dml操作">3.1.3. DML操作</a>
<ul class="sectlevel4">
<li><a href="#_load">Load</a></li>
<li><a href="#_insert">Insert</a></li>
<li><a href="#_select">SELECT</a></li>
</ul>
</li>
<li><a href="#_hive_join">3.1.4. Hive Join</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_hive_shell参数">4. Hive Shell参数</a>
<ul class="sectlevel2">
<li><a href="#_hive命令行">4.1. Hive命令行</a></li>
<li><a href="#_hive参数配置方式">4.2. Hive参数配置方式</a></li>
</ul>
</li>
<li><a href="#_hive函数">5. Hive函数</a>
<ul class="sectlevel2">
<li><a href="#_内置运算符">5.1. 内置运算符</a></li>
<li><a href="#_内置函数">5.2. 内置函数</a></li>
<li><a href="#_hive自定义函数和transform">5.3. Hive自定义函数和Transform</a></li>
<li><a href="#_自定义函数类别">5.4. 自定义函数类别</a></li>
<li><a href="#_udf开发实例">5.5. UDF开发实例</a></li>
<li><a href="#_transform实现">5.6. Transform实现</a></li>
</ul>
</li>
<li><a href="#_hive实战">6. Hive实战</a>
<ul class="sectlevel2">
<li><a href="#_hive_实战案例1_数据etl">6.1. Hive 实战案例1——数据ETL</a></li>
<li><a href="#_实战案例2_访问时长统计">6.2. 实战案例2——访问时长统计</a></li>
<li><a href="#_实战案例3_级联求和">6.3. 实战案例3——级联求和</a></li>
</ul>
</li>
</ul>
</div>
<div class="sect1">
<h2 id="_hive简介">1. Hive简介</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_什么是hive">1.1. 什么是Hive</h3>
<div class="paragraph">
<p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。</p>
</div>
</div>
<div class="sect2">
<h3 id="_为什么使用hive">1.2. 为什么使用Hive</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">直接使用hadoop所面临的问题 </dt>
<dd>
<p>人员学习成本太高<br>
项目周期要求太短<br>
MapReduce实现复杂查询逻辑开发难度太大</p>
</dd>
<dt class="hdlist1">为什么要使用Hive </dt>
<dd>
<p>避免了去写MapReduce，减少开发人员的学习成本。+
扩展功能很方便。</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_hive的特点">1.3. Hive的特点</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">可扩展 </dt>
<dd>
<p>Hive可以自由的扩展集群的规模，一般情况下不需要重启服务。</p>
</dd>
<dt class="hdlist1">延展性 </dt>
<dd>
<p>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</p>
</dd>
<dt class="hdlist1">容错 </dt>
<dd>
<p>良好的容错性，节点出现问题SQL仍可完成执行</p>
</dd>
</dl>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="__strong_hive_strong_架构">2. <strong>Hive</strong> 架构</h2>
<div class="sectionbody">
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/2017-03-17_155621.png" alt="2017 03 17 155621">
</div>
</div>
<hr>
<div class="dlist">
<dl>
<dt class="hdlist1">Jobtracker </dt>
<dd>
<p>hadoop1.x中的组件，它的功能相当于： Resourcemanager+AppMaster</p>
</dd>
<dt class="hdlist1">TaskTracker </dt>
<dd>
<p>相当于：  Nodemanager  +  yarnchild</p>
</dd>
</dl>
</div>
<div class="sect2">
<h3 id="_基本组成">2.1. 基本组成</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">用户接口</dt>
<dd>
<p>包括 CLI、JDBC/ODBC、WebGUI。</p>
</dd>
<dt class="hdlist1">元数据存储</dt>
<dd>
<p>通常是存储在关系数据库如 mysql,derby中。</p>
</dd>
<dt class="hdlist1">解释器、编译器、优化器、执行器</dt>
<dd>
<p>-</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_各组件的基本功能">2.2. 各组件的基本功能</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">用户接口主要由三个</dt>
<dd>
<p>CLI、JDBC/ODBC和WebGUI。其中，CLI为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。</p>
</dd>
<dt class="hdlist1">元数据存储 </dt>
<dd>
<p>Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</p>
</dd>
<dt class="hdlist1">解释器、编译器、优化器 </dt>
<dd>
<p>完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行。</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_hive与hadoop的关系">2.3. Hive与Hadoop的关系</h3>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>Hive利用HDFS存储数据，利用MapReduce查询数据</p>
</div>
</blockquote>
</div>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/2017-03-17_160404.png" alt="2017 03 17 160404">
</div>
</div>
<hr>
</div>
<div class="sect2">
<h3 id="_hive与传统数据库对比">2.4. Hive与传统数据库对比</h3>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/2017-03-17_160454.png" alt="2017 03 17 160454">
</div>
</div>
<hr>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析</p>
</div>
</blockquote>
</div>
</div>
<div class="sect2">
<h3 id="_对比">2.5. 对比</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>查询语言。由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。</p>
</li>
<li>
<p>数据存储位置。Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</p>
</li>
<li>
<p>数据格式。Hive 中没有定义专门的数据格式，数据格式可以由用户指定，用户定义数据格式需要指定三个属性：列分隔符（通常为空格、”\t”、”\x001″）、行分隔符（”\n”）以及读取文件数据的方法（Hive 中默认有三个文件格式 TextFile，SequenceFile 以及 RCFile）。由于在加载数据的过程中，不需要从用户数据格式到 Hive 定义的数据格式的转换，因此，Hive 在加载的过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的 HDFS 目录中。而在数据库中，不同的数据库有不同的存储引擎，定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。</p>
</li>
<li>
<p>数据更新。由于 Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive 中不支持对数据的改写和添加，所有的数据都是在加载的时候中确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO &#8230;&#8203;  VALUES 添加数据，使用 UPDATE &#8230;&#8203; SET 修改数据。</p>
</li>
<li>
<p>索引。之前已经说过，Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 Key</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_hive的数据存储">2.6. Hive的数据存储</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Hive</strong> 中所有的数据都存储在 <strong>HDFS</strong>  中，没有专门的数据存储格式（可支持*Text* ，<strong>SequenceFile</strong> ，<strong>ParquetFile</strong> ，<strong>RCFILE</strong> 等）</p>
</li>
<li>
<p>只需要在创建表的时候告诉 <strong>Hive</strong>  数据中的列分隔符和行分隔符，<strong>Hive</strong>  就可以解析数据。</p>
</li>
<li>
<p><strong>Hive</strong>  中包含以下数据模型：<strong>DB</strong> 、<strong>Table</strong> ，<strong>External</strong>  <strong>Table</strong> ，<strong>Partition</strong> ，<strong>Bucket</strong> 。</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p><strong>db</strong> ：在 <strong>hdfs</strong> 中表现为${<strong>hive</strong> .<strong>metastore</strong> .<strong>warehouse</strong> .<strong>dir</strong> }目录下一个文件夹</p>
</li>
<li>
<p><strong>table</strong> ：在 <strong>hdfs</strong> 中表现所属*db* 目录下一个文件夹</p>
</li>
<li>
<p><strong>external table</strong> ：外部表, 与*table* 类似，不过其数据存放位置可以在任意指定路径
普通表: 删除表后, <strong>hdfs</strong> 上的文件都删了</p>
</li>
<li>
<p><strong>External</strong> 外部表删除后, <strong>hdfs</strong> 上的文件没有删除, 只是把文件删除了</p>
</li>
<li>
<p><strong>partition</strong> ：在 <strong>hdfs</strong> 中表现为 <strong>table</strong> 目录下的子目录</p>
</li>
<li>
<p><strong>bucket</strong> ：桶, 在 <strong>hdfs</strong> 中表现为同一个表目录下根据 <strong>hash</strong> 散列之后的多个文件, 会根据不同的文件把数据放到不同的文件中</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="__strong_hive_strong_的安装部署">2.7. <strong>HIVE</strong> 的安装部署</h3>
<div class="paragraph">
<p><a href="/post/bigdata/hadoop/hive">hive 安装</a></p>
</div>
<div class="sect3">
<h4 id="_使用方式">2.7.1. 使用方式</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Hive交互shell</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>bin/hive</code></pre>
</div>
</div>
</li>
<li>
<p>Hive thrift服务</p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>启动方式，（假如是在hadoop01上）</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">启动为前台 </dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>bin/hiveserver2</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">启动为后台 </dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>nohup bin/hiveserver2 1&gt;/var/log/hiveserver.log 2&gt;/var/log/hiveserver.err &amp;</code></pre>
</div>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>启动成功后，可以在别的节点上用beeline去连接</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">方式（1）</dt>
<dd>
<p><code>hive/bin/beeline</code>  回车，进入beeline的命令界面<br>
输入命令连接 <code>hiveserver2</code></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>beeline&gt; !connect jdbc:hive2//mini1:10000</code></pre>
</div>
</div>
<div class="paragraph">
<p>（hadoop01是hiveserver2所启动的那台主机名，端口默认是10000）</p>
</div>
</dd>
<dt class="hdlist1">方式（2） </dt>
<dd>
<p>或者启动就连接：</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>bin/beeline -u jdbc:hive2://mini1:10000 -n hadoop</code></pre>
</div>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>接下来就可以做正常sql查询了</p>
</div>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_hive基本操作">3. Hive基本操作</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_ddl操作">3.1. DDL操作</h3>
<div class="sect3">
<h4 id="_创建表">3.1.1. 创建表</h4>
<div class="dlist">
<dl>
<dt class="hdlist1">建表语法</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name
   [(col_name data_type [COMMENT col_comment], ...)]
   [COMMENT table_comment]
   [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
   [CLUSTERED BY (col_name, col_name, ...)
   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
   [ROW FORMAT row_format]
   [STORED AS file_format]
   [LOCATION hdfs_path]</code></pre>
</div>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>说明：</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>CREATE</strong> <strong>TABLE</strong> 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 <strong>IF</strong> <strong>NOT</strong> <strong>EXISTS</strong> 选项来忽略这个异常。</p>
</li>
<li>
<p><strong>EXTERNAL</strong> 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（<strong>LOCATION</strong>），<strong>Hive</strong> 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p>
</li>
<li>
<p><strong>LIKE</strong> 允许用户复制现有的表结构，但是不复制数据。</p>
</li>
<li>
<p><strong>ROW</strong> <strong>FORMAT</strong></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</code></pre>
</div>
</div>
<div class="paragraph">
<p>用户在建表的时候可以自定义 <strong>SerDe</strong> 或者使用自带的 <strong>SerDe</strong>。如果没有指定 <strong>ROW</strong> <strong>FORMAT</strong> 或者 <strong>ROW</strong> <strong>FORMAT</strong> <strong>DELIMITED</strong>，将会使用自带的 <strong>SerDe</strong>。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 <strong>SerDe</strong>，<strong>Hive*通过 *SerDe</strong> 确定表的具体的列的数据。</p>
</div>
</li>
<li>
<p><strong>STORED</strong> <strong>AS</strong>
<strong>SEQUENCEFILE</strong>|<strong>TEXTFILE</strong>|<strong>RCFILE</strong>
如果文件数据是纯文本，可以使用 <strong>STORED</strong> <strong>AS</strong> <strong>TEXTFILE</strong>。如果数据需要压缩，使用 <strong>STORED</strong> <strong>AS</strong> <strong>SEQUENCEFILE</strong>。</p>
</li>
<li>
<p><strong>CLUSTERED</strong> <strong>BY</strong>
对于每一个表（<strong>table</strong>）或者分区， <strong>Hive</strong> 可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。<strong>Hive</strong> 也是 针对某一列进行桶的组织。<strong>Hive</strong> 采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。
把表（或者分区）组织成桶（<strong>Bucket</strong>）有两个理由：</p>
<div class="openblock">
<div class="content">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>获得更高的查询处理效率。桶为表加上了额外的结构，<strong>Hive</strong> 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 <strong>Map</strong> 端连接 （<strong>Map</strong>-<strong>side</strong> <strong>join</strong>）高效的实现。比如 <strong>JOIN</strong> 操作。对于 <strong>JOIN</strong> 操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行 <strong>JOIN</strong> 操作就可以，可以大大较少 <strong>JOIN</strong> 的数据量。</p>
</li>
<li>
<p>使取样（<strong>sampling</strong>）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。</p>
</li>
</ol>
</div>
</div>
</div>
</li>
</ol>
</div>
<div class="sect4">
<h5 id="_具体实例">具体实例</h5>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>创建内部表mytable</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image010.png" alt="image010">
</div>
</div>
<hr>
</li>
<li>
<p>创建外部表pageview</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image012.png" alt="image012">
</div>
</div>
<hr>
</li>
<li>
<p>创建分区表invites</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>create table student_p(Sno int,Sname string,Sex string,Sage int,Sdept string) partitioned by(part string) row format delimited fields terminated by ','stored as textfile;</code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image014.png" alt="image014">
</div>
</div>
<hr>
</li>
<li>
<p>创建带桶的表student</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image016.png" alt="image016">
</div>
</div>
<hr>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_修改表">3.1.2. 修改表</h4>
<div class="sect4">
<h5 id="_增加_删除分区">增加/删除分区</h5>
<div class="dlist">
<dl>
<dt class="hdlist1">语法结构</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION 'location1' ] partition_spec [ LOCATION 'location2' ] ...
partition_spec:
: PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)

ALTER TABLE table_name DROP partition_spec, partition_spec,...</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">具体实例</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>alter table student_p add partition(part='a') partition(part='b');</code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image018.png" alt="image018">
</div>
</div>
<hr>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image020.png" alt="image020">
</div>
</div>
<hr>
</dd>
</dl>
</div>
</div>
<div class="sect4">
<h5 id="_重命名表">重命名表</h5>
<div class="dlist">
<dl>
<dt class="hdlist1">语法结构</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>ALTER TABLE table_name RENAME TO new_table_name</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">具体实例</dt>
<dd>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image022.png" alt="image022">
</div>
</div>
<hr>
</dd>
</dl>
</div>
</div>
<div class="sect4">
<h5 id="_增加_更新列">增加/更新列</h5>
<div class="dlist">
<dl>
<dt class="hdlist1">语法结构</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</code></pre>
</div>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</p>
</div>
</blockquote>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">具体实例</dt>
<dd>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image024.png" alt="image024">
</div>
</div>
<hr>
</dd>
</dl>
</div>
</div>
<div class="sect4">
<h5 id="_显示命令">显示命令</h5>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>show tables
show databases
show partitions
show functions
desc extended t_name;
desc formatted table_name;</code></pre>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_dml操作">3.1.3. DML操作</h4>
<div class="sect4">
<h5 id="_load">Load</h5>
<div class="dlist">
<dl>
<dt class="hdlist1">语法结构</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO
TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">说明</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。</p>
</li>
<li>
<p>filepath</p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>相对路径，例如：<strong>project/data1</strong><br>
绝对路径，例如：<strong>/user/hive/project/data1</strong><br>
包含模式的完整 URI，列如：+
<strong>hdfs://namenode:9000/user/hive/project/data1</strong></p>
</div>
</div>
</div>
</li>
<li>
<p>LOCAL关键字</p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。<br>
如果没有指定 LOCAL 关键字，则根据inpath中的uri 查找文件</p>
</div>
</div>
</div>
</li>
<li>
<p>OVERWRITE 关键字</p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。<br>
如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。</p>
</div>
</div>
</div>
</li>
</ol>
</div>
</dd>
<dt class="hdlist1">具体实例</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>加载相对路径数据</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image026.png" alt="image026">
</div>
</div>
<hr>
</li>
<li>
<p>加载绝对路径数据</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image028.png" alt="image028">
</div>
</div>
<hr>
</li>
<li>
<p>加载包含模式数据</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image030.png" alt="image030">
</div>
</div>
<hr>
</li>
<li>
<p>OVERWRITE关键字使用</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image032.png" alt="image032">
</div>
</div>
<hr>
</li>
</ol>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect4">
<h5 id="_insert">Insert</h5>
<div class="dlist">
<dl>
<dt class="hdlist1">语法结构</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement

Multiple inserts:
FROM from_statement
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1
[INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...

Dynamic partition inserts:
INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">具体实例</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>基本模式插入</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image034.png" alt="image034">
</div>
</div>
<hr>
</li>
<li>
<p>多插入模式</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image036.png" alt="image036">
</div>
</div>
<hr>
</li>
<li>
<p>自动分区模式</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image038.png" alt="image038">
</div>
</div>
<hr>
</li>
</ol>
</div>
</dd>
</dl>
</div>
<div class="sect5">
<h6 id="_导出表数据">导出表数据</h6>
<div class="dlist">
<dl>
<dt class="hdlist1">语法结构</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...


multiple inserts:
FROM from_statement
INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1
[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ...</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">具体实例</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>导出文件到本地</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image040.png" alt="image040">
</div>
</div>
<hr>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>说明：
数据写入到文件系统时进行文本序列化，且每列用^A来区分，\n为换行符。用more命令查看时不容易看出分割符，可以使用:sed -e 's/\x01/|/g' filename 来查看。</p>
</div>
</blockquote>
</div>
</li>
<li>
<p>导出数据到HDFS</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image042.png" alt="image042">
</div>
</div>
<hr>
</li>
</ol>
</div>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect4">
<h5 id="_select">SELECT</h5>
<div class="dlist">
<dl>
<dt class="hdlist1">语法结构</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list [HAVING condition]]
[CLUSTER BY col_list
  | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]
]
[LIMIT number]</code></pre>
</div>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>注：
. order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。
. sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。
. distribute by根据distribute by指定的内容将数据分到同一个reducer。
. Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">具体实例</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>获取年龄大的3个学生</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image044.png" alt="image044">
</div>
</div>
<hr>
</li>
<li>
<p>查询学生信息按年龄，降序排序</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image046.png" alt="image046">
</div>
</div>
<hr>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image048.png" alt="image048">
</div>
</div>
<hr>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image050.png" alt="image050">
</div>
</div>
<hr>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image052.png" alt="image052">
</div>
</div>
<hr>
</li>
<li>
<p>按学生名称汇总学生年龄</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image054.png" alt="image054">
</div>
</div>
<hr>
</li>
</ol>
</div>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_hive_join">3.1.4. Hive Join</h4>
<div class="dlist">
<dl>
<dt class="hdlist1">语法结构</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>join_table:
  table_reference JOIN table_factor [join_condition]
  | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition
  | table_reference LEFT SEMI JOIN table_reference join_condition</code></pre>
</div>
</div>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>Hive 支持等值连接（<strong>equality joins</strong>）、外连接（<strong>outer joins</strong>）和（<strong>left/right joins</strong>）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。
另外，Hive 支持多于 2 个表的连接。</p>
</div>
</div>
</div>
<div class="paragraph">
<p>写 join 查询时，需要注意几个关键点：</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>只支持等值join
例如：</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>  SELECT a.* FROM a JOIN b ON (a.id = b.id)
  SELECT a.* FROM a JOIN b
    ON (a.id = b.id AND a.department = b.department)
是正确的，然而:
  SELECT a.* FROM a JOIN b ON (a.id&gt;b.id)
是错误的。</code></pre>
</div>
</div>
</li>
<li>
<p>可以 join 多于 2 个表。</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>例如
  SELECT a.val, b.val, c.val FROM a JOIN b
    ON (a.key = b.key1) JOIN c ON (c.key = b.key2)
如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，例如：
  SELECT a.val, b.val, c.val FROM a JOIN b
    ON (a.key = b.key1) JOIN c
    ON (c.key = b.key1)
被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key。
SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1)
  JOIN c ON (c.key = b.key2)
而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 join。</code></pre>
</div>
</div>
</li>
<li>
<p>join 时</p>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p>每次 map/reduce 任务的逻辑
reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如：</p>
</div>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>SELECT a.val, b.val, c.val FROM a
    JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>所有表都使用同一个 <strong>join key</strong>（使用 1 次 <strong>map/reduce</strong> 任务计算）。
<strong>Reduce</strong> 端会缓存 <strong>a</strong> 表和 <strong>b</strong> 表的记录，然后每次取得一个 c 表的记录就计算一次 <strong>join</strong> 结果，类似的还有：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>  SELECT a.val, b.val, c.val FROM a
    JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>这里用了 2 次 <strong>map/reduce</strong> 任务。第一次缓存 <strong>a</strong> 表，用 <strong>b</strong> 表序列化；第二次缓存第一次 <strong>map/reduce</strong> 任务的结果，然后用 c 表序列化。</p>
</div>
</li>
<li>
<p><strong>LEFT</strong>，<strong>RIGHT</strong> 和 <strong>FULL</strong> <strong>OUTER</strong> 关键字用于处理 <strong>join</strong> 中空记录的情况
例如：</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>  SELECT a.val, b.val FROM
    a LEFT OUTER JOIN b ON (a.key=b.key)</code></pre>
</div>
</div>
<div class="paragraph">
<p>对应所有 <strong>a</strong> 表中的记录都有一条记录输出。<br>
输出的结果应该是 <strong>a.val</strong>, <strong>b.val</strong>，当 <strong>a.key</strong>=<strong>b.key</strong> 时，而当 <strong>b.key</strong> 中找不到等值的 <strong>a.key</strong> 记录时也会输出:
<strong>a.val</strong>, <strong>NULL</strong>
所以 <strong>a</strong> 表中的所有记录都被保留了；
<strong>a</strong> <strong>RIGHT</strong> <strong>OUTER</strong> <strong>JOIN</strong> <strong>b</strong> 会保留所有 <strong>b</strong> 表的记录。</p>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p><strong>Join</strong> 发生在 <strong>WHERE</strong> 子句之前。如果你想限制 <strong>join</strong> 的输出，应该在 <strong>WHERE</strong> 子句中写过滤条件——或是在 <strong>join</strong> 子句中写。这里面一个容易混淆的问题是表分区的情况：</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>  SELECT a.val, b.val FROM a
  LEFT OUTER JOIN b ON (a.key=b.key)
  WHERE a.ds='2009-07-07' AND b.ds='2009-07-07'</code></pre>
</div>
</div>
<div class="paragraph">
<p>会 <strong>join</strong> <strong>a</strong> 表到 <strong>b</strong> 表（<strong>OUTER</strong> <strong>JOIN</strong>），列出 <strong>a.val</strong> 和 <strong>b.val</strong> 的记录。<strong>WHERE</strong> 从句中可以使用其他列作为过滤条件。但是，如前所述，如果 <strong>b</strong> 表中找不到对应 <strong>a</strong> 表的记录，<strong>b</strong> 表的所有列都会列出 <strong>NULL</strong>，包括 <strong>ds</strong> 列。也就是说，<strong>join</strong> 会过滤 <strong>b</strong> 表中不能找到匹配 <strong>a</strong> 表 <strong>join</strong> <strong>key</strong> 的所有记录。这样的话，<strong>LEFT</strong> <strong>OUTER</strong> 就使得查询结果与 <strong>WHERE</strong> 子句无关了。解决的办法是在 <strong>OUTER</strong> <strong>JOIN</strong> 时使用以下语法：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>  SELECT a.val, b.val FROM a LEFT OUTER JOIN b
  ON (a.key=b.key AND
      b.ds='2009-07-07' AND
      a.ds='2009-07-07')</code></pre>
</div>
</div>
<div class="paragraph">
<p>这一查询的结果是预先在 <strong>join</strong> 阶段过滤过的，所以不会存在上述问题。这一逻辑也可以应用于 <strong>RIGHT</strong> 和 <strong>FULL</strong> 类型的 <strong>join</strong> 中。</p>
</div>
</li>
<li>
<p><strong>Join</strong> 是不能交换位置的。无论是 <strong>LEFT</strong> 还是 <strong>RIGHT</strong> <strong>join</strong>，都是左连接的。</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>SELECT a.val1, a.val2, b.val, c.val
  FROM a
  JOIN b ON (a.key = b.key)
  LEFT OUTER JOIN c ON (a.key = c.key)</code></pre>
</div>
</div>
<div class="paragraph">
<p>先 <strong>join</strong> <strong>a</strong> 表到 <strong>b</strong> 表，丢弃掉所有 <strong>join</strong> <strong>key</strong> 中不匹配的记录，然后用这一中间结果和 <strong>c</strong> 表做 <strong>join</strong>。这一表述有一个不太明显的问题，就是当一个 <strong>key</strong> 在 <strong>a</strong> 表和 <strong>c</strong> 表都存在，但是 <strong>b</strong> 表中不存在的时候：整个记录在第一次 <strong>join</strong>，即 <strong>a</strong> <strong>JOIN</strong> <strong>b</strong> 的时候都被丢掉了（包括*a.val*1，<strong>a.val*2和*a.key</strong>），然后我们再和 <strong>c</strong> 表 <strong>join</strong> 的时候，如果 <strong>c.key</strong> 与 <strong>a.key</strong> 或 <strong>b.key</strong> 相等，就会得到这样的结果：<strong>NULL</strong>, <strong>NULL</strong>, <strong>NULL</strong>, <strong>c.val</strong></p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</dd>
<dt class="hdlist1">具体实例</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>获取已经分配班级的学生姓名</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image056.png" alt="image056">
</div>
</div>
<hr>
</li>
<li>
<p>获取尚未分配班级的学生姓名</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image058.png" alt="image058">
</div>
</div>
<hr>
</li>
<li>
<p>LEFT  SEMI  JOIN是IN/EXISTS的高效实现</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image060.png" alt="image060">
</div>
</div>
<hr>
</li>
</ol>
</div>
</dd>
</dl>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_hive_shell参数">4. Hive Shell参数</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_hive命令行">4.1. Hive命令行</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">语法结构</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S]</code></pre>
</div>
</div>
<div class="paragraph">
<p>说明：</p>
</div>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><strong>-i</strong> 从文件初始化HQL。<br>
<strong>-e</strong> 从命令行执行指定的HQL<br>
<strong>-f</strong> 执行HQL脚本<br>
<strong>-v</strong> 输出执行的HQL语句到控制台<br>
<strong>-p</strong> &lt;port&gt; connect to Hive Server on port number<br>
<strong>-hiveconf</strong> x=y Use this to set hive/hadoop configuration variables.<br></p>
</div>
</div>
</div>
</dd>
<dt class="hdlist1">具体实例</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>运行一个查询</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image062.png" alt="image062">
</div>
</div>
<hr>
</li>
<li>
<p>运行一个文件</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image064.png" alt="image064">
</div>
</div>
<hr>
</li>
<li>
<p>运行参数文件</p>
<div class="imageblock">
<div class="content">
<img src="/src/img/hadoop/image066.png" alt="image066">
</div>
</div>
<hr>
</li>
</ol>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_hive参数配置方式">4.2. Hive参数配置方式</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">Hive参数大全</dt>
<dd>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties" class="bare">https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties</a></p>
<div class="literalblock">
<div class="content">
<pre>开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有起作用？这通常是错误的设定方式导致的。</pre>
</div>
</div>
</dd>
<dt class="hdlist1">对于一般参数，有以下三种设定方式</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>配置文件</p>
</li>
<li>
<p>命令行参数</p>
</li>
<li>
<p>参数声明</p>
</li>
</ol>
</div>
</dd>
<dt class="hdlist1">配置文件</dt>
<dd>
<p>Hive的配置文件包括</p>
<div class="paragraph">
<p>用户自定义配置文件：<code>$HIVE_CONF_DIR/hive-site.xml</code><br>
默认配置文件：<code>$HIVE_CONF_DIR/hive-default.xml</code><br>
用户自定义配置会覆盖默认配置。
另外，<strong>Hive</strong> 也会读入 <strong>Hadoop</strong> 的配置，因为*Hive*是作为*Hadoop*的客户端启动的， <strong>Hive</strong> 的配置会覆盖 <strong>Hadoop</strong> 的配置。
配置文件的设定对本机启动的所有Hive进程都有效。</p>
</div>
</dd>
<dt class="hdlist1">命令行参数</dt>
<dd>
<p>启动 <strong>Hive</strong>（客户端或Server方式）时，可以在命令行添加 <strong>-hiveconf param=value</strong> 来设定参数，<br>
例如：</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>bin/hive -hiveconf hive.root.logger=INFO,console</code></pre>
</div>
</div>
<div class="paragraph">
<p>这一设定对本次启动的 <strong>Session</strong>（对于 <strong>Server</strong> 方式启动，则是所有请求的 <strong>Sessions</strong> ）有效。</p>
</div>
</dd>
<dt class="hdlist1">参数声明</dt>
<dd>
<p>可以在 <strong>HQL</strong> 中使用 <strong>SET</strong> 关键字设定参数，例如：</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>set mapred.reduce.tasks=100;</code></pre>
</div>
</div>
<div class="paragraph">
<p>这一设定的作用域也是 <strong>session</strong> 级的。<br>
上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。<br>
注意某些系统级的参数，例如 <strong>log4j</strong> 相关的设定，必须用前两种方式设定，因为那些参数的读取在 <strong>Session</strong> 建立以前已经完成了。</p>
</div>
</dd>
</dl>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_hive函数">5. Hive函数</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_内置运算符">5.1. 内置运算符</h3>
<div class="paragraph">
<p>内容较多，见《Hive官方文档》</p>
</div>
</div>
<div class="sect2">
<h3 id="_内置函数">5.2. 内置函数</h3>
<div class="paragraph">
<p>内容较多，见《Hive官方文档》</p>
</div>
</div>
<div class="sect2">
<h3 id="_hive自定义函数和transform">5.3. Hive自定义函数和Transform</h3>
<div class="paragraph">
<p>当 <strong>Hive</strong> 提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（<strong>UDF：user-defined function</strong>）。</p>
</div>
</div>
<div class="sect2">
<h3 id="_自定义函数类别">5.4. 自定义函数类别</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">UDF</dt>
<dd>
<p>作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数）
<strong>UDAF</strong>（用户定义聚集函数）：接收多个输入数据行，并产生一个输出数据行。（<strong>count</strong>，<strong>max</strong>）</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_udf开发实例">5.5. UDF开发实例</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>先开发一个 <strong>java</strong> 类，继承 <strong>UDF</strong> ，并重载 <strong>evaluate</strong> 方法</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>package cn.itcast.bigdata.udf
import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.Text;

public final class Lower extends UDF{
  public Text evaluate(final Text s){
    if(s==null){return null;}
    return new Text(s.toString().toLowerCase());
  }
}</code></pre>
</div>
</div>
</li>
<li>
<p>打成 <strong>jar</strong> 包上传到服务器</p>
</li>
<li>
<p>将 <strong>jar</strong> 包添加到 <strong>hive</strong> 的 <strong>classpath</strong></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>hive&gt;add JAR /home/hadoop/udf.jar;</code></pre>
</div>
</div>
</li>
<li>
<p>创建临时函数与开发好的 <strong>java class</strong> 关联</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>hive&gt;create temporary function toprovince as 'cn.itcast.bigdata.udf.ToProvince';</code></pre>
</div>
</div>
</li>
<li>
<p>即可在 <strong>hql</strong> 中使用自定义的函数 <strong>strip</strong></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>select strip(name),age from t_test;</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_transform实现">5.6. Transform实现</h3>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p><strong>Hive</strong> 的 <strong>TRANSFORM</strong> 关键字提供了在SQL中调用自写脚本的功能
适合实现 <strong>Hive</strong> 中没有的功能又不想写UDF的情况</p>
</div>
</blockquote>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">使用示例1</dt>
<dd>
<p>下面这句sql就是借用了 <strong>weekday_mapper.py</strong> 对数据进行了处理</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>CREATE TABLE u_data_new (
  movieid INT,
  rating INT,
  weekday INT,
  userid INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';

add FILE weekday_mapper.py;

INSERT OVERWRITE TABLE u_data_new
SELECT
  TRANSFORM (movieid, rating, unixtime,userid)
  USING 'python weekday_mapper.py'
  AS (movieid, rating, weekday,userid)
FROM u_data;</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">weekday_mapper.py</div>
<div class="content">
<pre class="highlightjs highlight"><code>#!/bin/python
import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  movieid, rating, unixtime,userid = line.split('\t')
  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
  print '\t'.join([movieid, rating, str(weekday),userid])</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">使用示例2</dt>
<dd>
<p>下面的例子则是使用了 <strong>shell</strong> 的 <strong>cat</strong> 命令来处理数据</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>FROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING '/bin/cat' WHERE a.ds &gt; '2008-08-09';</code></pre>
</div>
</div>
</dd>
</dl>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_hive实战">6. Hive实战</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_hive_实战案例1_数据etl">6.1. Hive 实战案例1——数据ETL</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">需求</dt>
<dd>
<p>对web点击流日志基础数据表进行etl（按照仓库模型设计）<br>
按各时间维度统计来源域名top10</p>
</dd>
<dt class="hdlist1">已有数据表 t_orgin_weblog </dt>
<dd>
<div class="listingblock">
<div class="content">
<pre>+------------------+------------+----------+--+
|     col_name     | data_type  | comment  |
+------------------+------------+----------+--+
| valid            | string     |          |
| remote_addr      | string     |          |
| remote_user      | string     |          |
| time_local       | string     |          |
| request          | string     |          |
| status           | string     |          |
| body_bytes_sent  | string     |          |
| http_referer     | string     |          |
| http_user_agent  | string     |          |
+------------------+------------+----------+--+</pre>
</div>
</div>
</dd>
<dt class="hdlist1">数据示例</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre>| true|1.162.203.134| - | 18/Sep/2013:13:47:35| /images/my.jpg                        | 200| 19939 | "http://www.angularjs.cn/A0d9"                      | "Mozilla/5.0 (Windows   |

| true|1.202.186.37 | - | 18/Sep/2013:15:39:11| /wp-content/uploads/2013/08/windjs.png| 200| 34613 | "http://cnodejs.org/topic/521a30d4bee8d3cb1272ac0f" | "Mozilla/5.0 (Macintosh;|</pre>
</div>
</div>
</dd>
<dt class="hdlist1">实现步骤</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>对原始数据进行抽取转换
将来访 <strong>url</strong> 分离出 <strong>host</strong>  <strong>path</strong>  <strong>query</strong>  <strong>query</strong> <strong>id</strong></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>drop table if exists t_etl_referurl;
create table t_etl_referurl as
SELECT a.*,b.*
FROM t_orgin_weblog a LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, "\"", ""), 'HOST', 'PATH','QUERY', 'QUERY:id') b as host, path, query, query_id</code></pre>
</div>
</div>
</li>
<li>
<p>从前述步骤进一步分离出日期时间形成 <strong>ETL</strong> 明细表 <strong>t_etl_detail</strong>    day tm</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>drop table if exists t_etl_detail;
create table t_etl_detail as
select b.*,substring(time_local,0,11) as daystr,
substring(time_local,13) as tmstr,
substring(time_local,4,3) as month,
substring(time_local,0,2) as day,
substring(time_local,13,2) as hour
from t_etl_referurl b;</code></pre>
</div>
</div>
</li>
<li>
<p>对 <strong>etl</strong> 数据进行分区(包含所有数据的结构化信息)</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>drop table t_etl_detail_prt;
create table t_etl_detail_prt(
valid                   string,
remote_addr            string,
remote_user            string,
time_local               string,
request                 string,
status                  string,
body_bytes_sent         string,
http_referer             string,
http_user_agent         string,
host                   string,
path                   string,
query                  string,
query_id               string,
daystr                 string,
tmstr                  string,
month                  string,
day                    string,
hour                   string)
partitioned by (mm string,dd string);</code></pre>
</div>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>导入数据</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>insert into table t_etl_detail_prt partition(mm='Sep',dd='18')
select * from t_etl_detail where daystr='18/Sep/2013';

insert into table t_etl_detail_prt partition(mm='Sep',dd='19')
select * from t_etl_detail where daystr='19/Sep/2013';</code></pre>
</div>
</div>
</li>
<li>
<p>分个时间维度统计各 <strong>referer_host</strong> 的访问次数并排序</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>create table t_refer_host_visit_top_tmp as
select referer_host,count(*) as counts,mm,dd,hh from t_display_referer_counts group by hh,dd,mm,referer_host order by hh asc,dd asc,mm asc,counts desc;</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>来源访问次数 <strong>topn</strong> 各时间维度 <strong>URL</strong>
取各时间维度的 <strong>referer_host</strong> 访问次数 <strong>topn</strong></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>select * from (select referer_host,counts,concat(hh,dd),row_number() over (partition by concat(hh,dd) order by concat(hh,dd) asc) as od from t_refer_host_visit_top_tmp) t where od&lt;=3;</code></pre>
</div>
</div>
</li>
</ol>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_实战案例2_访问时长统计">6.2. 实战案例2——访问时长统计</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">需求</dt>
<dd>
<p>从web日志中统计每日访客平均停留时间</p>
</dd>
<dt class="hdlist1">实现步骤</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>由于要从大量请求中分辨出用户的各次访问，逻辑相对复杂，通过 <strong>hive</strong> 直接实现有困难，因此编写一个 <strong>mr</strong> 程序来求出访客访问信息（详见代码）
启动 <strong>mr</strong> 程序获取结果：</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>[hadoop@hdp-node-01 ~]$ hadoop jar weblog.jar cn.itcast.bigdata.hive.mr.UserStayTime /weblog/input /weblog/stayout</code></pre>
</div>
</div>
</li>
<li>
<p>将 <strong>mr</strong> 的处理结果导入 <strong>hive</strong> 表</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>drop table t_display_access_info_tmp;
create table t_display_access_info_tmp(remote_addr string,firt_req_time string,last_req_time string,stay_long bigint)
row format delimited fields terminated by '\t';

load data inpath '/weblog/stayout4' into table t_display_access_info_tmp;</code></pre>
</div>
</div>
</li>
<li>
<p>得出访客访问信息表 <strong>t_display_access_info</strong>
由于有一些访问记录是单条记录，mr程序处理处的结果给的时长是0，所以考虑给单次请求的停留时间一个默认市场30秒</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>drop table t_display_access_info;
create table t_display_access_info as
select remote_addr,firt_req_time,last_req_time,
case stay_long
when 0 then 30000
else stay_long
end as stay_long
from t_display_access_info_tmp;</code></pre>
</div>
</div>
</li>
<li>
<p>统计所有用户停留时间平均值</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>select avg(stay_long) from t_display_access_info;</code></pre>
</div>
</div>
</li>
</ol>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="_实战案例3_级联求和">6.3. 实战案例3——级联求和</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">需求</dt>
<dd>
<p>有如下访客访问次数统计表 <strong>t_access_times</strong></p>
<div class="listingblock">
<div class="content">
<pre>访客    月份       访问次数
A     2015-01-02      5
A     2015-01-03      15
B     2015-01-01      5
A     2015-01-04      8
B     2015-01-05      25
A     2015-01-06      5
A     2015-02-02      4
A     2015-02-06      6
B     2015-02-06      10
B     2015-02-07      5
……    ……              ……</pre>
</div>
</div>
<div class="paragraph">
<p>需要输出报表：<strong>t_access_times_accumulate</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre>访客  月份     月访问总计    累计访问总计
A     2015-01     33              33
A     2015-02     10              43
…….   …….         …….             …….
B     2015-01     30              30
B     2015-02     15              45
…….   …….         …….             …….</pre>
</div>
</div>
</dd>
<dt class="hdlist1">实现步骤</dt>
<dd>
<p>可以用一个hql语句即可实现：</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code>select A.username,A.month,max(A.salary) as salary,sum(B.salary) as accumulate
from
(select username,month,sum(salary) as salary from t_access_times group by username,month) A
inner join
(select username,month,sum(salary) as salary from t_access_times group by username,month) B
on
A.username=B.username
where B.month &lt;= A.month
group by A.username,A.month
order by A.username,A.month;</code></pre>
</div>
</div>
</dd>
</dl>
</div>
</div>
</div>
</div>

		</div>
		
	</article>
	
<div class="mr-author-box clearfix">
	<figure class="mr-author-box-avatar">
		<img alt="dishui avatar" src="/src/img/dishui.png" class="avatar avatar-90 photo" height="90" width="90">
	</figure>
	<div class="mr-author-box-header">
		<span class="mr-author-box-name">About dishui</span>
	</div>
	<div class="mr-author-box-bio">
		辛勤的搬运工!!!
	</div>
</div>

	

	<nav class="mr-post-nav mr-row clearfix" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
		
		<div class="mr-col-1-2 mr-post-nav-item mr-post-nav-prev">
			<a href="/post/bigdata/hadoop/flume-sqoop/" rel="prev"><span>«Previous</span><p>flume-sqoop</p></a>
		</div>
		
		
		<div class="mr-col-1-2 mr-post-nav-item mr-post-nav-next">
			<a href="/post/bigdata/hadoop/hive/" rel="next"><span>Next»</span><p>hive</p></a>
		</div>
		
	</nav>


	
</div>

<aside class="mr-sidebar" itemscope="itemscope" itemtype="http://schema.org/WPSideBar">
<div class="mr-widget widget_search navbar-wrapper" >
    <form class="search-form" role="search" >
        <label>
            <span class="screen-reader-text">Search for:</span>
            <input id="lanren" type="search" class="search-field" placeholder="Search..." value="" name="q">
        </label>
        <div id="list-container" class="bdsug" style="height: auto; display: block;">
        </div>
    </form>
    <div id="side-toc" class="entry-content">
    
    </div>
</div>
</aside>
	</div>
		<div class="mr-copyright-wrap">
			<div class="mr-container mr-container-inner clearfix">
				<p class="mr-copyright">&copy; 2017 零零散散. <a href="https://git.oschina.net/dishui/dishui" target="_blank" rel="nofollow noopener noreferrer">dishui</a>.</p>
			</div>
		</div>
	</div>

<script>
	var navigation = responsiveNav(".menu", {
		navClass: "menu--collapse",
	});
</script>
<script src="/js/asciinema-player.js"></script>
<script data-main="/js/app.js" src="/js/require.js"></script>


</body>
</html>