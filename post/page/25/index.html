<!DOCTYPE html>
<html lang="zh-cn">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Posts</title>
<meta name="description" content=" John Doe&#39;s Personal blog about everything">
<meta name="generator" content="Hugo 0.17" />
<meta property="og:title" content="Posts" />
<meta property="og:description" content=" John Doe&#39;s Personal blog about everything" />
<meta property="og:type" content="website" />
<meta property="og:url" content="/post/" />













<link rel="stylesheet" href="/css/google-font.css?family=Open+Sans:400,400italic,700,600" type="text/css" media="all" />

<link rel="stylesheet" href="/css/railscasts.css">
<link rel="stylesheet" href="/css/style.css" type="text/css" media="all" />
<link rel="stylesheet" href="/css/custom.css" type="text/css" media="all" />
<link rel="stylesheet" href="/css/jquery.bigautocomplete.css" type="text/css" media="all" />
<link rel="stylesheet" href="/css/asciinema-player.css" type="text/css" media="all" />
<link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet">
<link href="//cdn.bootcss.com/font-awesome/4.7.0/fonts/fontawesome-webfont.svg" rel="stylesheet">

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/javascript" src="/js/scripts.js"></script>
<!--[if lt IE 9]>
	<script src="/js/css3-mediaqueries.js"></script>
<![endif]-->

</head>
<body id="mr-mobile" class="home blog mr-right-sb" itemscope="itemscope" itemtype="http://schema.org/WebPage">
	<div class="mr-container mr-container-outer">
		<div class="mr-header-mobile-nav clearfix"></div>
			<header class="mr-header" itemscope="itemscope" itemtype="http://schema.org/WPHeader">
				<div class="mr-container mr-container-inner mr-row clearfix">
					<div class="mr-custom-header clearfix">
						<div class="mr-site-identity">
							<div class="mr-site-logo" role="banner" itemscope="itemscope" itemtype="http://schema.org/Brand">
								<div class="mr-header-text">
									<a class="mr-header-text-link" href="/" title="零零碎碎" rel="home">
										<h1 class="mr-header-title">零零碎碎</h1>
										<h2 class="mr-header-tagline">点滴记录</h2>
									</a>
								</div>
							</div>
						</div>
					</div>
				</div>
				<div class="mr-main-nav-wrap">
					<nav class="menu" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
	<ul class="menu__list">
        
            <li class="menu__item"><a class="menu__link" href="/categories/docker">docker</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/hadoop">hadoop</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/scala">scala</a></li>
        
            <li class="menu__item"><a class="menu__link" href="/categories/spark">spark</a></li>
        
	</ul>
</nav>
				</div>
			</header>
		<div class="mr-wrapper clearfix">


	<div id="main-content" class="mr-loop mr-content" role="main">
		
		<header class="page-header">
			<h1 class="page-title">Posts</h1>
		</header>
		
		
		
		<article class="mr-loop-item post type-post hentry clearfix">
			
			<div class="mr-loop-content clearfix">
				<header class="mr-loop-header">
					<h3 class="entry-title mr-loop-title">
						<a href="/post/bigdata/spark/sparknotes/rdd_action_operator/02/" rel="bookmark"></a>
					</h3>
					<div class="mr-meta mr-loop-meta">
						<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
						<time class="mr-meta-date updated" datetime="0001-01-01 00:00:00 &#43;0000 UTC">January 01, 0001</time>
					</div>
				</header>
				<div class="mr-loop-excerpt">
					<div class="mr-excerpt">
						take、top、takeOrdered take def take(num: Int): Array[T]
take用于获取RDD中从0到num-1下标的元素，不排序
scala&gt; var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3)) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at makeRDD at :21 scala&gt; rdd1.take(1) res0: Array[Int] = Array(10) scala&gt; rdd1.take(2) res1: Array[Int] = Array(10, 4)  top def top(num: Int)(implicit ord: Ordering[T]):Array[T]
top函数用于从RDD中，按照默认（降序）或者指定的排序规则，返回前num个元素
scala&gt; var rdd1 = sc.makeRDD(Seq(10, 4, 2, 12, 3)) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at makeRDD at :21 scala&gt; rdd1.top(1) res2: Array[Int] = Array(12) scala&gt; rdd1.
					</div>
				</div>
			</div>
		</article>
		
		<article class="mr-loop-item post type-post hentry clearfix">
			
			<div class="mr-loop-content clearfix">
				<header class="mr-loop-header">
					<h3 class="entry-title mr-loop-title">
						<a href="/post/bigdata/spark/sparknotes/rdd_action_operator/03/" rel="bookmark"></a>
					</h3>
					<div class="mr-meta mr-loop-meta">
						<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
						<time class="mr-meta-date updated" datetime="0001-01-01 00:00:00 &#43;0000 UTC">January 01, 0001</time>
					</div>
				</header>
				<div class="mr-loop-excerpt">
					<div class="mr-excerpt">
						aggregate、fold、lookup aggregate def aggregateU(seqOp:(U,T)) =&gt; U,combOp:(U,U) =&gt; U)(implicit arg0: ClassTag[U]):U
aggregate用户聚合RDD中的元素，先使用seqOp将RDD中每个分区中的T类型元素聚合成U类型，再使用combOp将之前每个分区聚合后的U类型聚合成U类型，特别注意seqOp和comOp都会使用zeroValue的值，zeroValue的类型为U。
var rdd1 = sc.makeRDD(1 to 10,2) rdd1.mapPartitionsWithIndex{ (partIdx,iter) =&gt; { var part_map = scala.collection.mutable.Map[String,List[Int]]() while(iter.hasNext){ var part_name = &quot;part_&quot; + partIdx; var elem = iter.next() if(part_map.contains(part_name)) { var elems = part_map(part_name) elems ::= elem part_map(part_name) = elems } else { part_map(part_name) = List[Int]{elem} } } part_map.iterator } }.collect res16: Array[(String, List[Int])] = Array((part_0,List(5, 4, 3, 2, 1)), (part_1,List(10, 9, 8, 7, 6)))  第一个分区中包含5,4,3,2,1 第二个分区中包含10,9,8,7,6
					</div>
				</div>
			</div>
		</article>
		
		<article class="mr-loop-item post type-post hentry clearfix">
			
			<div class="mr-loop-content clearfix">
				<header class="mr-loop-header">
					<h3 class="entry-title mr-loop-title">
						<a href="/post/bigdata/spark/sparknotes/rdd_action_operator/04/" rel="bookmark"></a>
					</h3>
					<div class="mr-meta mr-loop-meta">
						<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
						<time class="mr-meta-date updated" datetime="0001-01-01 00:00:00 &#43;0000 UTC">January 01, 0001</time>
					</div>
				</header>
				<div class="mr-loop-excerpt">
					<div class="mr-excerpt">
						countByKey、foreach、foreachPartition、sortBy countByKey def countByKey(): Map[K, Long]
countByKey用于统计RDD[K,V]中每个K的数量。
scala&gt; var rdd1 = sc.makeRDD(Array((&quot;A&quot;,0),(&quot;A&quot;,2),(&quot;B&quot;,1),(&quot;B&quot;,2),(&quot;B&quot;,3))) rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[7] at makeRDD at :21 scala&gt; rdd1.countByKey res5: scala.collection.Map[String,Long] = Map(A -&gt; 2, B -&gt; 3)  foreach def foreach(f: (T) ⇒ Unit): Unit
foreach用于遍历RDD,将函数f应用于每一个元素。
但要注意，如果对RDD执行foreach，只会在Executor端有效，而并不是Driver端。
在Spark1.4中是这样，不知道是否真如此。
这时候，使用accumulator共享变量与foreach结合，倒是个不错的选择。
scala&gt; var cnt = sc.accumulator(0) cnt: org.apache.spark.Accumulator[Int] = 0 scala&gt; var rdd1 = sc.makeRDD(1 to 10,2) rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at makeRDD at :21 scala&gt; rdd1.
					</div>
				</div>
			</div>
		</article>
		
		<article class="mr-loop-item post type-post hentry clearfix">
			
			<div class="mr-loop-content clearfix">
				<header class="mr-loop-header">
					<h3 class="entry-title mr-loop-title">
						<a href="/post/bigdata/spark/sparknotes/rdd_action_operator/05/" rel="bookmark"></a>
					</h3>
					<div class="mr-meta mr-loop-meta">
						<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
						<time class="mr-meta-date updated" datetime="0001-01-01 00:00:00 &#43;0000 UTC">January 01, 0001</time>
					</div>
				</header>
				<div class="mr-loop-excerpt">
					<div class="mr-excerpt">
						 saveAsSequenceFile、saveAsObjectFile saveAsTextFile def saveAsTextFile(path: String): Unit
def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit
saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。
codec参数可以指定压缩的类名。
var rdd1 = sc.makeRDD(1 to 10,2) scala&gt; rdd1.saveAsTextFile(&quot;hdfs:///user/apple/testspark&quot;) //保存到HDFS scala&gt; rdd1.saveAsTextFile(&quot;file:///Users/apple/testspark&quot;) //保存到本地 //由于设置为两个分区，因此保存的路径中，有part-00000,part-00000  指定压缩格式保存
//需要将使用的压缩工具包导入 rdd1.saveAsTextFile(&quot;hdfs:///user/apple/testspark1&quot;,classOf[com.hadoop.compression.lzo.LzopCodec])  saveAsSequenceFile saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。
用法同saveAsTextFile。
spark 1.6.1 没有改方法
saveAsObjectFile def saveAsObjectFile(path: String): Unit
saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。
对于HDFS，默认采用SequenceFile保存。
rdd1.saveAsObjectFile(&quot;/user/apple/sparktest02&quot;) hadoop fs -cat /user/apple/sparktest02/part-00000 SEQ !org.apache.hadoop.io.NullWritable&quot;org.apache.hadoop.io.BytesWritableT  
					</div>
				</div>
			</div>
		</article>
		
		<article class="mr-loop-item post type-post hentry clearfix">
			
			<div class="mr-loop-content clearfix">
				<header class="mr-loop-header">
					<h3 class="entry-title mr-loop-title">
						<a href="/post/bigdata/spark/sparknotes/rdd_action_operator/06/" rel="bookmark"></a>
					</h3>
					<div class="mr-meta mr-loop-meta">
						<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
						<time class="mr-meta-date updated" datetime="0001-01-01 00:00:00 &#43;0000 UTC">January 01, 0001</time>
					</div>
				</header>
				<div class="mr-loop-excerpt">
					<div class="mr-excerpt">
						saveAsHadoopFile、saveAsHadoopDataset saveAsHadoopFile def saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[_, ]], codec: Class[ &lt;: CompressionCodec]): Unit
def saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[_, ]], conf: JobConf = …, codec: Option[Class[ &lt;: CompressionCodec]] = None): Unit
saveAsHadoopFile是讲RDD存储在HDFS上的文件中，支持老版本Hadoop API。
每个分区输出一个文件。
var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7))) import org.apache.hadoop.mapred.TextOutputFormat import org.apache.hadoop.io.Text import org.apache.hadoop.io.IntWritable rdd1.saveAsHadoopFile(&quot;/tmp/lxw1234.com/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]]) rdd1.saveAsHadoopFile(&quot;/tmp/lxw1234.com/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]], classOf[com.hadoop.compression.lzo.LzopCodec])  saveAsHadoopDataset def saveAsHadoopDataset(conf: JobConf): Unit
saveAsHadoopDataset用于将RDD保存到除了HDFS的其他存储中，比如HBase。
在JobConf中，通常需要关注或者设置五个参数：
 文件的保存路劲 key值的class类型 value值的class类型 RDD的输出格式（OutputFormat） 压缩的相关参数  使用saveAsHadoopDataset将RDD保存到HDFS中
					</div>
				</div>
			</div>
		</article>
		
		<article class="mr-loop-item post type-post hentry clearfix">
			
			<div class="mr-loop-content clearfix">
				<header class="mr-loop-header">
					<h3 class="entry-title mr-loop-title">
						<a href="/post/bigdata/spark/sparknotes/rdd_action_operator/07/" rel="bookmark"></a>
					</h3>
					<div class="mr-meta mr-loop-meta">
						<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
						<time class="mr-meta-date updated" datetime="0001-01-01 00:00:00 &#43;0000 UTC">January 01, 0001</time>
					</div>
				</header>
				<div class="mr-loop-excerpt">
					<div class="mr-excerpt">
						saveAsNewAPIHadoopFile、saveAsNewAPIHadoopDataset saveAsNewAPIHadoopFile def saveAsNewAPIHadoopFileF &lt;: OutputFormat[K, V](implicit fm: ClassTag[F]): Unit
def saveAsNewAPIHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit
saveAsNewAPIHadoopFile用于将RDD数据保存到HDFS上，使用新版本Hadoop API。 用法基本同saveAsHadoopFile。
import org.apache.spark.SparkConf import org.apache.spark.SparkContext import SparkContext._ import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat import org.apache.hadoop.io.Text import org.apache.hadoop.io.IntWritable var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7))) rdd1.saveAsNewAPIHadoopFile(&quot;/tmp/lxw1234/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])  saveAsNewAPIHadoopDataset def saveAsNewAPIHadoopDataset(conf: Configuration): Unit
作用同saveAsHadoopDataset,只不过采用新版本Hadoop API。
以写入HBase为例：
HBase建表：
create ‘lxw1234′,{NAME =&gt; ‘f1′,VERSIONS =&gt; 1},{NAME =&gt; ‘f2′,VERSIONS =&gt; 1},{NAME =&gt; ‘f3′,VERSIONS =&gt; 1}
					</div>
				</div>
			</div>
		</article>
		
		<article class="mr-loop-item post type-post hentry clearfix">
			
			<div class="mr-loop-content clearfix">
				<header class="mr-loop-header">
					<h3 class="entry-title mr-loop-title">
						<a href="/post/bigdata/spark/sparknotes/rdd_transform_operator/01/" rel="bookmark"></a>
					</h3>
					<div class="mr-meta mr-loop-meta">
						<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
						<time class="mr-meta-date updated" datetime="0001-01-01 00:00:00 &#43;0000 UTC">January 01, 0001</time>
					</div>
				</header>
				<div class="mr-loop-excerpt">
					<div class="mr-excerpt">
						map、flagMap、distinct map 将一个RDD中的每个数据项，通过map中的函数映射变为一个新的元素。 输入分区与输出分区一对一，即：有多少个输入分区，就有多少个输出分区。
hadoop fs -cat /tmp/lxw1234/1.txt hello world hello spark hello hive //读取HDFS文件到RDD scala&gt; var data = sc.textFile(&quot;/tmp/lxw1234/1.txt&quot;) data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at :21 //使用map算子 scala&gt; var mapresult = data.map(line =&gt; line.split(&quot;\\s+&quot;)) mapresult: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at :23 //运算map算子结果 scala&gt; mapresult.collect res0: Array[Array[String]] = Array(Array(hello, world), Array(hello, spark), Array(hello, hive))  flatMap 属于Transformation算子，第一步和map一样，最后将所有的输出分区合并成一个。
/使用flatMap算子 scala&gt; var flatmapresult = data.flatMap(line =&gt; line.split(&quot;\\s+&quot;)) flatmapresult: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at flatMap at :23 //运算flagMap算子结果 scala&gt; flatmapresult.
					</div>
				</div>
			</div>
		</article>
		
		<article class="mr-loop-item post type-post hentry clearfix">
			
			<div class="mr-loop-content clearfix">
				<header class="mr-loop-header">
					<h3 class="entry-title mr-loop-title">
						<a href="/post/bigdata/spark/sparknotes/rdd_%E9%94%AE%E5%80%BC%E8%BD%AC%E6%8D%A2/01/" rel="bookmark"></a>
					</h3>
					<div class="mr-meta mr-loop-meta">
						<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
						<time class="mr-meta-date updated" datetime="0001-01-01 00:00:00 &#43;0000 UTC">January 01, 0001</time>
					</div>
				</header>
				<div class="mr-loop-excerpt">
					<div class="mr-excerpt">
						partitionBy、mapValues、flatMapValues partitionBy def partitionBy(partitioner: Partitioner): RDD[(K, V)]
该函数根据partitioner函数生成新的ShuffleRDD，将原RDD重新分区。
scala&gt; var rdd1 = sc.makeRDD(Array((1,&quot;A&quot;),(2,&quot;B&quot;),(3,&quot;C&quot;),(4,&quot;D&quot;)),2) rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[23] at makeRDD at :21 scala&gt; rdd1.partitions.size res20: Int = 2 //查看rdd1中每个分区的元素 scala&gt; rdd1.mapPartitionsWithIndex{ | (partIdx,iter) =&gt; { | var part_map = scala.collection.mutable.Map[String,List[(Int,String)]]() | while(iter.hasNext){ | var part_name = &quot;part_&quot; + partIdx; | var elem = iter.next() | if(part_map.contains(part_name)) { | var elems = part_map(part_name) | elems ::= elem | part_map(part_name) = elems | } else { | part_map(part_name) = List[(Int,String)]{elem} | } | } | part_map.
					</div>
				</div>
			</div>
		</article>
		
		<article class="mr-loop-item post type-post hentry clearfix">
			
			<div class="mr-loop-content clearfix">
				<header class="mr-loop-header">
					<h3 class="entry-title mr-loop-title">
						<a href="/post/bigdata/spark/sparknotes/rdd_%E9%94%AE%E5%80%BC%E8%BD%AC%E6%8D%A2/03/" rel="bookmark"></a>
					</h3>
					<div class="mr-meta mr-loop-meta">
						<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
						<time class="mr-meta-date updated" datetime="0001-01-01 00:00:00 &#43;0000 UTC">January 01, 0001</time>
					</div>
				</header>
				<div class="mr-loop-excerpt">
					<div class="mr-excerpt">
						groupByKey、reduceByKey、reduceByKeyLocally groupByKey def groupByKey(): RDD[(K, Iterable[V])]
def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]
该函数用于将RDD[K,V]中每个K对应的V值，合并到一个集合Iterable[V]中，
参数numPartitions用于指定分区数；
参数partitioner用于指定分区函数；
scala&gt; var rdd1 = sc.makeRDD(Array((&quot;A&quot;,0),(&quot;A&quot;,2),(&quot;B&quot;,1),(&quot;B&quot;,2),(&quot;C&quot;,1))) rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[89] at makeRDD at :21 scala&gt; rdd1.groupByKey().collect res81: Array[(String, Iterable[Int])] = Array((A,CompactBuffer(0, 2)), (B,CompactBuffer(2, 1)), (C,CompactBuffer(1)))  reduceByKey def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]
def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]
def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)]
					</div>
				</div>
			</div>
		</article>
		
		<article class="mr-loop-item post type-post hentry clearfix">
			
			<div class="mr-loop-content clearfix">
				<header class="mr-loop-header">
					<h3 class="entry-title mr-loop-title">
						<a href="/post/bigdata/spark/sparknotes/README/" rel="bookmark"></a>
					</h3>
					<div class="mr-meta mr-loop-meta">
						<svg class="icon icon-time" height="14" viewBox="0 0 16 16" width="14" xmlns="http://www.w3.org/2000/svg"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
						<time class="mr-meta-date updated" datetime="0001-01-01 00:00:00 &#43;0000 UTC">January 01, 0001</time>
					</div>
				</header>
				<div class="mr-loop-excerpt">
					<div class="mr-excerpt">
						Spark 学习笔记 分享我学习spark期间，做的笔记，有很多东西，也是集百家之言，做出来的总结。 由于，在整理笔记期间，很多学习过程中，参考的书籍以及网页都已经忘记，部分不能标明内容出处，再次，向原作者致歉。
该 Spark学习笔记，会常常更新，以督促自己学习。
					</div>
				</div>
			</div>
		</article>
		

		
<div class="mr-loop-pagination clearfix">
	
	<a class="page-numbers" href="/post/page/24/">«</a>
	
	<span class="page-numbers current">25/32</span>
	
	<a class="page-numbers" href="/post/page/26/">»</a>
	
</div>

	</div>

<aside class="mr-sidebar" itemscope="itemscope" itemtype="http://schema.org/WPSideBar">
	
<div class="mr-widget widget_search navbar-wrapper" >
	<div class="search-form" role="search" >
		<label>
			<span class="screen-reader-text">Search for:</span>
			<input id="lanren" type="text" class="search-field" placeholder="Search..." autocomplete="off" value="" />
		</label>
		
        <div id="list-container" class="bdsug" style="height: auto; display: block;">
        </div>
	</div>
</div>

	
	


<div class="mr-widget widget_categories">
	<h4 class="mr-widget-title"><span class="mr-widget-title-inner">Categories</span></h4>
	<ul>
		
		<li class="cat-item"><a href="/categories/algorithm">Algorithm</a></li>
		
		<li class="cat-item"><a href="/categories/bug">Bug</a></li>
		
		<li class="cat-item"><a href="/categories/djt">Djt</a></li>
		
		<li class="cat-item"><a href="/categories/docker">Docker</a></li>
		
		<li class="cat-item"><a href="/categories/dw">Dw</a></li>
		
		<li class="cat-item"><a href="/categories/go">Go</a></li>
		
		<li class="cat-item"><a href="/categories/hadoop">Hadoop</a></li>
		
		<li class="cat-item"><a href="/categories/java">Java</a></li>
		
		<li class="cat-item"><a href="/categories/js">Js</a></li>
		
		<li class="cat-item"><a href="/categories/jvm">Jvm</a></li>
		
		<li class="cat-item"><a href="/categories/linux">Linux</a></li>
		
		<li class="cat-item"><a href="/categories/mailiqng-app">Mailiqng-App</a></li>
		
		<li class="cat-item"><a href="/categories/mysql">Mysql</a></li>
		
		<li class="cat-item"><a href="/categories/scala">Scala</a></li>
		
		<li class="cat-item"><a href="/categories/spark">Spark</a></li>
		
		<li class="cat-item"><a href="/categories/storm">Storm</a></li>
		
		<li class="cat-item"><a href="/categories/virtualbox">Virtualbox</a></li>
		
		<li class="cat-item"><a href="/categories/work">Work</a></li>
		
		<li class="cat-item"><a href="/categories/zookeeper">Zookeeper</a></li>
		
		<li class="cat-item"><a href="/categories/%e7%ae%97%e6%b3%95">算法</a></li>
		
		<li class="cat-item"><a href="/categories/%e9%ab%98%e6%95%b0">高数</a></li>
		
	</ul>
</div>



    


<div class="mr-widget widget_tag_cloud">
	<h4 class="mr-widget-title"><span class="mr-widget-title-inner">Tags</span></h4>
	<div class="tagcloud">
		
			<a href="/tags/" class="tag-link" title="" style="font-size: 12px;"></a>
		
			<a href="/tags/docker" class="tag-link" title="docker" style="font-size: 12px;">docker</a>
		
			<a href="/tags/elasticsearch" class="tag-link" title="elasticsearch" style="font-size: 12px;">elasticsearch</a>
		
			<a href="/tags/git" class="tag-link" title="git" style="font-size: 12px;">git</a>
		
			<a href="/tags/hugo" class="tag-link" title="hugo" style="font-size: 12px;">hugo</a>
		
			<a href="/tags/latex" class="tag-link" title="latex" style="font-size: 12px;">latex</a>
		
			<a href="/tags/neo4j" class="tag-link" title="neo4j" style="font-size: 12px;">neo4j</a>
		
			<a href="/tags/sublime" class="tag-link" title="sublime" style="font-size: 12px;">sublime</a>
		
			<a href="/tags/vagrant" class="tag-link" title="vagrant" style="font-size: 12px;">vagrant</a>
		
			<a href="/tags/windows" class="tag-link" title="windows" style="font-size: 12px;">windows</a>
		
			<a href="/tags/wuliu" class="tag-link" title="wuliu" style="font-size: 12px;">wuliu</a>
		
			<a href="/tags/%e6%ba%90%e7%a0%81%e5%88%86%e6%9e%90" class="tag-link" title="源码分析" style="font-size: 12px;">源码分析</a>
		
			<a href="/tags/%e8%bf%9b%e7%a8%8b%e7%ae%a1%e7%90%86" class="tag-link" title="进程管理" style="font-size: 12px;">进程管理</a>
		
	</div>
</div>



</aside>
	</div>
		<div class="mr-copyright-wrap">
			<div class="mr-container mr-container-inner clearfix">
				<p class="mr-copyright">&copy; 2019 零零碎碎. <a href="https://git.oschina.net/dishui/dishui" target="_blank" rel="nofollow noopener noreferrer">dishui</a>.</p>
			</div>
		</div>
	</div>

<script>
	var navigation = responsiveNav(".menu", {
		navClass: "menu--collapse",
	});
</script>
<script src="/js/asciinema-player.js"></script>
<script data-main="/js/app.js" src="/js/require.js"></script>


</body>
</html>
