<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on 工作笔记</title>
    <link>/post/index.xml</link>
    <description>Recent content in Post-rsses on 工作笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 13 Apr 2017 14:04:07 +0000</lastBuildDate>
    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Zhen He spark rdd api</title>
      <link>/post/bigdata/spark/spark-rdd-api/</link>
      <pubDate>Thu, 13 Apr 2017 14:04:07 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-rdd-api/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;spark&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_part1&#34;&gt;1. part1&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_aggregate&#34;&gt;1.1. aggregate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_aggregatebykey_pair&#34;&gt;1.2. aggregateByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_cartesian&#34;&gt;1.3. cartesian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_checkpoint&#34;&gt;1.4. checkpoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_coalesce_repartition&#34;&gt;1.5. coalesce, repartition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_cogroup_pair_groupwith_pair&#34;&gt;1.6. cogroup [Pair], groupWith [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_collect_toarray&#34;&gt;1.7. collect, toArray&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_collectasmap_pair&#34;&gt;1.8. collectAsMap [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_combinebykey_pair&#34;&gt;1.9. combineByKey[Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_compute&#34;&gt;1.10. compute&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_context_sparkcontext&#34;&gt;1.11. context, sparkContext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_count&#34;&gt;1.12. count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countapprox&#34;&gt;1.13. countApprox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countapproxdistinct&#34;&gt;1.14. countApproxDistinct&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countapproxdistinctbykey_pair&#34;&gt;1.15. countApproxDistinctByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_part2&#34;&gt;2. part2&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_countbykey_pair&#34;&gt;2.1. countByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countbykeyapprox_pair&#34;&gt;2.2. countByKeyApprox [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countbyvalue&#34;&gt;2.3. countByValue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countbyvalueapprox&#34;&gt;2.4. countByValueApprox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_dependencies&#34;&gt;2.5. dependencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_distinct&#34;&gt;2.6. distinct&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_first&#34;&gt;2.7. first&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_filter&#34;&gt;2.8. filter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_filterbyrange_ordered&#34;&gt;2.9. filterByRange [Ordered]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_filterwith_deprecated&#34;&gt;2.10. filterWith  (deprecated)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_flatmap&#34;&gt;2.11. flatMap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_flatmapvalues&#34;&gt;2.12. flatMapValues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_flatmapwith_deprecated&#34;&gt;2.13. flatMapWith (deprecated)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_fold&#34;&gt;2.14. fold&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foldbykey_pair&#34;&gt;2.15. foldByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foreach&#34;&gt;2.16. foreach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foreachpartition&#34;&gt;2.17. foreachPartition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foreachwith_deprecated&#34;&gt;2.18. foreachWith (Deprecated)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_fullouterjoin_pair&#34;&gt;2.19. fullOuterJoin [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_part3&#34;&gt;3. part3&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_generator_setgenerator&#34;&gt;3.1. generator, setGenerator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_getcheckpointfile&#34;&gt;3.2. getCheckpointFile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_preferredlocations&#34;&gt;3.3. preferredLocations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_getstoragelevel&#34;&gt;3.4. getStorageLevel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_glom&#34;&gt;3.5. glom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_groupby&#34;&gt;3.6. groupBy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_groupbykey_pair&#34;&gt;3.7. groupByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_histogram_double&#34;&gt;3.8. histogram [Double]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_id&#34;&gt;3.9. id&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_intersection&#34;&gt;3.10. intersection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_ischeckpointed&#34;&gt;3.11. isCheckpointed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_iterator&#34;&gt;3.12. iterator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_join_pair&#34;&gt;3.13. join [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_keyby&#34;&gt;3.14. keyBy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_keys_pair&#34;&gt;3.15. keys [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_leftouterjoin_pair&#34;&gt;3.16. leftOuterJoin [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_lookup&#34;&gt;3.17. lookup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_map&#34;&gt;3.18. map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mappartitions&#34;&gt;3.19. mapPartitions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mappartitionswithcontext_deprecated_and_developer_api&#34;&gt;3.20. mapPartitionsWithContext   (deprecated and developer API)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mappartitionswithindex&#34;&gt;3.21. mapPartitionsWithIndex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mappartitionswithsplit&#34;&gt;3.22. mapPartitionsWithSplit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mapvalues_pair&#34;&gt;3.23. mapValues [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mapwith_deprecated&#34;&gt;3.24. mapWith  (deprecated)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_part4&#34;&gt;4. part4&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_max&#34;&gt;4.1. max&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mean_double_meanapprox_double&#34;&gt;4.2. mean [Double], meanApprox [Double]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_min&#34;&gt;4.3. min&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_name_setname&#34;&gt;4.4. name, setName&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_partitionby_pair&#34;&gt;4.5. partitionBy [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_partitioner&#34;&gt;4.6. partitioner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_partitions&#34;&gt;4.7. partitions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_persist_cache&#34;&gt;4.8. persist, cache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_pipe&#34;&gt;4.9. pipe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_randomsplit&#34;&gt;4.10. randomSplit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_reduce&#34;&gt;4.11. reduce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_reducebykey_pair_reducebykeylocally_pair_reducebykeytodriver_pair&#34;&gt;4.12. reduceByKey [Pair],  reduceByKeyLocally [Pair], reduceByKeyToDriver [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_repartition&#34;&gt;4.13. repartition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_repartitionandsortwithinpartitions_ordered&#34;&gt;4.14. repartitionAndSortWithinPartitions [Ordered]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_rightouterjoin_pair&#34;&gt;4.15. rightOuterJoin [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sample&#34;&gt;4.16. sample&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_samplebykey_pair&#34;&gt;4.17. sampleByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_samplebykeyexact_pair_experimental&#34;&gt;4.18. sampleByKeyExact [Pair, experimental]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_saveashadoopfile_pair_saveashadoopdataset_pair_saveasnewapihadoopfile_pair&#34;&gt;4.19. saveAsHadoopFile [Pair], saveAsHadoopDataset [Pair], saveAsNewAPIHadoopFile [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_saveasobjectfile&#34;&gt;4.20. saveAsObjectFile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_saveassequencefile_seqfile&#34;&gt;4.21. saveAsSequenceFile [SeqFile]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_saveastextfile&#34;&gt;4.22. saveAsTextFile&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_part5&#34;&gt;5. part5&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_stats_double&#34;&gt;5.1. stats [Double]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sortby&#34;&gt;5.2. sortBy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sortbykey_ordered&#34;&gt;5.3. sortByKey [Ordered]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_subtract&#34;&gt;5.4. subtract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_subtractbykey_pair&#34;&gt;5.5. subtractByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sum_double_sumapprox_double&#34;&gt;5.6. sum [Double], sumApprox [Double]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_take&#34;&gt;5.7. take&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_takeordered&#34;&gt;5.8. takeOrdered&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_takesample&#34;&gt;5.9. takeSample&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_todebugstring&#34;&gt;5.10. toDebugString&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_tolocaliterator&#34;&gt;5.11. toLocalIterator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_top&#34;&gt;5.12. top&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_tostring&#34;&gt;5.13. toString&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_treeaggregate&#34;&gt;5.14. treeAggregate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_treereduce&#34;&gt;5.15. treeReduce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_union&#34;&gt;5.16. union, ++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_unpersist&#34;&gt;5.17. unpersist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_values&#34;&gt;5.18. values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_variance_double_samplevariance_double&#34;&gt;5.19. variance [Double], sampleVariance [Double]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_zip&#34;&gt;5.20. zip&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_zipwithindex&#34;&gt;5.21. zipWithIndex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_zipwithuniqueid&#34;&gt;5.22. zipWithUniqueId&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_part1&#34;&gt;1. part1&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_aggregate&#34;&gt;1.1. aggregate&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The aggregate function allows the user to apply two different reduce functions to the RDD. The first reduce function is applied within each partition to reduce the data within each partition into a single result. The second reduce function is used to combine the different reduced results of all partitions together to arrive at one final result. The ability to have two separate reduce functions for intra partition versus across partition reducing adds a lot of flexibility. For example the first reduce function can be the max function and the second one can be the sum function. The user also specifies an initial value. Here are some important facts.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;The initial value is applied at both levels of reduce. So both at the intra partition reduction and across partition reduction.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Both reduce functions have to be commutative and associative.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do not assume any execution order for either partition computations or combining partitions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why would one want to use two input data types? Let us assume we do an archaeological site survey using a metal detector. While walking through the site we take GPS coordinates of important findings based on the output of the metal detector. Later, we intend to draw an image of a map that highlights these locations using the aggregate function. In this case the zeroValue could be an area map with no highlights. The possibly huge set of input data is stored as GPS coordinates across many partitions. seqOp (first reducer) could convert the GPS coordinates to map coordinates and put a marker on the map at the respective position. combOp (second reducer) will receive these highlights as partial maps and combine them into a single final output map.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;&lt;strong&gt;Listing Variants&lt;/strong&gt; &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&amp;gt; U, combOp: (U, U) =&amp;gt; U): U&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Examples 1 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(1,2,3,4,5,6), 2)

// lets first print out the contents of the RDD with partition labels
def myfunc(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}

z.mapPartitionsWithIndex(myfunc).collect
res28: Array[String] = Array([partID:0, val: 1], [partID:0, val: 2], [partID:0, val: 3], [partID:1, val: 4], [partID:1, val: 5], [partID:1, val: 6])

z.aggregate(0)(math.max(_, _), _ + _)
res40: Int = 9

// This example returns 16 since the initial value is 5
// reduce of partition 0 will be max(5, 1, 2, 3) = 5
// reduce of partition 1 will be max(5, 4, 5, 6) = 6
// final reduce across partitions will be 5 + 5 + 6 = 16
// note the final reduce include the initial value
z.aggregate(5)(math.max(_, _), _ + _)
res29: Int = 16


val z = sc.parallelize(List(&#34;a&#34;,&#34;b&#34;,&#34;c&#34;,&#34;d&#34;,&#34;e&#34;,&#34;f&#34;),2)

//lets first print out the contents of the RDD with partition labels
def myfunc(index: Int, iter: Iterator[(String)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}

z.mapPartitionsWithIndex(myfunc).collect
res31: Array[String] = Array([partID:0, val: a], [partID:0, val: b], [partID:0, val: c], [partID:1, val: d], [partID:1, val: e], [partID:1, val: f])

z.aggregate(&#34;&#34;)(_ + _, _+_)
res115: String = abcdef

// See here how the initial value &#34;x&#34; is applied three times.
//  - once for each partition
//  - once when combining all the partitions in the second reduce function.
z.aggregate(&#34;x&#34;)(_ + _, _+_)
res116: String = xxdefxabc

// Below are some more advanced examples. Some are quite tricky to work out.

val z = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;345&#34;,&#34;4567&#34;),2)
z.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.max(x.length, y.length).toString, (x,y) =&amp;gt; x + y)
res141: String = 42

z.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.min(x.length, y.length).toString, (x,y) =&amp;gt; x + y)
res142: String = 11

val z = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;345&#34;,&#34;&#34;),2)
z.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.min(x.length, y.length).toString, (x,y) =&amp;gt; x + y)
res143: String = 10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;The main issue with the code above is that the result of the inner min is a string of length 1.
The zero in the output is due to the empty string being the last string in the list. We see this result because we are not recursively reducing any further within the partition for the final string.&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Examples 2 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;&#34;,&#34;345&#34;),2)
z.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.min(x.length, y.length).toString, (x,y) =&amp;gt; x + y)
res144: String = 11&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;In contrast to the previous example, this example has the empty string at the beginning of the second partition. This results in length of zero being input to the second reduce which then upgrades it a length of 1. (Warning: The above example shows bad design since the output is dependent on the order of the data inside the partitions.)&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_aggregatebykey_pair&#34;&gt;1.2. aggregateByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Works like the aggregate function except the aggregation is applied to the values with the same key. Also unlike the aggregate function the initial value is not applied to the second reduce.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def aggregateByKey[U](zeroValue: U)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
def aggregateByKey[U](zeroValue: U, numPartitions: Int)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
def aggregateByKey[U](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val pairRDD = sc.parallelize(List( (&#34;cat&#34;,2), (&#34;cat&#34;, 5), (&#34;mouse&#34;, 4),(&#34;cat&#34;, 12), (&#34;dog&#34;, 12), (&#34;mouse&#34;, 2)), 2)

// lets have a look at what is in the partitions
def myfunc(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
pairRDD.mapPartitionsWithIndex(myfunc).collect

res2: Array[String] = Array([partID:0, val: (cat,2)], [partID:0, val: (cat,5)], [partID:0, val: (mouse,4)], [partID:1, val: (cat,12)], [partID:1, val: (dog,12)], [partID:1, val: (mouse,2)])

pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect
res3: Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6))

pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).collect
res4: Array[(String, Int)] = Array((dog,100), (cat,200), (mouse,200))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_cartesian&#34;&gt;1.3. cartesian&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Computes the cartesian product between two RDDs (i.e. Each item of the first RDD is joined with each item of the second RDD) and returns them as a new RDD. (Warning: Be careful when using this function.! Memory consumption can quickly become an issue!)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(List(1,2,3,4,5))
val y = sc.parallelize(List(6,7,8,9,10))
x.cartesian(y).collect
res0: Array[(Int, Int)] = Array((1,6), (1,7), (1,8), (1,9), (1,10), (2,6), (2,7), (2,8), (2,9), (2,10), (3,6), (3,7), (3,8), (3,9), (3,10), (4,6), (5,6), (4,7), (5,7), (4,8), (5,8), (4,9), (4,10), (5,9), (5,10))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_checkpoint&#34;&gt;1.4. checkpoint&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Will create a checkpoint when the RDD is computed next. Checkpointed RDDs are stored as a binary file within the checkpoint directory which can be specified using the Spark context. (Warning: Spark applies lazy evaluation. Checkpointing will not occur until an action is invoked.)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Important note: the directory  &#34;my_directory_name&#34; should exist in all slaves. As an alternative you could use an HDFS directory URL as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def checkpoint()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sc.setCheckpointDir(&#34;my_directory_name&#34;)
val a = sc.parallelize(1 to 4)
a.checkpoint
a.count
14/02/25 18:13:53 INFO SparkContext: Starting job: count at &amp;lt;console&amp;gt;:15
...
14/02/25 18:13:53 INFO MemoryStore: Block broadcast_5 stored as values to memory (estimated size 115.7 KB, free 296.3 MB)
14/02/25 18:13:53 INFO RDDCheckpointData: Done checkpointing RDD 11 to file:/home/cloudera/Documents/spark-0.9.0-incubating-bin-cdh4/bin/my_directory_name/65407913-fdc6-4ec1-82c9-48a1656b95d6/rdd-11, new parent is RDD 12
res23: Long = 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_coalesce_repartition&#34;&gt;1.5. coalesce, repartition&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Coalesces the associated data into a given number of partitions. repartition(numPartitions) is simply an abbreviation for coalesce(numPartitions, shuffle = true).&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def coalesce ( numPartitions : Int , shuffle : Boolean = false ): RDD [T]
def repartition ( numPartitions : Int ): RDD [T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(1 to 10, 10)
val z = y.coalesce(2, false)
z.partitions.length
res9: Int = 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_cogroup_pair_groupwith_pair&#34;&gt;1.6. cogroup [Pair], groupWith [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A very powerful set of functions that allow grouping up to 3 key-value RDDs together using their keys.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]
def cogroup[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Iterable[V], Iterable[W]))]
def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Iterable[V], Iterable[W]))]
def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)]): RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]
def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)], numPartitions: Int): RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]
def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)], partitioner: Partitioner): RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]
def groupWith[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]
def groupWith[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)]): RDD[(K, (Iterable[V], IterableW1], Iterable[W2]))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Examples &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1, 2, 1, 3), 1)
val b = a.map((_, &#34;b&#34;))
val c = a.map((_, &#34;c&#34;))
b.cogroup(c).collect
res7: Array[(Int, (Iterable[String], Iterable[String]))] = Array(
(2,(ArrayBuffer(b),ArrayBuffer(c))),
(3,(ArrayBuffer(b),ArrayBuffer(c))),
(1,(ArrayBuffer(b, b),ArrayBuffer(c, c)))
)

val d = a.map((_, &#34;d&#34;))
b.cogroup(c, d).collect
res9: Array[(Int, (Iterable[String], Iterable[String], Iterable[String]))] = Array(
(2,(ArrayBuffer(b),ArrayBuffer(c),ArrayBuffer(d))),
(3,(ArrayBuffer(b),ArrayBuffer(c),ArrayBuffer(d))),
(1,(ArrayBuffer(b, b),ArrayBuffer(c, c),ArrayBuffer(d, d)))
)

val x = sc.parallelize(List((1, &#34;apple&#34;), (2, &#34;banana&#34;), (3, &#34;orange&#34;), (4, &#34;kiwi&#34;)), 2)
val y = sc.parallelize(List((5, &#34;computer&#34;), (1, &#34;laptop&#34;), (1, &#34;desktop&#34;), (4, &#34;iPad&#34;)), 2)
x.cogroup(y).collect
res23: Array[(Int, (Iterable[String], Iterable[String]))] = Array(
(4,(ArrayBuffer(kiwi),ArrayBuffer(iPad))),
(2,(ArrayBuffer(banana),ArrayBuffer())),
(3,(ArrayBuffer(orange),ArrayBuffer())),
(1,(ArrayBuffer(apple),ArrayBuffer(laptop, desktop))),
(5,(ArrayBuffer(),ArrayBuffer(computer))))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_collect_toarray&#34;&gt;1.7. collect, toArray&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Converts the RDD into a Scala array and returns it. If you provide a standard map-function (i.e. f = T &amp;#8594; U) it will be applied before inserting the values into the result array.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def collect(): Array[T]
def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U]
def toArray(): Array[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;, &#34;Gnu&#34;, &#34;Rat&#34;), 2)
c.collect
res29: Array[String] = Array(Gnu, Cat, Rat, Dog, Gnu, Rat)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_collectasmap_pair&#34;&gt;1.8. collectAsMap [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to collect, but works on key-value RDDs and converts them into Scala maps to preserve their key-value structure.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def collectAsMap(): Map[K, V]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1, 2, 1, 3), 1)
val b = a.zip(a)
b.collectAsMap
res1: scala.collection.Map[Int,Int] = Map(2 -&amp;gt; 2, 1 -&amp;gt; 1, 3 -&amp;gt; 3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_combinebykey_pair&#34;&gt;1.9. combineByKey[Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very efficient implementation that combines the values of a RDD consisting of two-component tuples by applying multiple aggregators one after another.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def combineByKey[C](createCombiner: V =&amp;gt; C, mergeValue: (C, V) =&amp;gt; C, mergeCombiners: (C, C) =&amp;gt; C): RDD[(K, C)]
def combineByKey[C](createCombiner: V =&amp;gt; C, mergeValue: (C, V) =&amp;gt; C, mergeCombiners: (C, C) =&amp;gt; C, numPartitions: Int): RDD[(K, C)]
def combineByKey[C](createCombiner: V =&amp;gt; C, mergeValue: (C, V) =&amp;gt; C, mergeCombiners: (C, C) =&amp;gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializerClass: String = null): RDD[(K, C)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;,&#34;cat&#34;,&#34;gnu&#34;,&#34;salmon&#34;,&#34;rabbit&#34;,&#34;turkey&#34;,&#34;wolf&#34;,&#34;bear&#34;,&#34;bee&#34;), 3)
val b = sc.parallelize(List(1,1,2,2,2,1,2,2,2), 3)
val c = b.zip(a)
val d = c.combineByKey(List(_), (x:List[String], y:String) =&amp;gt; y :: x, (x:List[String], y:List[String]) =&amp;gt; x ::: y)
d.collect
res16: Array[(Int, List[String])] = Array((1,List(cat, dog, turkey)), (2,List(gnu, rabbit, salmon, bee, bear, wolf)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_compute&#34;&gt;1.10. compute&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Executes dependencies and computes the actual representation of the RDD. This function should not be called directly by users.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def compute(split: Partition, context: TaskContext): Iterator[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_context_sparkcontext&#34;&gt;1.11. context, sparkContext&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the SparkContext that was used to create the RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def compute(split: Partition, context: TaskContext): Iterator[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;), 2)
c.context
res8: org.apache.spark.SparkContext = org.apache.spark.SparkContext@58c1c2f1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_count&#34;&gt;1.12. count&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the number of items stored within a RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def count(): Long&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;), 2)
c.count
res2: Long = 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countapprox&#34;&gt;1.13. countApprox&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Marked as experimental feature! Experimental features are currently not covered by this document!&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def (timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countapproxdistinct&#34;&gt;1.14. countApproxDistinct&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Computes the approximate number of distinct values. For large RDDs which are spread across many nodes, this function may execute faster than other counting methods. The parameter relativeSD controls the accuracy of the computation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countApproxDistinct(relativeSD: Double = 0.05): Long&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 10000, 20)
val b = a++a++a++a++a
b.countApproxDistinct(0.1)
res14: Long = 8224

b.countApproxDistinct(0.05)
res15: Long = 9750

b.countApproxDistinct(0.01)
res16: Long = 9947

b.countApproxDistinct(0.001)
res0: Long = 10000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countapproxdistinctbykey_pair&#34;&gt;1.15. countApproxDistinctByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to countApproxDistinct, but computes the approximate number of distinct values for each distinct key. Hence, the RDD must consist of two-component tuples. For large RDDs which are spread across many nodes, this function may execute faster than other counting methods. The parameter relativeSD controls the accuracy of the computation.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countApproxDistinctByKey(relativeSD: Double = 0.05): RDD[(K, Long)]
def countApproxDistinctByKey(relativeSD: Double, numPartitions: Int): RDD[(K, Long)]
def countApproxDistinctByKey(relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;), 2)
val b = sc.parallelize(a.takeSample(true, 10000, 0), 20)
val c = sc.parallelize(1 to b.count().toInt, 20)
val d = b.zip(c)
d.countApproxDistinctByKey(0.1).collect
res15: Array[(String, Long)] = Array((Rat,2567), (Cat,3357), (Dog,2414), (Gnu,2494))

d.countApproxDistinctByKey(0.01).collect
res16: Array[(String, Long)] = Array((Rat,2555), (Cat,2455), (Dog,2425), (Gnu,2513))

d.countApproxDistinctByKey(0.001).collect
res0: Array[(String, Long)] = Array((Rat,2562), (Cat,2464), (Dog,2451), (Gnu,2521))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_part2&#34;&gt;2. part2&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countbykey_pair&#34;&gt;2.1. countByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very similar to count, but counts the values of a RDD consisting of two-component tuples for each distinct key separately.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countByKey(): Map[K, Long]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List((3, &#34;Gnu&#34;), (3, &#34;Yak&#34;), (5, &#34;Mouse&#34;), (3, &#34;Dog&#34;)), 2)
c.countByKey
res3: scala.collection.Map[Int,Long] = Map(3 -&amp;gt; 3, 5 -&amp;gt; 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countbykeyapprox_pair&#34;&gt;2.2. countByKeyApprox [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Marked as experimental feature! Experimental features are currently not covered by this document!&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countByKeyApprox(timeout: Long, confidence: Double = 0.95): PartialResult[Map[K, BoundedDouble]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countbyvalue&#34;&gt;2.3. countByValue&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns a map that contains all unique values of the RDD and their respective occurrence counts. (Warning: This operation will finally aggregate the information in a single reducer.)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countByValue(): Map[T, Long]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1))
b.countByValue
res27: scala.collection.Map[Int,Long] = Map(5 -&amp;gt; 1, 8 -&amp;gt; 1, 3 -&amp;gt; 1, 6 -&amp;gt; 1, 1 -&amp;gt; 6, 2 -&amp;gt; 3, 4 -&amp;gt; 2, 7 -&amp;gt; 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countbyvalueapprox&#34;&gt;2.4. countByValueApprox&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Marked as experimental feature! Experimental features are currently not covered by this document!&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countByValueApprox(timeout: Long, confidence: Double = 0.95): PartialResult[Map[T, BoundedDouble]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_dependencies&#34;&gt;2.5. dependencies&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the RDD on which this RDD depends.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;final def dependencies: Seq[Dependency[_]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1))
b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[32] at parallelize at &amp;lt;console&amp;gt;:12
b.dependencies.length
Int = 0

b.map(a =&amp;gt; a).dependencies.length
res40: Int = 1

b.cartesian(a).dependencies.length
res41: Int = 2

b.cartesian(a).dependencies
res42: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.rdd.CartesianRDD$$anon$1@576ddaaa, org.apache.spark.rdd.CartesianRDD$$anon$2@6d2efbbd)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_distinct&#34;&gt;2.6. distinct&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns a new RDD that contains each unique value only once.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def distinct(): RDD[T]
def distinct(numPartitions: Int): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;, &#34;Gnu&#34;, &#34;Rat&#34;), 2)
c.distinct.collect
res6: Array[String] = Array(Dog, Gnu, Cat, Rat)

val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10))
a.distinct(2).partitions.length
res16: Int = 2

a.distinct(3).partitions.length
res17: Int = 3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_first&#34;&gt;2.7. first&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Looks for the very first data item of the RDD and returns it.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def first(): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;), 2)
c.first
res1: String = Gnu&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_filter&#34;&gt;2.8. filter&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Evaluates a boolean function for each data item of the RDD and puts the items for which the function returned true into the resulting RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def filter(f: T =&amp;gt; Boolean): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 10, 3)
val b = a.filter(_ % 2 == 0)
b.collect
res3: Array[Int] = Array(2, 4, 6, 8, 10)
When you provide a filter function, it must be able to handle all data items contained in the RDD. Scala provides so-called partial functions to deal with mixed data-types. (Tip: Partial functions are very useful if you have some data which may be bad and you do not want to handle but for the good data (matching data) you want to apply some kind of map function. The following article is good. It teaches you about partial functions in a very nice way and explains why case has to be used for partial functions:  article)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Examples for mixed data without partial functions  &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(1 to 8)
b.filter(_ &amp;lt; 4).collect
res15: Array[Int] = Array(1, 2, 3)

val a = sc.parallelize(List(&#34;cat&#34;, &#34;horse&#34;, 4.0, 3.5, 2, &#34;dog&#34;))
a.filter(_ &amp;lt; 4).collect
&amp;lt;console&amp;gt;:15: error: value &amp;lt; is not a member of Any&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This fails because some components of a are not implicitly comparable against integers. Collect uses the isDefinedAt property of a function-object to determine whether the test-function is compatible with each data item. Only data items that pass this test (=filter) are then mapped using the function-object.&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Examples for mixed data with partial functions &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;cat&#34;, &#34;horse&#34;, 4.0, 3.5, 2, &#34;dog&#34;))
a.collect({case a: Int    =&amp;gt; &#34;is integer&#34; |
           case b: String =&amp;gt; &#34;is string&#34; }).collect
res17: Array[String] = Array(is string, is string, is integer, is string)

val myfunc: PartialFunction[Any, Any] = {
  case a: Int    =&amp;gt; &#34;is integer&#34; |
  case b: String =&amp;gt; &#34;is string&#34; }
myfunc.isDefinedAt(&#34;&#34;)
res21: Boolean = true

myfunc.isDefinedAt(1)
res22: Boolean = true

myfunc.isDefinedAt(1.5)
res23: Boolean = false&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Be careful! The above code works because it only checks the type itself! If you use operations on this type, you have to explicitly declare what type you want instead of any. Otherwise the compiler does (apparently) not know what bytecode it should produce:&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val myfunc2: PartialFunction[Any, Any] = {case x if (x &amp;lt; 4) =&amp;gt; &#34;x&#34;}
&amp;lt;console&amp;gt;:10: error: value &amp;lt; is not a member of Any

val myfunc2: PartialFunction[Int, Any] = {case x if (x &amp;lt; 4) =&amp;gt; &#34;x&#34;}
myfunc2: PartialFunction[Int,Any] = &amp;lt;function1&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_filterbyrange_ordered&#34;&gt;2.9. filterByRange [Ordered]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns an RDD containing only the items in the key range specified. From our testing, it appears this only works if your data is in key value pairs and it has already been sorted by key.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def filterByRange(lower: K, upper: K): RDD[P]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val randRDD = sc.parallelize(List( (2,&#34;cat&#34;), (6, &#34;mouse&#34;),(7, &#34;cup&#34;), (3, &#34;book&#34;), (4, &#34;tv&#34;), (1, &#34;screen&#34;), (5, &#34;heater&#34;)), 3)
val sortedRDD = randRDD.sortByKey()

sortedRDD.filterByRange(1, 3).collect
res66: Array[(Int, String)] = Array((1,screen), (2,cat), (3,book))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_filterwith_deprecated&#34;&gt;2.10. filterWith  (deprecated)&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is an extended version of filter. It takes two function arguments. The first argument must conform to Int &amp;#8594; T and is executed once per partition. It will transform the partition index to type T. The second function looks like (U, T) &amp;#8594; Boolean. T is the transformed partition index and U are the data items from the RDD. Finally the function has to return either true or false (i.e. Apply the filter).&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def filterWith[A: ClassTag](constructA: Int =&amp;gt; A)(p: (T, A) =&amp;gt; Boolean): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
val b = a.filterWith(i =&amp;gt; i)((x,i) =&amp;gt; x % 2 == 0 || i % 2 == 0)
b.collect
res37: Array[Int] = Array(1, 2, 3, 4, 6, 7, 8, 9)

val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 5)
a.filterWith(x=&amp;gt; x)((a, b) =&amp;gt;  b == 0).collect
res30: Array[Int] = Array(1, 2)

a.filterWith(x=&amp;gt; x)((a, b) =&amp;gt;  a % (b+1) == 0).collect
res33: Array[Int] = Array(1, 2, 4, 6, 8, 10)

a.filterWith(x=&amp;gt; x.toString)((a, b) =&amp;gt;  b == &#34;2&#34;).collect
res34: Array[Int] = Array(5, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_flatmap&#34;&gt;2.11. flatMap&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to map, but allows emitting more than one item in the map function.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def flatMap[U: ClassTag](f: T =&amp;gt; TraversableOnce[U]): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 10, 5)
a.flatMap(1 to _).collect
res47: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

sc.parallelize(List(1, 2, 3), 2).flatMap(x =&amp;gt; List(x, x, x)).collect
res85: Array[Int] = Array(1, 1, 1, 2, 2, 2, 3, 3, 3)

// The program below generates a random number of copies (up to 10) of the items in the list.
val x  = sc.parallelize(1 to 10, 3)
x.flatMap(List.fill(scala.util.Random.nextInt(10))(_)).collect

res1: Array[Int] = Array(1, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_flatmapvalues&#34;&gt;2.12. flatMapValues&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very similar to mapValues, but collapses the inherent structure of the values during mapping.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def flatMapValues[U](f: V =&amp;gt; TraversableOnce[U]): RDD[(K, U)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.flatMapValues(&#34;x&#34; + _ + &#34;x&#34;).collect
res6: Array[(Int, Char)] = Array((3,x), (3,d), (3,o), (3,g), (3,x), (5,x), (5,t), (5,i), (5,g), (5,e), (5,r), (5,x), (4,x), (4,l), (4,i), (4,o), (4,n), (4,x), (3,x), (3,c), (3,a), (3,t), (3,x), (7,x), (7,p), (7,a), (7,n), (7,t), (7,h), (7,e), (7,r), (7,x), (5,x), (5,e), (5,a), (5,g), (5,l), (5,e), (5,x))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_flatmapwith_deprecated&#34;&gt;2.13. flatMapWith (deprecated)&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to flatMap, but allows accessing the partition index or a derivative of the partition index from within the flatMap-function.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def flatMapWith[A: ClassTag, U: ClassTag](constructA: Int =&amp;gt; A, preservesPartitioning: Boolean = false)(f: (T, A) =&amp;gt; Seq[U]): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 3)
a.flatMapWith(x =&amp;gt; x, true)((x, y) =&amp;gt; List(y, x)).collect
res58: Array[Int] = Array(0, 1, 0, 2, 0, 3, 1, 4, 1, 5, 1, 6, 2, 7, 2, 8, 2, 9)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_fold&#34;&gt;2.14. fold&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Aggregates the values of each partition. The aggregation variable within each partition is initialized with zeroValue.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def fold(zeroValue: T)(op: (T, T) =&amp;gt; T): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1,2,3), 3)
a.fold(0)(_ + _)
res59: Int = 6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_foldbykey_pair&#34;&gt;2.15. foldByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very similar to fold, but performs the folding separately for each key of the RDD. This function is only available if the RDD consists of two-component tuples.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def foldByKey(zeroValue: V)(func: (V, V) =&amp;gt; V): RDD[(K, V)]
def foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) =&amp;gt; V): RDD[(K, V)]
def foldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) =&amp;gt; V): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;cat&#34;, &#34;owl&#34;, &#34;gnu&#34;, &#34;ant&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.foldByKey(&#34;&#34;)(_ + _).collect
res84: Array[(Int, String)] = Array((3,dogcatowlgnuant)

val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.foldByKey(&#34;&#34;)(_ + _).collect
res85: Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_foreach&#34;&gt;2.16. foreach&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Executes an parameterless function for each data item.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def foreach(f: T =&amp;gt; Unit)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;cat&#34;, &#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;gnu&#34;, &#34;crocodile&#34;, &#34;ant&#34;, &#34;whale&#34;, &#34;dolphin&#34;, &#34;spider&#34;), 3)
c.foreach(x =&amp;gt; println(x + &#34;s are yummy&#34;))
lions are yummy
gnus are yummy
crocodiles are yummy
ants are yummy
whales are yummy
dolphins are yummy
spiders are yummy&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_foreachpartition&#34;&gt;2.17. foreachPartition&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Executes an parameterless function for each partition. Access to the data items contained in the partition is provided via the iterator argument.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def foreachPartition(f: Iterator[T] =&amp;gt; Unit)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)
b.foreachPartition(x =&amp;gt; println(x.reduce(_ + _)))
6
15
24&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_foreachwith_deprecated&#34;&gt;2.18. foreachWith (Deprecated)&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Executes an parameterless function for each partition. Access to the data items contained in the partition is provided via the iterator argument.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def foreachWith[A: ClassTag](constructA: Int =&amp;gt; A)(f: (T, A) =&amp;gt; Unit)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
a.foreachWith(i =&amp;gt; i)((x,i) =&amp;gt; if (x % 2 == 1 &amp;amp;&amp;amp; i % 2 == 0) println(x) )
1
3
7
9&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_fullouterjoin_pair&#34;&gt;2.19. fullOuterJoin [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs the full outer join between two paired RDDs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def fullOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Option[V], Option[W]))]
def fullOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], Option[W]))]
def fullOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Option[V], Option[W]))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val pairRDD1 = sc.parallelize(List( (&#34;cat&#34;,2), (&#34;cat&#34;, 5), (&#34;book&#34;, 4),(&#34;cat&#34;, 12)))
val pairRDD2 = sc.parallelize(List( (&#34;cat&#34;,2), (&#34;cup&#34;, 5), (&#34;mouse&#34;, 4),(&#34;cat&#34;, 12)))
pairRDD1.fullOuterJoin(pairRDD2).collect

res5: Array[(String, (Option[Int], Option[Int]))] = Array((book,(Some(4),None)), (mouse,(None,Some(4))), (cup,(None,Some(5))), (cat,(Some(2),Some(2))), (cat,(Some(2),Some(12))), (cat,(Some(5),Some(2))), (cat,(Some(5),Some(12))), (cat,(Some(12),Some(2))), (cat,(Some(12),Some(12))))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_part3&#34;&gt;3. part3&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_generator_setgenerator&#34;&gt;3.1. generator, setGenerator&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Allows setting a string that is attached to the end of the RDD&amp;#8217;s name when printing the dependency graph.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;@transient var generator
def setGenerator(_generator: String)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_getcheckpointfile&#34;&gt;3.2. getCheckpointFile&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the path to the checkpoint file or null if RDD has not yet been checkpointed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def getCheckpointFile: Option[String]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sc.setCheckpointDir(&#34;/home/cloudera/Documents&#34;)
val a = sc.parallelize(1 to 500, 5)
val b = a++a++a++a++a
b.getCheckpointFile
res49: Option[String] = None

b.checkpoint
b.getCheckpointFile
res54: Option[String] = None

b.collect
b.getCheckpointFile
res57: Option[String] = Some(file:/home/cloudera/Documents/cb978ffb-a346-4820-b3ba-d56580787b20/rdd-40)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_preferredlocations&#34;&gt;3.3. preferredLocations&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the hosts which are preferred by this RDD. The actual preference of a specific host depends on various assumptions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;final def preferredLocations(split: Partition): Seq[String]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_getstoragelevel&#34;&gt;3.4. getStorageLevel&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Retrieves the currently set storage level of the RDD. This can only be used to assign a new storage level if the RDD does not have a storage level set yet. The example &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;below shows the error you will get, when you try to reassign the storage level.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def getStorageLevel&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 100000, 2)
a.persist(org.apache.spark.storage.StorageLevel.DISK_ONLY)
a.getStorageLevel.description
String = Disk Serialized 1x Replicated

a.cache
java.lang.UnsupportedOperationException: Cannot change storage level of an RDD after it was already assigned a level&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_glom&#34;&gt;3.5. glom&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Assembles an array that contains all elements of the partition and embeds it in an RDD. Each returned array contains the contents of one partition.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def glom(): RDD[Array[T]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 100, 3)
a.glom.collect
res8: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33), Array(34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66), Array(67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_groupby&#34;&gt;3.6. groupBy&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def groupBy[K: ClassTag](f: T =&amp;gt; K): RDD[(K, Iterable[T])]
def groupBy[K: ClassTag](f: T =&amp;gt; K, numPartitions: Int): RDD[(K, Iterable[T])]
def groupBy[K: ClassTag](f: T =&amp;gt; K, p: Partitioner): RDD[(K, Iterable[T])]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
a.groupBy(x =&amp;gt; { if (x % 2 == 0) &#34;even&#34; else &#34;odd&#34; }).collect
res42: Array[(String, Seq[Int])] = Array((even,ArrayBuffer(2, 4, 6, 8)), (odd,ArrayBuffer(1, 3, 5, 7, 9)))

val a = sc.parallelize(1 to 9, 3)
def myfunc(a: Int) : Int =
{
  a % 2
}
a.groupBy(myfunc).collect
res3: Array[(Int, Seq[Int])] = Array((0,ArrayBuffer(2, 4, 6, 8)), (1,ArrayBuffer(1, 3, 5, 7, 9)))

val a = sc.parallelize(1 to 9, 3)
def myfunc(a: Int) : Int =
{
  a % 2
}
a.groupBy(x =&amp;gt; myfunc(x), 3).collect
a.groupBy(myfunc(_), 1).collect
res7: Array[(Int, Seq[Int])] = Array((0,ArrayBuffer(2, 4, 6, 8)), (1,ArrayBuffer(1, 3, 5, 7, 9)))

import org.apache.spark.Partitioner
class MyPartitioner extends Partitioner {
def numPartitions: Int = 2
def getPartition(key: Any): Int =
{
    key match
    {
      case null     =&amp;gt; 0
      case key: Int =&amp;gt; key          % numPartitions
      case _        =&amp;gt; key.hashCode % numPartitions
    }
  }
  override def equals(other: Any): Boolean =
  {
    other match
    {
      case h: MyPartitioner =&amp;gt; true
      case _                =&amp;gt; false
    }
  }
}
val a = sc.parallelize(1 to 9, 3)
val p = new MyPartitioner()
val b = a.groupBy((x:Int) =&amp;gt; { x }, p)
val c = b.mapWith(i =&amp;gt; i)((a, b) =&amp;gt; (b, a))
c.collect
res42: Array[(Int, (Int, Seq[Int]))] = Array((0,(4,ArrayBuffer(4))), (0,(2,ArrayBuffer(2))), (0,(6,ArrayBuffer(6))), (0,(8,ArrayBuffer(8))), (1,(9,ArrayBuffer(9))), (1,(3,ArrayBuffer(3))), (1,(1,ArrayBuffer(1))), (1,(7,ArrayBuffer(7))), (1,(5,ArrayBuffer(5))))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_groupbykey_pair&#34;&gt;3.7. groupByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very similar to groupBy, but instead of supplying a function, the key-component of each pair will automatically be presented to the partitioner.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def groupByKey(): RDD[(K, Iterable[V])]
def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;spider&#34;, &#34;eagle&#34;), 2)
val b = a.keyBy(_.length)
b.groupByKey.collect
res11: Array[(Int, Seq[String])] = Array((4,ArrayBuffer(lion)), (6,ArrayBuffer(spider)), (3,ArrayBuffer(dog, cat)), (5,ArrayBuffer(tiger, eagle)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_histogram_double&#34;&gt;3.8. histogram [Double]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;These functions take an RDD of doubles and create a histogram with either even spacing (the number of buckets equals to bucketCount) or arbitrary spacing based on  custom bucket boundaries supplied by the user via an array of double values. The result type of both variants is slightly different, the first function will return a tuple consisting of two arrays. The first array contains the computed bucket boundary values and the second array contains the corresponding count of values (i.e. the histogram). The second variant of the function will just return the histogram as an array of integers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def histogram(bucketCount: Int): Pair[Array[Double], Array[Long]]
def histogram(buckets: Array[Double], evenBuckets: Boolean = false): Array[Long]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example  with even spacing &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1.1, 1.2, 1.3, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 9.0), 3)
a.histogram(5)
res11: (Array[Double], Array[Long]) = (Array(1.1, 2.68, 4.26, 5.84, 7.42, 9.0),Array(5, 0, 0, 1, 4))

val a = sc.parallelize(List(9.1, 1.0, 1.2, 2.1, 1.3, 5.0, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 10.0, 8.9, 5.5), 3)
a.histogram(6)
res18: (Array[Double], Array[Long]) = (Array(1.0, 2.5, 4.0, 5.5, 7.0, 8.5, 10.0),Array(6, 0, 1, 1, 3, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example with custom spacing &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1.1, 1.2, 1.3, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 9.0), 3)
a.histogram(Array(0.0, 3.0, 8.0))
res14: Array[Long] = Array(5, 3)

val a = sc.parallelize(List(9.1, 1.0, 1.2, 2.1, 1.3, 5.0, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 10.0, 8.9, 5.5), 3)
a.histogram(Array(0.0, 5.0, 10.0))
res1: Array[Long] = Array(6, 9)

a.histogram(Array(0.0, 5.0, 10.0, 15.0))
res1: Array[Long] = Array(6, 8, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_id&#34;&gt;3.9. id&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Retrieves the ID which has been assigned to the RDD by its device context.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val id: Int&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(1 to 10, 10)
y.id
res16: Int = 19&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_intersection&#34;&gt;3.10. intersection&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the elements in the two RDDs which are the same.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def intersection(other: RDD[T], numPartitions: Int): RDD[T]
def intersection(other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]
def intersection(other: RDD[T]): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(1 to 20)
val y = sc.parallelize(10 to 30)
val z = x.intersection(y)

z.collect
res74: Array[Int] = Array(16, 12, 20, 13, 17, 14, 18, 10, 19, 15, 11)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_ischeckpointed&#34;&gt;3.11. isCheckpointed&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Indicates whether the RDD has been checkpointed. The flag will only raise once the checkpoint has really been created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def isCheckpointed: Boolean&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sc.setCheckpointDir(&#34;/home/cloudera/Documents&#34;)
c.isCheckpointed
res6: Boolean = false

c.checkpoint
c.isCheckpointed
res8: Boolean = false

c.collect
c.isCheckpointed
res9: Boolean = true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_iterator&#34;&gt;3.12. iterator&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns a compatible iterator object for a partition of this RDD. This function should never be called directly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;final def iterator(split: Partition, context: TaskContext): Iterator[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_join_pair&#34;&gt;3.13. join [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs an inner join using two key-value RDDs. Please note that the keys must be generally comparable to make this work.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
def join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]
def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val b = a.keyBy(_.length)
val c = sc.parallelize(List(&#34;dog&#34;,&#34;cat&#34;,&#34;gnu&#34;,&#34;salmon&#34;,&#34;rabbit&#34;,&#34;turkey&#34;,&#34;wolf&#34;,&#34;bear&#34;,&#34;bee&#34;), 3)
val d = c.keyBy(_.length)
b.join(d).collect

res0: Array[(Int, (String, String))] = Array((6,(salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (6,(salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (3,(dog,dog)), (3,(dog,cat)), (3,(dog,gnu)), (3,(dog,bee)), (3,(rat,dog)), (3,(rat,cat)), (3,(rat,gnu)), (3,(rat,bee)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_keyby&#34;&gt;3.14. keyBy&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Constructs two-component tuples (key-value pairs) by applying a function on each data item. The result of the function becomes the key and the original data item becomes the value of the newly created tuples.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def keyBy[K](f: T =&amp;gt; K): RDD[(K, T)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val b = a.keyBy(_.length)
b.collect
res26: Array[(Int, String)] = Array((3,dog), (6,salmon), (6,salmon), (3,rat), (8,elephant))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_keys_pair&#34;&gt;3.15. keys [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Extracts the keys from all contained tuples and returns them in a new RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def keys: RDD[K]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.keys.collect
res2: Array[Int] = Array(3, 5, 4, 3, 7, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_leftouterjoin_pair&#34;&gt;3.16. leftOuterJoin [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs an left outer join using two key-value RDDs. Please note that the keys must be generally comparable to make this work correctly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]
def leftOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, Option[W]))]
def leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val b = a.keyBy(_.length)
val c = sc.parallelize(List(&#34;dog&#34;,&#34;cat&#34;,&#34;gnu&#34;,&#34;salmon&#34;,&#34;rabbit&#34;,&#34;turkey&#34;,&#34;wolf&#34;,&#34;bear&#34;,&#34;bee&#34;), 3)
val d = c.keyBy(_.length)
b.leftOuterJoin(d).collect

res1: Array[(Int, (String, Option[String]))] = Array((6,(salmon,Some(salmon))), (6,(salmon,Some(rabbit))), (6,(salmon,Some(turkey))), (6,(salmon,Some(salmon))), (6,(salmon,Some(rabbit))), (6,(salmon,Some(turkey))), (3,(dog,Some(dog))), (3,(dog,Some(cat))), (3,(dog,Some(gnu))), (3,(dog,Some(bee))), (3,(rat,Some(dog))), (3,(rat,Some(cat))), (3,(rat,Some(gnu))), (3,(rat,Some(bee))), (8,(elephant,None)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_lookup&#34;&gt;3.17. lookup&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scans the RDD for all keys that match the provided value and returns their values as a Scala sequence.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def lookup(key: K): Seq[V]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.lookup(5)
res0: Seq[String] = WrappedArray(tiger, eagle)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_map&#34;&gt;3.18. map&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Applies a transformation function on each item of the RDD and returns the result as a new RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def map[U: ClassTag](f: T =&amp;gt; U): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val b = a.map(_.length)
val c = a.zip(b)
c.collect
res0: Array[(String, Int)] = Array((dog,3), (salmon,6), (salmon,6), (rat,3), (elephant,8))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mappartitions&#34;&gt;3.19. mapPartitions&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is a specialized map that is called only once for each partition. The entire content of the respective partitions is available as a sequential stream of values via the input argument (Iterarator[T]). The custom function must return yet another Iterator[U]. The combined result iterators are automatically converted into a new RDD. Please note, that the tuples (3,4) and (6,7) are missing from the following result due to the partitioning we chose.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapPartitions[U: ClassTag](f: Iterator[T] =&amp;gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example 1&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
def myfunc[T](iter: Iterator[T]) : Iterator[(T, T)] = {
  var res = List[(T, T)]()
  var pre = iter.next
  while (iter.hasNext)
  {
    val cur = iter.next;
    res .::= (pre, cur)
    pre = cur;
  }
  res.iterator
}
a.mapPartitions(myfunc).collect
res0: Array[(Int, Int)] = Array((2,3), (1,2), (5,6), (4,5), (8,9), (7,8))
Example :: 2

val x = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9,10), 3)
def myfunc(iter: Iterator[Int]) : Iterator[Int] = {
  var res = List[Int]()
  while (iter.hasNext) {
    val cur = iter.next;
    res = res ::: List.fill(scala.util.Random.nextInt(10))(cur)
  }
  res.iterator
}
x.mapPartitions(myfunc).collect
// some of the number are not outputted at all. This is because the random number generated for it is zero.
res8: Array[Int] = Array(1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 7, 7, 7, 9, 9, 10)
The above program can also be written using flatMap as follows.

Example :: 2 using flatmap

val x  = sc.parallelize(1 to 10, 3)
x.flatMap(List.fill(scala.util.Random.nextInt(10))(_)).collect

res1: Array[Int] = Array(1, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mappartitionswithcontext_deprecated_and_developer_api&#34;&gt;3.20. mapPartitionsWithContext   (deprecated and developer API)&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to mapPartitions, but allows accessing information about the processing state within the mapper.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapPartitionsWithContext[U: ClassTag](f: (TaskContext, Iterator[T]) =&amp;gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]

Example ::

val a = sc.parallelize(1 to 9, 3)
import org.apache.spark.TaskContext
def myfunc(tc: TaskContext, iter: Iterator[Int]) : Iterator[Int] = {
  tc.addOnCompleteCallback(() =&amp;gt; println(
    &#34;Partition: &#34;     + tc.partitionId +
    &#34;, AttemptID: &#34;   + tc.attemptId ))

  iter.toList.filter(_ % 2 == 0).iterator
}
a.mapPartitionsWithContext(myfunc).collect

14/04/01 23:05:48 INFO SparkContext: Starting job: collect at &amp;lt;console&amp;gt;:20
...
14/04/01 23:05:48 INFO Executor: Running task ID 0
Partition: 0, AttemptID: 0, Interrupted: false
...
14/04/01 23:05:48 INFO Executor: Running task ID 1
14/04/01 23:05:48 INFO TaskSetManager: Finished TID 0 in 470 ms on localhost (progress: 0/3)
...
14/04/01 23:05:48 INFO Executor: Running task ID 2
14/04/01 23:05:48 INFO TaskSetManager: Finished TID 1 in 23 ms on localhost (progress: 1/3)
14/04/01 23:05:48 INFO DAGScheduler: Completed ResultTask(0, 1)

?
res0: Array[Int] = Array(2, 6, 4, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mappartitionswithindex&#34;&gt;3.21. mapPartitionsWithIndex&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to mapPartitions, but takes two parameters. The first parameter is the index of the partition and the second is an iterator through all the items within this partition. The output is an iterator containing the list of items after applying whatever transformation the function encodes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapPartitionsWithIndex[U: ClassTag](f: (Int, Iterator[T]) =&amp;gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 3)
def myfunc(index: Int, iter: Iterator[Int]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; index + &#34;,&#34; + x).iterator
}
x.mapPartitionsWithIndex(myfunc).collect()
res10: Array[String] = Array(0,1, 0,2, 0,3, 1,4, 1,5, 1,6, 2,7, 2,8, 2,9, 2,10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mappartitionswithsplit&#34;&gt;3.22. mapPartitionsWithSplit&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This method has been marked as deprecated in the API. So, you should not use this method anymore. Deprecated methods will not be covered in this document.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapPartitionsWithSplit[U: ClassTag](f: (Int, Iterator[T]) =&amp;gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mapvalues_pair&#34;&gt;3.23. mapValues [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Takes the values of a RDD that consists of two-component tuples, and applies the provided function to transform each value. Then, it forms new two-component tuples using the key and the transformed value and stores them in a new RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapValues[U](f: V =&amp;gt; U): RDD[(K, U)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.mapValues(&#34;x&#34; + _ + &#34;x&#34;).collect
res5: Array[(Int, String)] = Array((3,xdogx), (5,xtigerx), (4,xlionx), (3,xcatx), (7,xpantherx), (5,xeaglex))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mapwith_deprecated&#34;&gt;3.24. mapWith  (deprecated)&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is an extended version of map. It takes two function arguments. The first argument must conform to Int &amp;#8594; T and is executed once per partition. It will map the partition index to some transformed partition index of type T. This is where it is nice to do some kind of initialization code once per partition. Like create a Random number generator object. The second function must conform to (U, T) &amp;#8594; U. T is the transformed partition index and U is a data item of the RDD. Finally the function has to return a transformed data item of type U.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapWith[A: ClassTag, U: ClassTag](constructA: Int =&amp;gt; A, preservesPartitioning: Boolean = false)(f: (T, A) =&amp;gt; U): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;// generates 9 random numbers less than 1000.
val x = sc.parallelize(1 to 9, 3)
x.mapWith(a =&amp;gt; new scala.util.Random)((x, r) =&amp;gt; r.nextInt(1000)).collect
res0: Array[Int] = Array(940, 51, 779, 742, 757, 982, 35, 800, 15)

val a = sc.parallelize(1 to 9, 3)
val b = a.mapWith(&#34;Index:&#34; + _)((a, b) =&amp;gt; (&#34;Value:&#34; + a, b))
b.collect
res0: Array[(String, String)] = Array((Value:1,Index:0), (Value:2,Index:0), (Value:3,Index:0), (Value:4,Index:1), (Value:5,Index:1), (Value:6,Index:1), (Value:7,Index:2), (Value:8,Index:2), (Value:9,Index:2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_part4&#34;&gt;4. part4&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_max&#34;&gt;4.1. max&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the largest element in the RDD&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def max()(implicit ord: Ordering[T]): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(10 to 30)
y.max
res75: Int = 30

val a = sc.parallelize(List((10, &#34;dog&#34;), (3, &#34;tiger&#34;), (9, &#34;lion&#34;), (18, &#34;cat&#34;)))
a.max
res6: (Int, String) = (18,cat)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mean_double_meanapprox_double&#34;&gt;4.2. mean [Double], meanApprox [Double]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Calls stats and extracts the mean component. The approximate version of the function can finish somewhat faster in some scenarios. However, it trades accuracy for speed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mean(): Double
def meanApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(9.1, 1.0, 1.2, 2.1, 1.3, 5.0, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 10.0, 8.9, 5.5), 3)
a.mean
res0: Double = 5.3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_min&#34;&gt;4.3. min&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the smallest element in the RDD&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def min()(implicit ord: Ordering[T]): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(10 to 30)
y.min
res75: Int = 10


val a = sc.parallelize(List((10, &#34;dog&#34;), (3, &#34;tiger&#34;), (9, &#34;lion&#34;), (8, &#34;cat&#34;)))
a.min
res4: (Int, String) = (3,tiger)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_name_setname&#34;&gt;4.4. name, setName&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Allows a RDD to be tagged with a custom name.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;@transient var name: String
def setName(_name: String)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(1 to 10, 10)
y.name
res13: String = null
y.setName(&#34;Fancy RDD Name&#34;)
y.name
res15: String = Fancy RDD Name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_partitionby_pair&#34;&gt;4.5. partitionBy [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Repartitions as key-value RDD using its keys. The partitioner implementation can be supplied as the first argument.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def partitionBy(partitioner: Partitioner): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_partitioner&#34;&gt;4.6. partitioner&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Specifies a function pointer to the default partitioner that will be used for groupBy, subtract, reduceByKey (from PairedRDDFunctions), etc. functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;@transient val partitioner: Option[Partitioner]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_partitions&#34;&gt;4.7. partitions&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns an array of the partition objects associated with this RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;final def partitions: Array[Partition]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;, &#34;Gnu&#34;, &#34;Rat&#34;), 2)
b.partitions
res48: Array[org.apache.spark.Partition] = Array(org.apache.spark.rdd.ParallelCollectionPartition@18aa, org.apache.spark.rdd.ParallelCollectionPartition@18ab)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_persist_cache&#34;&gt;4.8. persist, cache&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;These functions can be used to adjust the storage level of a RDD. When freeing up memory, Spark will use the storage level identifier to decide which partitions should be kept. The parameterless variants persist() and cache() are just abbreviations for persist(StorageLevel.MEMORY_ONLY). (Warning: Once the storage level has been changed, it cannot be changed again!)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def cache(): RDD[T]
def persist(): RDD[T]
def persist(newLevel: StorageLevel): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;, &#34;Gnu&#34;, &#34;Rat&#34;), 2)
c.getStorageLevel
res0: org.apache.spark.storage.StorageLevel = StorageLevel(false, false, false, false, 1)
c.cache
c.getStorageLevel
res2: org.apache.spark.storage.StorageLevel = StorageLevel(false, true, false, true, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_pipe&#34;&gt;4.9. pipe&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Takes the RDD data of each partition and sends it via stdin to a shell-command. The resulting output of the command is captured and returned as a RDD of string values.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def pipe(command: String): RDD[String]
def pipe(command: String, env: Map[String, String]): RDD[String]
def pipe(command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String =&amp;gt; Unit) =&amp;gt; Unit = null, printRDDElement: (T, String =&amp;gt; Unit) =&amp;gt; Unit = null): RDD[String]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
a.pipe(&#34;head -n 1&#34;).collect
res2: Array[String] = Array(1, 4, 7)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_randomsplit&#34;&gt;4.10. randomSplit&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Randomly splits an RDD into multiple smaller RDDs according to a weights Array which specifies the percentage of the total data elements that is assigned to each smaller RDD. Note the actual size of each smaller RDD is only approximately equal to the percentages specified by the weights Array. The second example &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;below shows the number of items in each smaller RDD does not exactly match the weights Array.   A random optional seed can be specified. This function is useful for spliting data into a training set and a testing set for machine learning.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def randomSplit(weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(1 to 10)
val splits = y.randomSplit(Array(0.6, 0.4), seed = 11L)
val training = splits(0)
val test = splits(1)
training.collect
res:85 Array[Int] = Array(1, 4, 5, 6, 8, 10)
test.collect
res86: Array[Int] = Array(2, 3, 7, 9)

val y = sc.parallelize(1 to 10)
val splits = y.randomSplit(Array(0.1, 0.3, 0.6))

val rdd1 = splits(0)
val rdd2 = splits(1)
val rdd3 = splits(2)

rdd1.collect
res87: Array[Int] = Array(4, 10)
rdd2.collect
res88: Array[Int] = Array(1, 3, 5, 8)
rdd3.collect
res91: Array[Int] = Array(2, 6, 7, 9)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_reduce&#34;&gt;4.11. reduce&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This function provides the well-known reduce functionality in Spark. Please note that any function f you provide, should be commutative in order to generate reproducible results.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def reduce(f: (T, T) =&amp;gt; T): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 100, 3)
a.reduce(_ + _)
res41: Int = 5050&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_reducebykey_pair_reducebykeylocally_pair_reducebykeytodriver_pair&#34;&gt;4.12. reduceByKey [Pair],  reduceByKeyLocally [Pair], reduceByKeyToDriver [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This function provides the well-known reduce functionality in Spark. Please note that any function f you provide, should be commutative in order to generate reproducible results.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def reduceByKey(func: (V, V) =&amp;gt; V): RDD[(K, V)]
def reduceByKey(func: (V, V) =&amp;gt; V, numPartitions: Int): RDD[(K, V)]
def reduceByKey(partitioner: Partitioner, func: (V, V) =&amp;gt; V): RDD[(K, V)]
def reduceByKeyLocally(func: (V, V) =&amp;gt; V): Map[K, V]
def reduceByKeyToDriver(func: (V, V) =&amp;gt; V): Map[K, V]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;cat&#34;, &#34;owl&#34;, &#34;gnu&#34;, &#34;ant&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.reduceByKey(_ + _).collect
res86: Array[(Int, String)] = Array((3,dogcatowlgnuant))

val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.reduceByKey(_ + _).collect
res87: Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_repartition&#34;&gt;4.13. repartition&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This function changes the number of partitions to the number specified by the numPartitions parameter&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd = sc.parallelize(List(1, 2, 10, 4, 5, 2, 1, 1, 1), 3)
rdd.partitions.length
res2: Int = 3
val rdd2  = rdd.repartition(5)
rdd2.partitions.length
res6: Int = 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_repartitionandsortwithinpartitions_ordered&#34;&gt;4.14. repartitionAndSortWithinPartitions [Ordered]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def repartitionAndSortWithinPartitions(partitioner: Partitioner): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;// first we will do range partitioning which is not sorted
val randRDD = sc.parallelize(List( (2,&#34;cat&#34;), (6, &#34;mouse&#34;),(7, &#34;cup&#34;), (3, &#34;book&#34;), (4, &#34;tv&#34;), (1, &#34;screen&#34;), (5, &#34;heater&#34;)), 3)
val rPartitioner = new org.apache.spark.RangePartitioner(3, randRDD)
val partitioned = randRDD.partitionBy(rPartitioner)
def myfunc(index: Int, iter: Iterator[(Int, String)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
partitioned.mapPartitionsWithIndex(myfunc).collect

res0: Array[String] = Array([partID:0, val: (2,cat)], [partID:0, val: (3,book)], [partID:0, val: (1,screen)], [partID:1, val: (4,tv)], [partID:1, val: (5,heater)], [partID:2, val: (6,mouse)], [partID:2, val: (7,cup)])


// now lets repartition but this time have it sorted
val partitioned = randRDD.repartitionAndSortWithinPartitions(rPartitioner)
def myfunc(index: Int, iter: Iterator[(Int, String)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
partitioned.mapPartitionsWithIndex(myfunc).collect

res1: Array[String] = Array([partID:0, val: (1,screen)], [partID:0, val: (2,cat)], [partID:0, val: (3,book)], [partID:1, val: (4,tv)], [partID:1, val: (5,heater)], [partID:2, val: (6,mouse)], [partID:2, val: (7,cup)])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_rightouterjoin_pair&#34;&gt;4.15. rightOuterJoin [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs an right outer join using two key-value RDDs. Please note that the keys must be generally comparable to make this work correctly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def rightOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], W))]
def rightOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Option[V], W))]
def rightOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Option[V], W))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val b = a.keyBy(_.length)
val c = sc.parallelize(List(&#34;dog&#34;,&#34;cat&#34;,&#34;gnu&#34;,&#34;salmon&#34;,&#34;rabbit&#34;,&#34;turkey&#34;,&#34;wolf&#34;,&#34;bear&#34;,&#34;bee&#34;), 3)
val d = c.keyBy(_.length)
b.rightOuterJoin(d).collect

res2: Array[(Int, (Option[String], String))] = Array((6,(Some(salmon),salmon)), (6,(Some(salmon),rabbit)), (6,(Some(salmon),turkey)), (6,(Some(salmon),salmon)), (6,(Some(salmon),rabbit)), (6,(Some(salmon),turkey)), (3,(Some(dog),dog)), (3,(Some(dog),cat)), (3,(Some(dog),gnu)), (3,(Some(dog),bee)), (3,(Some(rat),dog)), (3,(Some(rat),cat)), (3,(Some(rat),gnu)), (3,(Some(rat),bee)), (4,(None,wolf)), (4,(None,bear)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sample&#34;&gt;4.16. sample&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Randomly selects a fraction of the items of a RDD and returns them in a new RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sample(withReplacement: Boolean, fraction: Double, seed: Int): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 10000, 3)
a.sample(false, 0.1, 0).count
res24: Long = 960

a.sample(true, 0.3, 0).count
res25: Long = 2888

a.sample(true, 0.3, 13).count
res26: Long = 2985&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_samplebykey_pair&#34;&gt;4.17. sampleByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Randomly samples the key value pair RDD according to the fraction of each key you want to appear in the final RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sampleByKey(withReplacement: Boolean, fractions: Map[K, Double], seed: Long = Utils.random.nextLong): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val randRDD = sc.parallelize(List( (7,&#34;cat&#34;), (6, &#34;mouse&#34;),(7, &#34;cup&#34;), (6, &#34;book&#34;), (7, &#34;tv&#34;), (6, &#34;screen&#34;), (7, &#34;heater&#34;)))
val sampleMap = List((7, 0.4), (6, 0.6)).toMap
randRDD.sampleByKey(false, sampleMap,42).collect

res6: Array[(Int, String)] = Array((7,cat), (6,mouse), (6,book), (6,screen), (7,heater))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_samplebykeyexact_pair_experimental&#34;&gt;4.18. sampleByKeyExact [Pair, experimental]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is labelled as experimental and so we do not document it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sampleByKeyExact(withReplacement: Boolean, fractions: Map[K, Double], seed: Long = Utils.random.nextLong): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_saveashadoopfile_pair_saveashadoopdataset_pair_saveasnewapihadoopfile_pair&#34;&gt;4.19. saveAsHadoopFile [Pair], saveAsHadoopDataset [Pair], saveAsNewAPIHadoopFile [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Saves the RDD in a Hadoop compatible format using any Hadoop outputFormat class the user specifies.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def saveAsHadoopDataset(conf: JobConf)
def saveAsHadoopFile[F &amp;lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F])
def saveAsHadoopFile[F &amp;lt;: OutputFormat[K, V]](path: String, codec: Class[_ &amp;lt;: CompressionCodec]) (implicit fm: ClassTag[F])
def saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &amp;lt;: OutputFormat[_, _]], codec: Class[_ &amp;lt;: CompressionCodec])
def saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &amp;lt;: OutputFormat[_, _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[_ &amp;lt;: CompressionCodec]] = None)
def saveAsNewAPIHadoopFile[F &amp;lt;: NewOutputFormat[K, V]](path: String)(implicit fm: ClassTag[F])
def saveAsNewAPIHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &amp;lt;: NewOutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_saveasobjectfile&#34;&gt;4.20. saveAsObjectFile&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Saves the RDD in binary format.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def saveAsObjectFile(path: String)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(1 to 100, 3)
x.saveAsObjectFile(&#34;objFile&#34;)
val y = sc.objectFile[Int](&#34;objFile&#34;)
y.collect
res52: Array[Int] =  Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_saveassequencefile_seqfile&#34;&gt;4.21. saveAsSequenceFile [SeqFile]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Saves the RDD as a Hadoop sequence file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def saveAsSequenceFile(path: String, codec: Option[Class[_ &amp;lt;: CompressionCodec]] = None)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val v = sc.parallelize(Array((&#34;owl&#34;,3), (&#34;gnu&#34;,4), (&#34;dog&#34;,1), (&#34;cat&#34;,2), (&#34;ant&#34;,5)), 2)
v.saveAsSequenceFile(&#34;hd_seq_file&#34;)
14/04/19 05:45:43 INFO FileOutputCommitter: Saved output of task &#39;attempt_201404190545_0000_m_000001_191&#39; to file:/home/cloudera/hd_seq_file

[cloudera@localhost ~]$ ll ~/hd_seq_file
total 8
-rwxr-xr-x 1 cloudera cloudera 117 Apr 19 05:45 part-00000
-rwxr-xr-x 1 cloudera cloudera 133 Apr 19 05:45 part-00001
-rwxr-xr-x 1 cloudera cloudera   0 Apr 19 05:45 _SUCCESS&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_saveastextfile&#34;&gt;4.22. saveAsTextFile&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Saves the RDD as text files. One line at a time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def saveAsTextFile(path: String)
def saveAsTextFile(path: String, codec: Class[_ &amp;lt;: CompressionCodec])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;without compression&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 10000, 3)
a.saveAsTextFile(&#34;mydata_a&#34;)
14/04/03 21:11:36 INFO FileOutputCommitter: Saved output of task &#39;attempt_201404032111_0000_m_000002_71&#39; to file:/home/cloudera/Documents/spark-0.9.0-incubating-bin-cdh4/bin/mydata_a


[cloudera@localhost ~]$ head -n 5 ~/Documents/spark-0.9.0-incubating-bin-cdh4/bin/mydata_a/part-00000
1
2
3
4
5

// Produces 3 output files since we have created the a RDD with 3 partitions
[cloudera@localhost ~]$ ll ~/Documents/spark-0.9.0-incubating-bin-cdh4/bin/mydata_a/
-rwxr-xr-x 1 cloudera cloudera 15558 Apr  3 21:11 part-00000
-rwxr-xr-x 1 cloudera cloudera 16665 Apr  3 21:11 part-00001
-rwxr-xr-x 1 cloudera cloudera 16671 Apr  3 21:11 part-00002

Example :: with compression

import org.apache.hadoop.io.compress.GzipCodec
a.saveAsTextFile(&#34;mydata_b&#34;, classOf[GzipCodec])

[cloudera@localhost ~]$ ll ~/Documents/spark-0.9.0-incubating-bin-cdh4/bin/mydata_b/
total 24
-rwxr-xr-x 1 cloudera cloudera 7276 Apr  3 21:29 part-00000.gz
-rwxr-xr-x 1 cloudera cloudera 6517 Apr  3 21:29 part-00001.gz
-rwxr-xr-x 1 cloudera cloudera 6525 Apr  3 21:29 part-00002.gz

val x = sc.textFile(&#34;mydata_b&#34;)
x.count
res2: Long = 10000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example writing into HDFS &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(List(1,2,3,4,5,6,6,7,9,8,10,21), 3)
x.saveAsTextFile(&#34;hdfs://localhost:8020/user/cloudera/test&#34;);

val sp = sc.textFile(&#34;hdfs://localhost:8020/user/cloudera/sp_data&#34;)
sp.flatMap(_.split(&#34; &#34;)).saveAsTextFile(&#34;hdfs://localhost:8020/user/cloudera/sp_x&#34;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_part5&#34;&gt;5. part5&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_stats_double&#34;&gt;5.1. stats [Double]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Simultaneously computes the mean, variance and the standard deviation of all values in the RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def stats(): StatCounter&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(List(1.0, 2.0, 3.0, 5.0, 20.0, 19.02, 19.29, 11.09, 21.0), 2)
x.stats
res16: org.apache.spark.util.StatCounter = (count: 9, mean: 11.266667, stdev: 8.126859)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sortby&#34;&gt;5.2. sortBy&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This function sorts the input RDD&amp;#8217;s data and stores it in a new RDD. The first parameter requires you to specify a function which  maps the input data into the key that you want to sortBy. The second parameter (optional) specifies whether you want the data to be sorted in ascending or descending order.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sortBy[K](f: (T) ⇒ K, ascending: Boolean = true, numPartitions: Int = this.partitions.size)(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(Array(5, 7, 1, 3, 2, 1))
y.sortBy(c =&amp;gt; c, true).collect
res101: Array[Int] = Array(1, 1, 2, 3, 5, 7)

y.sortBy(c =&amp;gt; c, false).collect
res102: Array[Int] = Array(7, 5, 3, 2, 1, 1)

val z = sc.parallelize(Array((&#34;H&#34;, 10), (&#34;A&#34;, 26), (&#34;Z&#34;, 1), (&#34;L&#34;, 5)))
z.sortBy(c =&amp;gt; c._1, true).collect
res109: Array[(String, Int)] = Array((A,26), (H,10), (L,5), (Z,1))

z.sortBy(c =&amp;gt; c._2, true).collect
res108: Array[(String, Int)] = Array((Z,1), (L,5), (H,10), (A,26))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sortbykey_ordered&#34;&gt;5.3. sortByKey [Ordered]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This function sorts the input RDD&amp;#8217;s data and stores it in a new RDD. The output RDD is a shuffled RDD because it stores data that is output by a reducer which has been shuffled. The implementation of this function is actually very clever. First, it uses a range partitioner to partition the data in ranges within the shuffled RDD. Then it sorts these ranges individually with mapPartitions using standard sort mechanisms.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.size): RDD[P]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;cat&#34;, &#34;owl&#34;, &#34;gnu&#34;, &#34;ant&#34;), 2)
val b = sc.parallelize(1 to a.count.toInt, 2)
val c = a.zip(b)
c.sortByKey(true).collect
res74: Array[(String, Int)] = Array((ant,5), (cat,2), (dog,1), (gnu,4), (owl,3))
c.sortByKey(false).collect
res75: Array[(String, Int)] = Array((owl,3), (gnu,4), (dog,1), (cat,2), (ant,5))

val a = sc.parallelize(1 to 100, 5)
val b = a.cartesian(a)
val c = sc.parallelize(b.takeSample(true, 5, 13), 2)
val d = c.sortByKey(false)
res56: Array[(Int, Int)] = Array((96,9), (84,76), (59,59), (53,65), (52,4))




stdev [Double], sampleStdev [Double]

Calls stats and extracts either stdev-component or corrected sampleStdev-component.

Listing Variants ::

def stdev(): Double
def sampleStdev(): Double&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val d = sc.parallelize(List(0.0, 0.0, 0.0), 3)
d.stdev
res10: Double = 0.0
d.sampleStdev
res11: Double = 0.0

val d = sc.parallelize(List(0.0, 1.0), 3)
d.stdev
d.sampleStdev
res18: Double = 0.5
res19: Double = 0.7071067811865476

val d = sc.parallelize(List(0.0, 0.0, 1.0), 3)
d.stdev
res14: Double = 0.4714045207910317
d.sampleStdev
res15: Double = 0.5773502691896257&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_subtract&#34;&gt;5.4. subtract&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs the well known standard set subtraction operation: A - B&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def subtract(other: RDD[T]): RDD[T]
def subtract(other: RDD[T], numPartitions: Int): RDD[T]
def subtract(other: RDD[T], p: Partitioner): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
val b = sc.parallelize(1 to 3, 3)
val c = a.subtract(b)
c.collect
res3: Array[Int] = Array(6, 9, 4, 7, 5, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_subtractbykey_pair&#34;&gt;5.5. subtractByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very similar to subtract, but instead of supplying a function, the key-component of each pair will be automatically used as criterion for removing items from the first RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def subtractByKey[W: ClassTag](other: RDD[(K, W)]): RDD[(K, V)]
def subtractByKey[W: ClassTag](other: RDD[(K, W)], numPartitions: Int): RDD[(K, V)]
def subtractByKey[W: ClassTag](other: RDD[(K, W)], p: Partitioner): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;spider&#34;, &#34;eagle&#34;), 2)
val b = a.keyBy(_.length)
val c = sc.parallelize(List(&#34;ant&#34;, &#34;falcon&#34;, &#34;squid&#34;), 2)
val d = c.keyBy(_.length)
b.subtractByKey(d).collect
res15: Array[(Int, String)] = Array((4,lion))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sum_double_sumapprox_double&#34;&gt;5.6. sum [Double], sumApprox [Double]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Computes the sum of all values contained in the RDD. The approximate version of the function can finish somewhat faster in some scenarios. However, it trades accuracy for speed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sum(): Double
def sumApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(List(1.0, 2.0, 3.0, 5.0, 20.0, 19.02, 19.29, 11.09, 21.0), 2)
x.sum
res17: Double = 101.39999999999999&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_take&#34;&gt;5.7. take&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Extracts the first n items of the RDD and returns them as an array. (Note: This sounds very easy, but it is actually quite a tricky problem for the implementors of Spark because the items in question can be in many different partitions.)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def take(num: Int): Array[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(&#34;dog&#34;, &#34;cat&#34;, &#34;ape&#34;, &#34;salmon&#34;, &#34;gnu&#34;), 2)
b.take(2)
res18: Array[String] = Array(dog, cat)

val b = sc.parallelize(1 to 10000, 5000)
b.take(100)
res6: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_takeordered&#34;&gt;5.8. takeOrdered&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Orders the data items of the RDD using their inherent implicit ordering function and returns the first n items as an array.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(&#34;dog&#34;, &#34;cat&#34;, &#34;ape&#34;, &#34;salmon&#34;, &#34;gnu&#34;), 2)
b.takeOrdered(2)
res19: Array[String] = Array(ape, cat)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_takesample&#34;&gt;5.9. takeSample&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Behaves different from sample in the following respects:
  It will return an exact number of samples (Hint: 2nd parameter)
  It returns an Array instead of RDD.
  It internally randomizes the order of the items returned.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def takeSample(withReplacement: Boolean, num: Int, seed: Int): Array[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(1 to 1000, 3)
x.takeSample(true, 100, 1)
res3: Array[Int] = Array(339, 718, 810, 105, 71, 268, 333, 360, 341, 300, 68, 848, 431, 449, 773, 172, 802, 339, 431, 285, 937, 301, 167, 69, 330, 864, 40, 645, 65, 349, 613, 468, 982, 314, 160, 675, 232, 794, 577, 571, 805, 317, 136, 860, 522, 45, 628, 178, 321, 482, 657, 114, 332, 728, 901, 290, 175, 876, 227, 130, 863, 773, 559, 301, 694, 460, 839, 952, 664, 851, 260, 729, 823, 880, 792, 964, 614, 821, 683, 364, 80, 875, 813, 951, 663, 344, 546, 918, 436, 451, 397, 670, 756, 512, 391, 70, 213, 896, 123, 858)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_todebugstring&#34;&gt;5.10. toDebugString&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns a string that contains debug information about the RDD and its dependencies.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def toDebugString: String&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
val b = sc.parallelize(1 to 3, 3)
val c = a.subtract(b)
c.toDebugString
res6: String =
MappedRDD[15] at subtract at &amp;lt;console&amp;gt;:16 (3 partitions)
  SubtractedRDD[14] at subtract at &amp;lt;console&amp;gt;:16 (3 partitions)
    MappedRDD[12] at subtract at &amp;lt;console&amp;gt;:16 (3 partitions)
      ParallelCollectionRDD[10] at parallelize at &amp;lt;console&amp;gt;:12 (3 partitions)
    MappedRDD[13] at subtract at &amp;lt;console&amp;gt;:16 (3 partitions)
      ParallelCollectionRDD[11] at parallelize at &amp;lt;console&amp;gt;:12 (3 partitions)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;toJavaRDD&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Embeds this RDD object within a JavaRDD object and returns it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def toJavaRDD() : JavaRDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;), 2)
c.toJavaRDD
res3: org.apache.spark.api.java.JavaRDD[String] = ParallelCollectionRDD[6] at parallelize at &amp;lt;console&amp;gt;:12&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_tolocaliterator&#34;&gt;5.11. toLocalIterator&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Converts the RDD into a scala iterator at the master node.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def toLocalIterator: Iterator[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(1,2,3,4,5,6), 2)
val iter = z.toLocalIterator

iter.next
res51: Int = 1

iter.next
res52: Int = 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_top&#34;&gt;5.12. top&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Utilizes the implicit ordering of $T$ to determine the top $k$ values and returns them as an array.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ddef top(num: Int)(implicit ord: Ordering[T]): Array[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(Array(6, 9, 4, 7, 5, 8), 2)
c.top(2)
res28: Array[Int] = Array(9, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_tostring&#34;&gt;5.13. toString&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Assembles a human-readable textual description of the RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;override def toString: String&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(1,2,3,4,5,6), 2)
z.toString
res61: String = ParallelCollectionRDD[80] at parallelize at &amp;lt;console&amp;gt;:21

val randRDD = sc.parallelize(List( (7,&#34;cat&#34;), (6, &#34;mouse&#34;),(7, &#34;cup&#34;), (6, &#34;book&#34;), (7, &#34;tv&#34;), (6, &#34;screen&#34;), (7, &#34;heater&#34;)))
val sortedRDD = randRDD.sortByKey()
sortedRDD.toString
res64: String = ShuffledRDD[88] at sortByKey at &amp;lt;console&amp;gt;:23&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_treeaggregate&#34;&gt;5.14. treeAggregate&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Computes the same thing as aggregate, except it aggregates the elements of the RDD in a multi-level tree pattern. Another difference is that it does not use the initial value for the second reduce function (combOp).  By default a tree of depth 2 is used, but this can be changed via the depth parameter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def treeAggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U, depth: Int = 2)(implicit arg0: ClassTag[U]): U&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(1,2,3,4,5,6), 2)

// lets first print out the contents of the RDD with partition labels
def myfunc(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}

z.mapPartitionsWithIndex(myfunc).collect
res28: Array[String] = Array([partID:0, val: 1], [partID:0, val: 2], [partID:0, val: 3], [partID:1, val: 4], [partID:1, val: 5], [partID:1, val: 6])

z.treeAggregate(0)(math.max(_, _), _ + _)
res40: Int = 9

// Note unlike normal aggregrate. Tree aggregate does not apply the initial value for the second reduce
// This example :: returns 11 since the initial value is 5
// reduce of partition 0 will be max(5, 1, 2, 3) = 5
// reduce of partition 1 will be max(4, 5, 6) = 6
// final reduce across partitions will be 5 + 6 = 11
// note the final reduce does not include the initial value
z.treeAggregate(5)(math.max(_, _), _ + _)
res42: Int = 11&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_treereduce&#34;&gt;5.15. treeReduce&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Works like reduce except reduces the elements of the RDD in a multi-level tree pattern.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def  treeReduce(f: (T, T) ⇒ T, depth: Int = 2): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(1,2,3,4,5,6), 2)
z.treeReduce(_+_)
res49: Int = 21&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_union&#34;&gt;5.16. union, ++&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs the standard set operation: A union B&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def ++(other: RDD[T]): RDD[T]
def union(other: RDD[T]): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 3, 1)
val b = sc.parallelize(5 to 7, 1)
(a ++ b).collect
res0: Array[Int] = Array(1, 2, 3, 5, 6, 7)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_unpersist&#34;&gt;5.17. unpersist&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Dematerializes the RDD (i.e. Erases all data items from hard-disk and memory). However, the RDD object remains. If it is referenced in a computation, Spark will regenerate it automatically using the stored dependency graph.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def unpersist(blocking: Boolean = true): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(1 to 10, 10)
val z = (y++y)
z.collect
z.unpersist(true)
14/04/19 03:04:57 INFO UnionRDD: Removing RDD 22 from persistence list
14/04/19 03:04:57 INFO BlockManager: Removing RDD 22&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_values&#34;&gt;5.18. values&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&amp;gt;Extracts the values from all contained tuples and returns them in a new RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def values: RDD[V]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.values.collect
res3: Array[String] = Array(dog, tiger, lion, cat, panther, eagle)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_variance_double_samplevariance_double&#34;&gt;5.19. variance [Double], sampleVariance [Double]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Calls stats and extracts either variance-component or corrected sampleVariance-component.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def variance(): Double
def sampleVariance(): Double&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(9.1, 1.0, 1.2, 2.1, 1.3, 5.0, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 10.0, 8.9, 5.5), 3)
a.variance
res70: Double = 10.605333333333332

val x = sc.parallelize(List(1.0, 2.0, 3.0, 5.0, 20.0, 19.02, 19.29, 11.09, 21.0), 2)
x.variance
res14: Double = 66.04584444444443

x.sampleVariance
res13: Double = 74.30157499999999&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_zip&#34;&gt;5.20. zip&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Joins two RDDs by combining the i-th of either partition with each other. The resulting RDD will consist of two-component tuples which are interpreted as key-value pairs by the methods provided by the PairRDDFunctions extension.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 100, 3)
val b = sc.parallelize(101 to 200, 3)
a.zip(b).collect
res1: Array[(Int, Int)] = Array((1,101), (2,102), (3,103), (4,104), (5,105), (6,106), (7,107), (8,108), (9,109), (10,110), (11,111), (12,112), (13,113), (14,114), (15,115), (16,116), (17,117), (18,118), (19,119), (20,120), (21,121), (22,122), (23,123), (24,124), (25,125), (26,126), (27,127), (28,128), (29,129), (30,130), (31,131), (32,132), (33,133), (34,134), (35,135), (36,136), (37,137), (38,138), (39,139), (40,140), (41,141), (42,142), (43,143), (44,144), (45,145), (46,146), (47,147), (48,148), (49,149), (50,150), (51,151), (52,152), (53,153), (54,154), (55,155), (56,156), (57,157), (58,158), (59,159), (60,160), (61,161), (62,162), (63,163), (64,164), (65,165), (66,166), (67,167), (68,168), (69,169), (70,170), (71,171), (72,172), (73,173), (74,174), (75,175), (76,176), (77,177), (78,...

val a = sc.parallelize(1 to 100, 3)
val b = sc.parallelize(101 to 200, 3)
val c = sc.parallelize(201 to 300, 3)
a.zip(b).zip(c).map((x) =&amp;gt; (x._1._1, x._1._2, x._2 )).collect
res12: Array[(Int, Int, Int)] = Array((1,101,201), (2,102,202), (3,103,203), (4,104,204), (5,105,205), (6,106,206), (7,107,207), (8,108,208), (9,109,209), (10,110,210), (11,111,211), (12,112,212), (13,113,213), (14,114,214), (15,115,215), (16,116,216), (17,117,217), (18,118,218), (19,119,219), (20,120,220), (21,121,221), (22,122,222), (23,123,223), (24,124,224), (25,125,225), (26,126,226), (27,127,227), (28,128,228), (29,129,229), (30,130,230), (31,131,231), (32,132,232), (33,133,233), (34,134,234), (35,135,235), (36,136,236), (37,137,237), (38,138,238), (39,139,239), (40,140,240), (41,141,241), (42,142,242), (43,143,243), (44,144,244), (45,145,245), (46,146,246), (47,147,247), (48,148,248), (49,149,249), (50,150,250), (51,151,251), (52,152,252), (53,153,253), (54,154,254), (55,155,255)...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;=== zipParititions&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to zip. But provides more control over the zipping process.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def zipPartitions[B: ClassTag, V: ClassTag](rdd2: RDD[B])(f: (Iterator[T], Iterator[B]) =&amp;gt; Iterator[V]): RDD[V]
def zipPartitions[B: ClassTag, V: ClassTag](rdd2: RDD[B], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B]) =&amp;gt; Iterator[V]): RDD[V]
def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C])(f: (Iterator[T], Iterator[B], Iterator[C]) =&amp;gt; Iterator[V]): RDD[V]
def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B], Iterator[C]) =&amp;gt; Iterator[V]): RDD[V]
def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D])(f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) =&amp;gt; Iterator[V]): RDD[V]
def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) =&amp;gt; Iterator[V]): RDD[V]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(0 to 9, 3)
val b = sc.parallelize(10 to 19, 3)
val c = sc.parallelize(100 to 109, 3)
def myfunc(aiter: Iterator[Int], biter: Iterator[Int], citer: Iterator[Int]): Iterator[String] =
{
  var res = List[String]()
  while (aiter.hasNext &amp;amp;&amp;amp; biter.hasNext &amp;amp;&amp;amp; citer.hasNext)
  {
    val x = aiter.next + &#34; &#34; + biter.next + &#34; &#34; + citer.next
    res ::= x
  }
  res.iterator
}
a.zipPartitions(b, c)(myfunc).collect
res50: Array[String] = Array(2 12 102, 1 11 101, 0 10 100, 5 15 105, 4 14 104, 3 13 103, 9 19 109, 8 18 108, 7 17 107, 6 16 106)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_zipwithindex&#34;&gt;5.21. zipWithIndex&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Zips the elements of the RDD with its element indexes. The indexes start from 0. If the RDD is spread across multiple partitions then a spark Job is started to perform this operation.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def zipWithIndex(): RDD[(T, Long)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(Array(&#34;A&#34;, &#34;B&#34;, &#34;C&#34;, &#34;D&#34;))
val r = z.zipWithIndex
res110: Array[(String, Long)] = Array((A,0), (B,1), (C,2), (D,3))

val z = sc.parallelize(100 to 120, 5)
val r = z.zipWithIndex
r.collect
res11: Array[(Int, Long)] = Array((100,0), (101,1), (102,2), (103,3), (104,4), (105,5), (106,6), (107,7), (108,8), (109,9), (110,10), (111,11), (112,12), (113,13), (114,14), (115,15), (116,16), (117,17), (118,18), (119,19), (120,20))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_zipwithuniqueid&#34;&gt;5.22. zipWithUniqueId&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is different from zipWithIndex since just gives a unique id to each data element but the ids may not match the index number of the data element. This operation does not start a spark job even if the RDD is spread across multiple partitions.
Compare the results of the example below with that of the 2nd example of zipWithIndex. You should be able to see the difference.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def zipWithUniqueId(): RDD[(T, Long)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(100 to 120, 5)
val r = z.zipWithUniqueId
r.collect

res12: Array[(Int, Long)] = Array((100,0), (101,5), (102,10), (103,15), (104,1), (105,6), (106,11), (107,16), (108,2), (109,7), (110,12), (111,17), (112,3), (113,8), (114,13), (115,18), (116,4), (117,9), (118,14), (119,19), (120,24))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>spark-rdd</title>
      <link>/post/bigdata/spark/spark-rdd/</link>
      <pubDate>Sat, 08 Apr 2017 18:42:52 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-rdd/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;spark rdd&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_mappartitionswithindex&#34;&gt;1. mapPartitionsWithIndex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_aggregate&#34;&gt;2. aggregate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_aggregatebykey&#34;&gt;3. aggregateByKey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_checkpoint&#34;&gt;4. checkpoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_combinebykey&#34;&gt;5. combineByKey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countbykey&#34;&gt;6. countByKey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_filterbyrange&#34;&gt;7. filterByRange&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foldbykey&#34;&gt;8. foldByKey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foreachpartition&#34;&gt;9. foreachPartition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_keys_values&#34;&gt;10. keys values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_tmp&#34;&gt;tmp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_mappartitionswithindex&#34;&gt;1. mapPartitionsWithIndex&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;asciinema-player src=&#34;/src/records/spark/2017-04-10.json&#34; cols=&#34;95&#34; rows=&#34;22&#34; speed=&#34;2&#34;&gt;&lt;/asciinema-player&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;map是对每个元素操作, mapPartitions是对其中的每个partition操作

mapPartitionsWithIndex : 把每个partition中的分区号和对应的值拿出来, 看源码
val func = (index: Int, iter: Iterator[(Int)]) =&amp;gt; {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
rdd1.mapPartitionsWithIndex(func).collect&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_aggregate&#34;&gt;2. aggregate&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;asciinema-player src=&#34;/src/records/spark/aggregate.json&#34; cols=&#34;95&#34; rows=&#34;22&#34; speed=&#34;2&#34;&gt;&lt;/asciinema-player&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def func1(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
rdd1.mapPartitionsWithIndex(func1).collect
###是action操作, 第一个参数是初始值, 二:是2个函数[每个函数都是2个参数(第一个参数:先对个个分区进行合并, 第二个:对个个分区合并后的结果再进行合并), 输出一个参数]
###0 + (0+1+2+3+4   +   0+5+6+7+8+9)
rdd1.aggregate(0)(_+_, _+_)
rdd1.aggregate(0)(math.max(_, _), _ + _)
###5和1比, 得5再和234比得5 --&amp;gt; 5和6789比,得9 --&amp;gt; 5 + (5+9)
rdd1.aggregate(5)(math.max(_, _), _ + _)


val rdd2 = sc.parallelize(List(&#34;a&#34;,&#34;b&#34;,&#34;c&#34;,&#34;d&#34;,&#34;e&#34;,&#34;f&#34;),2)
def func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
rdd2.aggregate(&#34;&#34;)(_ + _, _ + _)
rdd2.aggregate(&#34;=&#34;)(_ + _, _ + _)

val rdd3 = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;345&#34;,&#34;4567&#34;),2)
rdd3.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.max(x.length, y.length).toString, (x,y) =&amp;gt; x + y)

val rdd4 = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;345&#34;,&#34;&#34;),2)
rdd4.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.min(x.length, y.length).toString, (x,y) =&amp;gt; x + y)

val rdd5 = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;&#34;,&#34;345&#34;),2)
rdd5.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.min(x.length, y.length).toString, (x,y) =&amp;gt; x + y)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_aggregatebykey&#34;&gt;3. aggregateByKey&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;asciinema-player src=&#34;/src/records/spark/aggregateByKey.json&#34; cols=&#34;95&#34; rows=&#34;22&#34; speed=&#34;2&#34;&gt;&lt;/asciinema-player&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val pairRDD = sc.parallelize(List( (&#34;cat&#34;,2), (&#34;cat&#34;, 5), (&#34;mouse&#34;, 4),(&#34;cat&#34;, 12), (&#34;dog&#34;, 12), (&#34;mouse&#34;, 2)), 2)
def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
pairRDD.mapPartitionsWithIndex(func2).collect
pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect
pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).collect&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_checkpoint&#34;&gt;4. checkpoint&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sc.setCheckpointDir(&#34;hdfs://node-1.itcast.cn:9000/ck&#34;)
val rdd = sc.textFile(&#34;hdfs://node-1.itcast.cn:9000/wc&#34;).flatMap(_.split(&#34; &#34;)).map((_, 1)).reduceByKey(_+_)
rdd.checkpoint
rdd.isCheckpointed
rdd.count
rdd.isCheckpointed
rdd.getCheckpointFile

coalesce, repartition
val rdd1 = sc.parallelize(1 to 10, 10)
val rdd2 = rdd1.coalesce(2, false)
rdd2.partitions.length

collectAsMap : Map(b -&amp;gt; 2, a -&amp;gt; 1)
val rdd = sc.parallelize(List((&#34;a&#34;, 1), (&#34;b&#34;, 2)))
rdd.collectAsMap&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_combinebykey&#34;&gt;5. combineByKey&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;asciinema-player src=&#34;/src/records/spark/combineByKey.json&#34; cols=&#34;95&#34; rows=&#34;22&#34; speed=&#34;2&#34;&gt;&lt;/asciinema-player&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;和reduceByKey是相同的效果
###第一个参数x:原封不动取出来, 第二个参数:是函数, 局部运算, 第三个:是函数, 对局部运算后的结果再做运算
###每个分区中每个key中value中的第一个值, (hello,1)(hello,1)(good,1)--&amp;gt;(hello(1,1),good(1))--&amp;gt;x就相当于hello的第一个1, good中的1



val rdd1 = sc.textFile(&#34;hdfs://master:9000/wordcount/input/&#34;).flatMap(_.split(&#34; &#34;)).map((_, 1))
val rdd2 = rdd1.combineByKey(x =&amp;gt; x, (a: Int, b: Int) =&amp;gt; a + b, (m: Int, n: Int) =&amp;gt; m + n)
rdd1.collect
rdd2.collect

###当input下有3个文件时(有3个block块, 不是有3个文件就有3个block, ), 每个会多加3个10
val rdd3 = rdd1.combineByKey(x =&amp;gt; x + 10, (a: Int, b: Int) =&amp;gt; a + b, (m: Int, n: Int) =&amp;gt; m + n)
rdd3.collect


val rdd4 = sc.parallelize(List(&#34;dog&#34;,&#34;cat&#34;,&#34;gnu&#34;,&#34;salmon&#34;,&#34;rabbit&#34;,&#34;turkey&#34;,&#34;wolf&#34;,&#34;bear&#34;,&#34;bee&#34;), 3)
val rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2), 3)

val rdd7 = rdd6.combineByKey(List(_), (x: List[String], y: String) =&amp;gt; x :+ y, (m: List[String], n: List[String]) =&amp;gt; m ++ n)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_countbykey&#34;&gt;6. countByKey&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd1 = sc.parallelize(List((&#34;a&#34;, 1), (&#34;b&#34;, 2), (&#34;b&#34;, 2), (&#34;c&#34;, 2), (&#34;c&#34;, 1)))
rdd1.countByKey
rdd1.countByValue&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_filterbyrange&#34;&gt;7. filterByRange&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd1 = sc.parallelize(List((&#34;e&#34;, 5), (&#34;c&#34;, 3), (&#34;d&#34;, 4), (&#34;c&#34;, 2), (&#34;a&#34;, 1)))
val rdd2 = rdd1.filterByRange(&#34;b&#34;, &#34;d&#34;)
rdd2.collect

flatMapValues  :  Array((a,1), (a,2), (b,3), (b,4))
val rdd3 = sc.parallelize(List((&#34;a&#34;, &#34;1 2&#34;), (&#34;b&#34;, &#34;3 4&#34;)))
val rdd4 = rdd3.flatMapValues(_.split(&#34; &#34;))
rdd4.collect&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_foldbykey&#34;&gt;8. foldByKey&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd1 = sc.parallelize(List(&#34;dog&#34;, &#34;wolf&#34;, &#34;cat&#34;, &#34;bear&#34;), 2)
val rdd2 = rdd1.map(x =&amp;gt; (x.length, x))
val rdd3 = rdd2.foldByKey(&#34;&#34;)(_+_)

val rdd = sc.textFile(&#34;hdfs://node-1.itcast.cn:9000/wc&#34;).flatMap(_.split(&#34; &#34;)).map((_, 1))
rdd.foldByKey(0)(_+_)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_foreachpartition&#34;&gt;9. foreachPartition&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)
rdd1.foreachPartition(x =&amp;gt; println(x.reduce(_ + _)))

keyBy : 以传入的参数做key
val rdd1 = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val rdd2 = rdd1.keyBy(_.length)
rdd2.collect&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_keys_values&#34;&gt;10. keys values&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd1 = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val rdd2 = rdd1.map(x =&amp;gt; (x.length, x))
rdd2.keys.collect
rdd2.values.collect&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_tmp&#34; class=&#34;sect0&#34;&gt;tmp&lt;/h1&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;ssh root@196.168.1.34

docker exec -it spark-master /bin/bash

cd $SPARK_HOME \
&amp;amp;&amp;amp; bin/spark-shell --master spark://master:7077&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;(1,(CompactBuffer(b, b),CompactBuffer(c, c))),
(3,(CompactBuffer(b),CompactBuffer(c))),
(2,(CompactBuffer(b),CompactBuffer(c)))&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>spark-基础</title>
      <link>/post/bigdata/spark/spark-%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Sat, 08 Apr 2017 18:42:52 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-%E5%9F%BA%E7%A1%80/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;spark-基础&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_spark集群安装&#34;&gt;1. Spark集群安装&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_安装&#34;&gt;1.1. 安装&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_机器部署&#34;&gt;1.1.1. 机器部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_下载spark安装包&#34;&gt;1.1.2. 下载Spark安装包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_配置spark&#34;&gt;1.1.3. 配置Spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_执行第一个spark程序&#34;&gt;1.2. 执行第一个spark程序&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_启动spark_shell&#34;&gt;1.3. 启动Spark Shell&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_启动spark_shell_2&#34;&gt;1.3.1. 启动spark shell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_在spark_shell中编写wordcount程序&#34;&gt;1.3.2. 在spark shell中编写WordCount程序&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_在idea中编写wordcount程序&#34;&gt;1.4. 在IDEA中编写WordCount程序&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_spark集群安装&#34;&gt;1. Spark集群安装&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装&#34;&gt;1.1. 安装&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_机器部署&#34;&gt;1.1.1. 机器部署&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;准备两台以上Linux服务器，安装好JDK1.7&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_下载spark安装包&#34;&gt;1.1.2. 下载Spark安装包&lt;/h4&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;下载&lt;br&gt;
&lt;a href=&#34;http://www.apache.org/dyn/closer.lua/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz&#34; class=&#34;bare&#34;&gt;http://www.apache.org/dyn/closer.lua/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传解压安装包&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;上传 &lt;strong&gt;spark-1.5.2-bin-hadoop2.6.tgz&lt;/strong&gt; 安装包到Linux上&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压安装包到指定位置&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf spark-1.5.2-bin-hadoop2.6.tgz -C /usr/local&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_配置spark&#34;&gt;1.1.3. 配置Spark&lt;/h4&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;进入到Spark安装目录&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /usr/local/spark-1.5.2-bin-hadoop2.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;进入conf目录并重命名并修改spark-env.sh.template文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd conf/
mv spark-env.sh.template spark-env.sh
vi spark-env.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在该配置文件中添加如下配置&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;export JAVA_HOME=/usr/java/jdk1.7.0_45
export SPARK_MASTER_IP=node1.itcast.cn
export SPARK_MASTER_PORT=7077&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重命名并修改slaves.template文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mv slaves.template slaves
vi slaves
//在该文件中添加子节点所在的位置（Worker节点）
node2.itcast.cn
node3.itcast.cn
node4.itcast.cn&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将配置好的Spark拷贝到其他节点上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;scp -r spark-1.5.2-bin-hadoop2.6/ node2:/usr/local/
scp -r spark-1.5.2-bin-hadoop2.6/ node3:/usr/local/
scp -r spark-1.5.2-bin-hadoop2.6/ node4:/usr/local/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spark集群配置完毕，目前是1个 &lt;strong&gt;Master&lt;/strong&gt; ，3个 &lt;strong&gt;Work&lt;/strong&gt; ，在 &lt;strong&gt;node1&lt;/strong&gt; 上启动 &lt;strong&gt;Spark&lt;/strong&gt; 集群&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/sbin/start-all.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动后执行jps命令&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：
http://node1:8080/
到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spark集群规划&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;node1，node2是Master；node3，node4，node5是Worker
安装配置zk集群，并启动zk集群
停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;export SPARK_DAEMON_JAVA_OPTS=&#34;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark&#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;node1&lt;/strong&gt; 节点上修改 &lt;strong&gt;slaves&lt;/strong&gt; 配置文件内容指定 &lt;strong&gt;worker&lt;/strong&gt; 节点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;node1&lt;/strong&gt; 上执行 &lt;strong&gt;sbin/start-all.sh&lt;/strong&gt; 脚本，然后在 &lt;strong&gt;node2&lt;/strong&gt; 上执行 &lt;strong&gt;sbin/start-master.sh&lt;/strong&gt; 启动第二个 &lt;strong&gt;Master&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;执行Spark程序&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_执行第一个spark程序&#34;&gt;1.2. 执行第一个spark程序&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://node1.itcast.cn:7077 \
--executor-memory 1G \
--total-executor-cores 2 \
/usr/local/spark-1.5.2-bin-hadoop2.6/lib/spark-examples-1.5.2-hadoop2.6.0.jar \
100&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;该算法是利用蒙特·卡罗算法求PI&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_启动spark_shell&#34;&gt;1.3. 启动Spark Shell&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_启动spark_shell_2&#34;&gt;1.3.1. 启动spark shell&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-shell \
--master spark://node1.itcast.cn:7077 \ &lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
--executor-memory 2g \ &lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
--total-executor-cores 2 &lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;colist arabic&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;master spark://node1.itcast.cn:7077 指定Master的地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;executor-memory 2g 指定每个worker可用内存为2G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;total-executor-cores 2 指定整个集群使用的cup核数为2个&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock tip&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-tip&#34; title=&#34;Tip&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_在spark_shell中编写wordcount程序&#34;&gt;1.3.2. 在spark shell中编写WordCount程序&lt;/h4&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;首先启动hdfs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向hdfs上传一个文件到hdfs://node1.itcast.cn:9000/words.txt&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在spark shell中用scala语言编写spark程序&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sc.textFile(&#34;hdfs://node1.itcast.cn:9000/words.txt&#34;).flatMap(_.split(&#34; &#34;))
.map((_,1)).reduceByKey(_+_).saveAsTextFile(&#34;hdfs://node1.itcast.cn:9000/out&#34;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用hdfs命令查看结果&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hdfs dfs -ls hdfs://node1.itcast.cn:9000/out/p*&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock tip&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-tip&#34; title=&#34;Tip&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
 &lt;strong&gt;sc&lt;/strong&gt; 是 &lt;strong&gt;SparkContext&lt;/strong&gt; 对象，该对象时提交 &lt;strong&gt;spark&lt;/strong&gt; 程序的入口
&lt;strong&gt;textFile(hdfs://node1.itcast.cn:9000/words.txt)&lt;/strong&gt; 是hdfs中读取数据
&lt;strong&gt;flatMap(&lt;em&gt;.split(&#34; &#34;))&lt;/strong&gt; 先map在压平
&lt;strong&gt;map&lt;/em&gt;,1&lt;/strong&gt; 将单词和1构成元组
&lt;strong&gt;reduceByKey(&lt;em&gt;+&lt;/em&gt;)&lt;/strong&gt; 按照 &lt;strong&gt;key&lt;/strong&gt; 进行r &lt;strong&gt;educe&lt;/strong&gt; ，并将v &lt;strong&gt;alue&lt;/strong&gt; 累加
&lt;strong&gt;saveAsTextFile(&#34;hdfs://node1.itcast.cn:9000/out&#34;)&lt;/strong&gt; 将结果写入到hdfs中
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_在idea中编写wordcount程序&#34;&gt;1.4. 在IDEA中编写WordCount程序&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;spark shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理jar包的依赖。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建一个项目&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;选择Maven项目，然后点击next&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;填写maven的GAV，然后点击next&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;填写项目名称，然后点击finish&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建好maven项目后，点击Enable Auto-Import&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置Maven的pom.xml&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&amp;gt;
&amp;lt;project xmlns=&#34;http://maven.apache.org/POM/4.0.0&#34;
         xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34;
         xsi:schemaLocation=&#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&#34;&amp;gt;
    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;

    &amp;lt;groupId&amp;gt;cn.itcast.spark&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spark-mvn&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;1.0-SNAPSHOT&amp;lt;/version&amp;gt;

    &amp;lt;properties&amp;gt;
        &amp;lt;maven.compiler.source&amp;gt;1.7&amp;lt;/maven.compiler.source&amp;gt;
        &amp;lt;maven.compiler.target&amp;gt;1.7&amp;lt;/maven.compiler.target&amp;gt;
        &amp;lt;encoding&amp;gt;UTF-8&amp;lt;/encoding&amp;gt;
        &amp;lt;scala.version&amp;gt;2.10.6&amp;lt;/scala.version&amp;gt;
        &amp;lt;scala.compat.version&amp;gt;2.10&amp;lt;/scala.compat.version&amp;gt;
    &amp;lt;/properties&amp;gt;

    &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.scala-lang&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;scala-library&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;${scala.version}&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;1.5.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spark-streaming_2.10&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;1.5.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;2.6.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;

    &amp;lt;build&amp;gt;
        &amp;lt;sourceDirectory&amp;gt;src/main/scala&amp;lt;/sourceDirectory&amp;gt;
        &amp;lt;testSourceDirectory&amp;gt;src/test/scala&amp;lt;/testSourceDirectory&amp;gt;
        &amp;lt;plugins&amp;gt;
            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;net.alchim31.maven&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;scala-maven-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;3.2.0&amp;lt;/version&amp;gt;
                &amp;lt;executions&amp;gt;
                    &amp;lt;execution&amp;gt;
                        &amp;lt;goals&amp;gt;
                            &amp;lt;goal&amp;gt;compile&amp;lt;/goal&amp;gt;
                            &amp;lt;goal&amp;gt;testCompile&amp;lt;/goal&amp;gt;
                        &amp;lt;/goals&amp;gt;
                        &amp;lt;configuration&amp;gt;
                            &amp;lt;args&amp;gt;
                                &amp;lt;arg&amp;gt;-make:transitive&amp;lt;/arg&amp;gt;
                                &amp;lt;arg&amp;gt;-dependencyfile&amp;lt;/arg&amp;gt;
                                &amp;lt;arg&amp;gt;${project.build.directory}/.scala_dependencies&amp;lt;/arg&amp;gt;
                            &amp;lt;/args&amp;gt;
                        &amp;lt;/configuration&amp;gt;
                    &amp;lt;/execution&amp;gt;
                &amp;lt;/executions&amp;gt;
            &amp;lt;/plugin&amp;gt;
            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;maven-surefire-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;2.18.1&amp;lt;/version&amp;gt;
                &amp;lt;configuration&amp;gt;
                    &amp;lt;useFile&amp;gt;false&amp;lt;/useFile&amp;gt;
                    &amp;lt;disableXmlReport&amp;gt;true&amp;lt;/disableXmlReport&amp;gt;
                    &amp;lt;includes&amp;gt;
                        &amp;lt;include&amp;gt;**/*Test.*&amp;lt;/include&amp;gt;
                        &amp;lt;include&amp;gt;**/*Suite.*&amp;lt;/include&amp;gt;
                    &amp;lt;/includes&amp;gt;
                &amp;lt;/configuration&amp;gt;
            &amp;lt;/plugin&amp;gt;

            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;maven-shade-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;2.3&amp;lt;/version&amp;gt;
                &amp;lt;executions&amp;gt;
                    &amp;lt;execution&amp;gt;
                        &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;
                        &amp;lt;goals&amp;gt;
                            &amp;lt;goal&amp;gt;shade&amp;lt;/goal&amp;gt;
                        &amp;lt;/goals&amp;gt;
                        &amp;lt;configuration&amp;gt;
                            &amp;lt;filters&amp;gt;
                                &amp;lt;filter&amp;gt;
                                    &amp;lt;artifact&amp;gt;*:*&amp;lt;/artifact&amp;gt;
                                    &amp;lt;excludes&amp;gt;
                                        &amp;lt;exclude&amp;gt;META-INF/*.SF&amp;lt;/exclude&amp;gt;
                                        &amp;lt;exclude&amp;gt;META-INF/*.DSA&amp;lt;/exclude&amp;gt;
                                        &amp;lt;exclude&amp;gt;META-INF/*.RSA&amp;lt;/exclude&amp;gt;
                                    &amp;lt;/excludes&amp;gt;
                                &amp;lt;/filter&amp;gt;
                            &amp;lt;/filters&amp;gt;
                            &amp;lt;transformers&amp;gt;
                                &amp;lt;transformer implementation=&#34;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&#34;&amp;gt;
                                    &amp;lt;mainClass&amp;gt;cn.itcast.spark.WordCount&amp;lt;/mainClass&amp;gt;
                                &amp;lt;/transformer&amp;gt;
                            &amp;lt;/transformers&amp;gt;
                        &amp;lt;/configuration&amp;gt;
                    &amp;lt;/execution&amp;gt;
                &amp;lt;/executions&amp;gt;
            &amp;lt;/plugin&amp;gt;
        &amp;lt;/plugins&amp;gt;
    &amp;lt;/build&amp;gt;
&amp;lt;/project&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将src/main/java和src/test/java分别修改成src/main/scala和src/test/scala，与pom.xml中的配置保持一致&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;新建一个scala class，类型为Object&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编写spark程序&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;package cn.itcast.spark

import org.apache.spark.{SparkContext, SparkConf}

object WordCount {
  def main(args: Array[String]) {
    //创建SparkConf()并设置App名称
    val conf = new SparkConf().setAppName(&#34;WC&#34;)
    //创建SparkContext，该对象是提交spark App的入口
    val sc = new SparkContext(conf)
    //使用sc创建RDD并执行相应的transformation和action
    sc.textFile(args(0)).flatMap(_.split(&#34; &#34;)).map((_, 1)).reduceByKey(_+_, 1).sortBy(_._2, false).saveAsTextFile(args(1))
    //停止sc，结束该任务
    sc.stop()
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用Maven打包：首先修改pom.xml中的main class&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;点击idea右侧的Maven Project选项&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;点击Lifecycle,选择clean和package，然后点击Run Maven Build&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;选择编译成功的jar包，并将该jar上传到Spark集群中的某个节点上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;首先启动hdfs和Spark集群
启动hdfs&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/hadoop-2.6.1/sbin/start-dfs.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;启动spark&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/sbin/start-all.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用spark-submit命令提交Spark应用（注意参数的顺序）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \
--class cn.itcast.spark.WordCount \
--master spark://node1.itcast.cn:7077 \
--executor-memory 2G \
--total-executor-cores 4 \
/root/spark-mvn-1.0-SNAPSHOT.jar \
hdfs://node1.itcast.cn:9000/words.txt \
hdfs://node1.itcast.cn:9000/out&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看程序执行结果&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hdfs dfs -cat hdfs://node1.itcast.cn:9000/out/part-00000
(hello,6)
(tom,3)
(kitty,2)
(jerry,1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>spark参考</title>
      <link>/post/bigdata/spark/spark-%E5%8F%82%E8%80%83/</link>
      <pubDate>Sat, 08 Apr 2017 18:42:52 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-%E5%8F%82%E8%80%83/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;spark&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_rdd&#34;&gt;1. RDD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_rdd&#34;&gt;1. RDD&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html&#34;&gt;SparkRDDAPIExamples&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scala实战</title>
      <link>/post/bigdata/scala/scala%E5%AE%9E%E6%88%98/</link>
      <pubDate>Fri, 07 Apr 2017 15:06:47 +0000</pubDate>
      
      <guid>/post/bigdata/scala/scala%E5%AE%9E%E6%88%98/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Scala实战&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_scala实战&#34;&gt;1. Scala实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_项目概述&#34;&gt;2. 项目概述&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_需求&#34;&gt;2.1. 需求&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_akka简介&#34;&gt;2.2. Akka简介&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_项目实现&#34;&gt;3. 项目实现&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_架构图&#34;&gt;3.1. 架构图&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_重要类介绍&#34;&gt;3.2. 重要类介绍&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_actorsystem&#34;&gt;3.2.1. ActorSystem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_actor&#34;&gt;3.2.2. Actor&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_master类&#34;&gt;3.3. Master类&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_worker类&#34;&gt;3.4. Worker类&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_scala实战&#34;&gt;1. Scala实战&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_项目概述&#34;&gt;2. 项目概述&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_需求&#34;&gt;2.1. 需求&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;目前大多数的分布式架构底层通信都是通过RPC实现的，RPC框架非常多，比如前我们学过的Hadoop项目的RPC通信框架，但是Hadoop在设计之初就是为了运行长达数小时的批量而设计的，在某些极端的情况下，任务提交的延迟很高，所有Hadoop的RPC显得有些笨重。

Spark 的RPC是通过Akka类库实现的，Akka用Scala语言开发，基于Actor并发模型实现，Akka具有高可靠、高性能、可扩展等特点，使用Akka可以轻松实现分布式RPC功能。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_akka简介&#34;&gt;2.2. Akka简介&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Akka基于Actor模型，提供了一个用于构建可扩展的（Scalable）、弹性的（Resilient）、快速响应的（Responsive）应用程序的平台。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Actor模型&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在计算机科学领域，Actor模型是一个并行计算（Concurrent Computation）模型，它把actor作为并行计算的基本元素来对待：为响应一个接收到的消息，一个actor能够自己做出一些决策，如创建更多的actor，或发送更多的消息，或者确定如何去响应接收到的下一个消息。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_151513.png&#34; alt=&#34;2017 04 07 151513&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Actor&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;是Akka中最核心的概念，它是一个封装了状态和行为的对象，Actor之间可以通过交换消息的方式进行通信，每个Actor都有自己的收件箱（Mailbox）。通过Actor能够简化锁及线程管理，可以非常容易地开发出正确地并发程序和并行系统，Actor具有如下特性：&lt;/p&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;提供了一种高级抽象，能够简化在并发（Concurrency）/并行（Parallelism）应用场景下的编程开发&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;提供了异步非阻塞的、高性能的事件驱动编程模型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;超级轻量级事件处理（每GB堆内存几百万Actor）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_项目实现&#34;&gt;3. 项目实现&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_架构图&#34;&gt;3.1. 架构图&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_151523.png&#34; alt=&#34;2017 04 07 151523&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_重要类介绍&#34;&gt;3.2. 重要类介绍&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_actorsystem&#34;&gt;3.2.1. ActorSystem&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在 &lt;strong&gt;Akka&lt;/strong&gt; 中， &lt;strong&gt;ActorSystem&lt;/strong&gt; 是一个重量级的结构，他需要分配多个线程，所以在实际应用中， &lt;strong&gt;ActorSystem&lt;/strong&gt; 通常是一个单例对象，我们可以使用这个 &lt;strong&gt;ActorSystem&lt;/strong&gt; 创建很多 &lt;strong&gt;Actor&lt;/strong&gt; 。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_actor&#34;&gt;3.2.2. Actor&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Akka中，Actor负责通信，在Actor中有一些重要的生命周期方法。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;preStart()&lt;/strong&gt; 方法：该方法在 &lt;strong&gt;Actor&lt;/strong&gt; 对象构造方法执行后执行，整个 &lt;strong&gt;Actor&lt;/strong&gt; 生命周期中仅执行一次。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;receive()&lt;/strong&gt; 方法：该方法在 &lt;strong&gt;Actor&lt;/strong&gt; 的 &lt;strong&gt;preStart&lt;/strong&gt; 方法执行完成后执行，用于接收消息，会被反复执行。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_master类&#34;&gt;3.3. Master类&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.concurrent.duration._
import akka.actor.{Props, ActorSystem, Actor}
import akka.actor.Actor.Receive
import com.typesafe.config.ConfigFactory

import scala.collection.mutable

/**
  * Master为整个集群中的主节点
  * Master继承了Actor
  */
class Master extends Actor{

  //保存WorkerID和Work信息的map
  val idToWorker = new mutable.HashMap[String, WorkerInfo]
  //保存所有Worker信息的Set
  val workers = new mutable.HashSet[WorkerInfo]
  //Worker超时时间
  val WORKER_TIMEOUT = 10 * 1000
  //重新receive方法

  //导入隐式转换，用于启动定时器
  import context.dispatcher

  //构造方法执行完执行一次
  override def preStart(): Unit = {
    //启动定时器，定时执行
    context.system.scheduler.schedule(0 millis, WORKER_TIMEOUT millis, self, CheckOfTimeOutWorker)
  }

  //该方法会被反复执行，用于接收消息，通过case class模式匹配接收消息
  override def receive: Receive = {
    //Worker向Master发送的注册消息
    case RegisterWorker(id, workerHost, memory, cores) =&amp;gt; {
      if(!idToWorker.contains(id)) {
        val worker = new WorkerInfo(id, workerHost, memory, cores)
        workers.add(worker)
        idToWorker(id) = worker
        sender ! RegisteredWorker(&#34;192.168.10.1&#34;)
      }
    }

    //Worker向Master发送的心跳消息
    case HeartBeat(workerId) =&amp;gt; {
      val workerInfo = idToWorker(workerId)
      workerInfo.lastHeartbeat = System.currentTimeMillis()
    }

    //Master自己向自己发送的定期检查超时Worker的消息
    case CheckOfTimeOutWorker =&amp;gt; {
      val currentTime = System.currentTimeMillis()
      val toRemove = workers.filter(w =&amp;gt; currentTime - w.lastHeartbeat &amp;gt;WORKER_TIMEOUT).toArray
      for(worker &amp;lt;- toRemove){
        workers -= worker
        idToWorker.remove(worker.id)
      }
      println(&#34;worker size: &#34;+ workers.size)
    }
  }
}

object Master {
  //程序执行入口
  def main(args: Array[String]) {

    val host = &#34;192.168.10.1&#34;
    val port = 8888
    //创建ActorSystem的必要参数
    val configStr =
      s&#34;&#34;&#34;
         |akka.actor.provider = &#34;akka.remote.RemoteActorRefProvider&#34;
         |akka.remote.netty.tcp.hostname = &#34;$host&#34;
         |akka.remote.netty.tcp.port = &#34;$port&#34;
&#34;&#34;&#34;.stripMargin
    val config = ConfigFactory.parseString(configStr)
    //ActorSystem是单例的，用来创建Actor
    val actorSystem = ActorSystem.create(&#34;MasterActorSystem&#34;, config)
    //启动Actor，Master会被实例化，生命周期方法会被调用
    actorSystem.actorOf(Props[Master], &#34;Master&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_worker类&#34;&gt;3.4. Worker类&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import java.util.UUID
import scala.concurrent.duration._
import akka.actor.{ActorSelection, Props, ActorSystem, Actor}
import akka.actor.Actor.Receive
import com.typesafe.config.ConfigFactory

/**
  * Worker为整个集群的从节点
  * Worker继承了Actor
  */
class Worker extends Actor{

  //Worker端持有Master端的引用（代理对象）
  var master: ActorSelection = null
  //生成一个UUID，作为Worker的标识
  val id = UUID.randomUUID().toString

  //构造方法执行完执行一次
  override def preStart(): Unit = {
    //Worker向MasterActorSystem发送建立连接请求
    master = context.system.actorSelection(&#34;akka.tcp://MasterActorSystem@192.168.10.1:8888/user/Master&#34;)
    //Worker向Master发送注册消息
    master ! RegisterWorker(id, &#34;192.168.10.1&#34;, 10240, 8)
  }

  //该方法会被反复执行，用于接收消息，通过case class模式匹配接收消息
  override def receive: Receive = {
    //Master向Worker的反馈信息
    case RegisteredWorker(masterUrl) =&amp;gt; {
      import context.dispatcher
      //启动定时任务，向Master发送心跳
      context.system.scheduler.schedule(0 millis, 5000 millis, self, SendHeartBeat)
    }

    case SendHeartBeat =&amp;gt; {
      println(&#34;worker send heartbeat&#34;)
      master ! HeartBeat(id)
    }
  }
}

object Worker {
  def main(args: Array[String]) {
    val clientPort = 2552
    //创建WorkerActorSystem的必要参数
    val configStr =
      s&#34;&#34;&#34;
         |akka.actor.provider = &#34;akka.remote.RemoteActorRefProvider&#34;
         |akka.remote.netty.tcp.port = $clientPort
&#34;&#34;&#34;.stripMargin
    val config = ConfigFactory.parseString(configStr)
    val actorSystem = ActorSystem(&#34;WorkerActorSystem&#34;, config)
    //启动Actor，Master会被实例化，生命周期方法会被调用
    actorSystem.actorOf(Props[Worker], &#34;Worker&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scala高级特性</title>
      <link>/post/bigdata/scala/scala%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/</link>
      <pubDate>Fri, 07 Apr 2017 14:47:38 +0000</pubDate>
      
      <guid>/post/bigdata/scala/scala%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Scala高级特性&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_高阶函数&#34;&gt;1. 高阶函数&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_概念&#34;&gt;1.1. 概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_作为值的函数&#34;&gt;1.2. 作为值的函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_匿名函数&#34;&gt;1.3. 匿名函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_将方法转换成函数&#34;&gt;1.4. 将方法转换成函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_柯里化&#34;&gt;1.5. 柯里化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_例子&#34;&gt;1.6. 例子&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_隐式转换和隐式参数&#34;&gt;2. 隐式转换和隐式参数&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_概念_2&#34;&gt;2.1. 概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_作用&#34;&gt;2.2. 作用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_隐式转换函数&#34;&gt;2.3. 隐式转换函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_隐式转换例子&#34;&gt;2.4. 隐式转换例子&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_隐式转换变量值&#34;&gt;2.4.1. 隐式转换变量值&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_隐式转换方法&#34;&gt;2.4.2. 隐式转换方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_高阶函数&#34;&gt;1. 高阶函数&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_概念&#34;&gt;1.1. 概念&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala混合了面向对象和函数式的特性，我们通常将可以做为参数传递到方法中的表达式叫做函数。在函数式编程语言中，函数是“头等公民”，高阶函数包含：作为值的函数、匿名函数、闭包、柯里化等等。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_作为值的函数&#34;&gt;1.2. 作为值的函数&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;可以像任何其他数据类型一样被传递和操作的函数，每当你想要给算法传入具体动作时这个特性就会变得非常有用。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145546.png&#34; alt=&#34;2017 04 07 145546&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;定义函数时格式：val 变量名 =(输入参数类型和个数)&amp;#8658;函数实现和返回值类型和个数
“=”表示将函数赋给一个变量
“&amp;#8658;”左面表示输入参数名称、类型和个数，右边表示函数的实现和返回值类型和参数个数&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_匿名函数&#34;&gt;1.3. 匿名函数&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中，你不需要给每一个函数命名，没有将函数赋给变量的函数叫做匿名函数&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145555.png&#34; alt=&#34;2017 04 07 145555&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;由于Scala可以自动推断出参数的类型，所有可以写的跟精简一些&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145603.png&#34; alt=&#34;2017 04 07 145603&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;还记得神奇的下划线吗？这才是终极方式&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145612.png&#34; alt=&#34;2017 04 07 145612&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_将方法转换成函数&#34;&gt;1.4. 将方法转换成函数&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中，方法和函数是不一样的，最本质的区别是函数可以做为参数传递到方法中
但是方法可以被转换成函数，神奇的下划线又出场了&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145620.png&#34; alt=&#34;2017 04 07 145620&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_柯里化&#34;&gt;1.5. 柯里化&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的方法的过程&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145627.png&#34; alt=&#34;2017 04 07 145627&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_例子&#34;&gt;1.6. 例子&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object FunDemo {
  def main(args: Array[String]) {
    def f2(x: Int) = x * 2
    val f3 = (x: Int) =&amp;gt; x * 3
    val f4: (Int) =&amp;gt; Int = { x =&amp;gt; x * 4 }
    val f4a: (Int) =&amp;gt; Int = _ * 4
    val f5 = (_: Int) * 5
    val list = List(1, 2, 3, 4, 5)
    var new_list: List[Int] = null
    //第一种：最直观的方式 (Int) =&amp;gt; Int
    //new_list = list.map((x: Int) =&amp;gt; x * 3)

    //第二种：由于map方法知道你会传入一个类型为(Int) =&amp;gt; Int的函数，你可以简写
    //new_list = list.map((x) =&amp;gt; x * 3)

    //第三种：对于只有一个参数的函数，你可以省去参数外围的()
    //new_list = list.map(x =&amp;gt; x * 3)

    //第四种：(终极方式)如果参数在=&amp;gt;右侧只出现一次，可以使用_
    new_list = list.map(_ * 3)

    new_list.foreach(println(_))

    var a = Array(1,2,3)
    a.map(_* 3)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_隐式转换和隐式参数&#34;&gt;2. 隐式转换和隐式参数&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_概念_2&#34;&gt;2.1. 概念&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;隐式转换和隐式参数是Scala中两个非常强大的功能，利用隐式转换和隐式参数，你可以提供优雅的类库，对类库的使用者隐匿掉那些枯燥乏味的细节。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_作用&#34;&gt;2.2. 作用&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;隐式的对类的方法进行增强，丰富现有类库的功能&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_隐式转换函数&#34;&gt;2.3. 隐式转换函数&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;是指那种以implicit关键字声明的带有单个参数的函数&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_隐式转换例子&#34;&gt;2.4. 隐式转换例子&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;//隐式的增强File类的方法
class RichFile(val from: File) {
  def read = Source.fromFile(from.getPath).mkString
}

object RichFile {
  //隐式转换方法
  implicit def file2RichFile(from: File) = new RichFile(from)

}

object MainApp{
  def main(args: Array[String]): Unit = {
    //导入隐式转换
    import RichFile._
    //import RichFile.file2RichFile
    println(new File(&#34;c://words.txt&#34;).read)

  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import java.awt.GridLayout

object ImplicitContext {

  //implicit def girl2Ordered(g : Girl) = new Ordered[Girl]{
  //  override def compare(that: Girl): Int = if (g.faceValue &amp;gt; that.faceValue) 1 else -1
  //}

  implicit object OrderingGirl extends Ordering[Girl] {
    override def compare(x: Girl, y: Girl): Int = if (x.faceValue &amp;gt; y.faceValue) 1 else -1
  }

}

class Girl(var name: String, var faceValue: Double) {
  override def toString: String = s&#34;name : $name, faveValue : $faceValue&#34;
}

//class MissRight[T &amp;lt;% Ordered[T]](f: T, s: T){
//  def choose() = if(f &amp;gt; s) f else s
//}
//class MissRight[T](f: T, s: T){
//  def choose()(implicit ord: T =&amp;gt; Ordered[T]) = if (f &amp;gt; s) f else s
//}

class MissRight[T: Ordering](val f: T, val s: T) {
  def choose()(implicit ord: Ordering[T]) = if (ord.gt(f, s)) f else s
}

object MissRight {
  def main(args: Array[String]) {
    import ImplicitContext.OrderingGirl
    val g1 = new Girl(&#34;yuihatano&#34;, 99)
    val g2 = new Girl(&#34;jzmb&#34;, 98)
    val mr = new MissRight(g1, g2)
    val result = mr.choose()
    println(result)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_隐式转换变量值&#34;&gt;2.4.1. 隐式转换变量值&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/svg/scala-implic.svg&#34; alt=&#34;scala implic&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_隐式转换方法&#34;&gt;2.4.2. 隐式转换方法&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/svg/scala-implic2.svg&#34; alt=&#34;scala implic2&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scala Actor编程</title>
      <link>/post/bigdata/scala/scala-Actor%E7%BC%96%E7%A8%8B/</link>
      <pubDate>Wed, 05 Apr 2017 09:47:03 +0000</pubDate>
      
      <guid>/post/bigdata/scala/scala-Actor%E7%BC%96%E7%A8%8B/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Scala Actor编程&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_scala_actor编程&#34;&gt;1. Scala Actor编程&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_什么是scala_actor&#34;&gt;1.1. 什么是Scala Actor&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_概念&#34;&gt;1.1.1. 概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_传统java并发编程与scala_actor编程的区别&#34;&gt;1.1.2. 传统java并发编程与Scala Actor编程的区别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_actor方法执行顺序&#34;&gt;1.1.3. Actor方法执行顺序&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_发送消息的方式&#34;&gt;1.1.4. 发送消息的方式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_actor实战&#34;&gt;1.2. Actor实战&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_第一个例子&#34;&gt;1.2.1. 第一个例子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_第二个例子_可以不断地接收消息&#34;&gt;1.2.2. 第二个例子（可以不断地接收消息）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_第三个例子_react方式会复用线程_比receive更高效&#34;&gt;1.2.3. 第三个例子（react方式会复用线程，比receive更高效）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_第四个例子_结合case_class发送消息&#34;&gt;1.2.4. 第四个例子（结合case class发送消息）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_练习&#34;&gt;1.3. 练习&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_scala_actor编程&#34;&gt;1. Scala Actor编程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_什么是scala_actor&#34;&gt;1.1. 什么是Scala Actor&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_概念&#34;&gt;1.1.1. 概念&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala中的Actor能够实现并行编程的强大功能，它是基于事件模型的并发机制，Scala是运用消息（message）的发送、接收来实现多线程的。使用Scala能够更容易地实现多线程应用的开发。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_传统java并发编程与scala_actor编程的区别&#34;&gt;1.1.2. 传统java并发编程与Scala Actor编程的区别&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-05_095545.png&#34; alt=&#34;2017 04 05 095545&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;对于Java，我们都知道它的多线程实现需要对共享资源（变量、对象等）使用synchronized 关键字进行代码块同步、对象锁互斥等等。而且，常常一大块的try…catch语句块中加上wait方法、notify方法、notifyAll方法是让人很头疼的。原因就在于Java中多数使用的是可变状态的对象资源，对这些资源进行共享来实现多线程编程的话，控制好资源竞争与防止对象状态被意外修改是非常重要的，而对象状态的不变性也是较难以保证的。 而在Scala中，我们可以通过复制不可变状态的资源（即对象，Scala中一切都是对象，连函数、方法也是）的一个副本，再基于Actor的消息发送、接收机制进行并行编程&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_actor方法执行顺序&#34;&gt;1.1.3. Actor方法执行顺序&lt;/h4&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;首先调用start()方法启动Actor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用start()方法后其act()方法会被执行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向Actor发送消息&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_发送消息的方式&#34;&gt;1.1.4. 发送消息的方式&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;!
发送异步消息，没有返回值。
!?
发送同步消息，等待返回值。
!!
发送异步消息，返回值是 Future[Any]。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_actor实战&#34;&gt;1.2. Actor实战&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_第一个例子&#34;&gt;1.2.1. 第一个例子&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object MyActor1 extends Actor{
  //重新act方法
  def act(){
    for(i &amp;lt;- 1 to 10){
      println(&#34;actor-1 &#34;+ i)
      Thread.sleep(2000)
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object MyActor2 extends Actor{
  //重新act方法
  def act(){
    for(i &amp;lt;- 1 to 10){
      println(&#34;actor-2 &#34;+ i)
      Thread.sleep(2000)
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ActorTest extends App{
  //启动Actor
  MyActor1.start()
  MyActor2.start()
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：上面分别调用了两个单例对象的start()方法，他们的act()方法会被执行，相同与在java中开启了两个线程，线程的run()方法会被执行
注意：这两个Actor是并行执行的，act()方法中的for循环执行完成后actor程序就退出了&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_第二个例子_可以不断地接收消息&#34;&gt;1.2.2. 第二个例子（可以不断地接收消息）&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class MyActor extends Actor {

  override def act(): Unit = {
    while (true) {
      receive {
        case &#34;start&#34;=&amp;gt; {
          println(&#34;starting ...&#34;)
          Thread.sleep(5000)
          println(&#34;started&#34;)
        }
        case &#34;stop&#34;=&amp;gt; {
          println(&#34;stopping ...&#34;)
          Thread.sleep(5000)
          println(&#34;stopped ...&#34;)
        }
      }
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object MyActor {
  def main(args: Array[String]) {
    val actor = new MyActor
    actor.start()
    actor ! &#34;start&#34;
    actor ! &#34;stop&#34;
    println(&#34;消息发送完成！&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：在act()方法中加入了while (true) 循环，就可以不停的接收消息
注意：发送start消息和stop的消息是异步的，但是Actor接收到消息执行的过程是同步的按顺序执行&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_第三个例子_react方式会复用线程_比receive更高效&#34;&gt;1.2.3. 第三个例子（react方式会复用线程，比receive更高效）&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class YourActor extends Actor {

  override def act(): Unit = {
    loop {
      react {
        case &#34;start&#34;=&amp;gt; {
          println(&#34;starting ...&#34;)
          Thread.sleep(5000)
          println(&#34;started&#34;)
        }
        case &#34;stop&#34;=&amp;gt; {
          println(&#34;stopping ...&#34;)
          Thread.sleep(8000)
          println(&#34;stopped ...&#34;)
        }
      }
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object YourActor {
  def main(args: Array[String]) {
    val actor = new YourActor
    actor.start()
    actor ! &#34;start&#34;
    actor ! &#34;stop&#34;
    println(&#34;消息发送完成！&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：?react 如果要反复执行消息处理，react外层要用loop，不能用while&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_第四个例子_结合case_class发送消息&#34;&gt;1.2.4. 第四个例子（结合case class发送消息）&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class AppleActor extends Actor {

  def act(): Unit = {
    while (true) {
      receive {
        case &#34;start&#34;=&amp;gt;println(&#34;starting ...&#34;)
        case SyncMsg(id, msg) =&amp;gt; {
          println(id + &#34;,sync &#34;+ msg)
          Thread.sleep(5000)
          sender ! ReplyMsg(3,&#34;finished&#34;)
        }
        case AsyncMsg(id, msg) =&amp;gt; {
          println(id + &#34;,async &#34;+ msg)
          Thread.sleep(5000)
        }
      }
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object AppleActor {
  def main(args: Array[String]) {
    val a = new AppleActor
    a.start()
    //异步消息
    a ! AsyncMsg(1, &#34;hello actor&#34;)
    println(&#34;异步消息发送完成&#34;)
    //同步消息
    //val content = a.!?(1000, SyncMsg(2, &#34;hello actor&#34;))
    //println(content)
    val reply = a !! SyncMsg(2, &#34;hello actor&#34;)
    println(reply.isSet)
    //println(&#34;123&#34;)
    val c = reply.apply()
    println(reply.isSet)
    println(c)
  }
}
case class SyncMsg(id : Int, msg: String)
case class AsyncMsg(id : Int, msg: String)
case class ReplyMsg(id : Int, msg: String)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_练习&#34;&gt;1.3. 练习&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;用actor并发编程写一个单机版的WorldCount，将多个文件作为输入，计算完成后将多个任务汇总，得到最终的结果&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class Task extends Actor {

  override def act(): Unit = {
    loop {
      react {
        case SubmitTask(fileName) =&amp;gt; {
          val contents = Source.fromFile(new File(fileName)).mkString
          val arr = contents.split(&#34;\r\n&#34;)
          val result = arr.flatMap(_.split(&#34;&#34;)).map((_, 1)).groupBy(_._1).mapValues(_.length)
          //val result = arr.flatMap(_.split(&#34;&#34;)).map((_, 1)).groupBy(_._1).mapValues(_.foldLeft(0)(_ + _._2))
          sender ! ResultTask(result)
        }
        case StopTask =&amp;gt; {
          exit()
        }
      }
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object WorkCount {
  def main(args: Array[String]) {
    val files = Array(&#34;c://words.txt&#34;, &#34;c://words.log&#34;)

    val replaySet = new mutable.HashSet[Future[Any]]
    val resultList = new mutable.ListBuffer[ResultTask]

    for(f &amp;lt;- files) {
      val t = new Task
      val replay = t.start() !! SubmitTask(f)
      replaySet += replay
    }

    while(replaySet.size &amp;gt;0){
      val toCumpute = replaySet.filter(_.isSet)
      for(r &amp;lt;- toCumpute){
        val result = r.apply()
        resultList += result.asInstanceOf[ResultTask]
        replaySet.remove(r)
      }
      Thread.sleep(100)
    }
    val finalResult = resultList.map(_.result).flatten.groupBy(_._1).mapValues(x =&amp;gt; x.foldLeft(0)(_ + _._2))
    println(finalResult)
  }
}

case class SubmitTask(fileName: String)
case object StopTask
case class ResultTask(result: Map[String, Int])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>scala基础</title>
      <link>/post/bigdata/scala/scala%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Tue, 04 Apr 2017 15:25:28 +0000</pubDate>
      
      <guid>/post/bigdata/scala/scala%E5%9F%BA%E7%A1%80/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;scala基础&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_scala基础&#34;&gt;1. Scala基础&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_声明变量&#34;&gt;1.1. 声明变量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_常用类型&#34;&gt;1.2. 常用类型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_条件表达式&#34;&gt;1.3. 条件表达式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_块表达式&#34;&gt;1.4. 块表达式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_循环&#34;&gt;1.5. 循环&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_调用方法和函数&#34;&gt;1.6. 调用方法和函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_定义方法和函数&#34;&gt;1.7. 定义方法和函数&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_定义方法&#34;&gt;1.7.1. 定义方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_定义函数&#34;&gt;1.7.2. 定义函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_方法和函数的区别&#34;&gt;1.7.3. 方法和函数的区别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_将方法转换成函数_神奇的下划线&#34;&gt;1.7.4. 将方法转换成函数（神奇的下划线）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_数组_映射_元组_集合&#34;&gt;2. 数组、映射、元组、集合&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_数组&#34;&gt;2.1. 数组&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_定长数组和变长数组&#34;&gt;2.1.1. 定长数组和变长数组&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_遍历数组&#34;&gt;2.1.2. 遍历数组&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_数组转换&#34;&gt;2.1.3. 数组转换&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_数组常用算法&#34;&gt;2.1.4. 数组常用算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_映射&#34;&gt;2.2. 映射&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_构建映射&#34;&gt;2.2.1. 构建映射&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_获取和修改映射中的值&#34;&gt;2.2.2. 获取和修改映射中的值&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_元组&#34;&gt;2.3. 元组&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_创建元组&#34;&gt;2.3.1. 创建元组&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_获取元组中的值&#34;&gt;2.3.2. 获取元组中的值&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_将对偶的集合转换成映射&#34;&gt;2.3.3. 将对偶的集合转换成映射&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_拉链操作&#34;&gt;2.3.4. 拉链操作&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_集合&#34;&gt;2.4. 集合&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_序列&#34;&gt;2.4.1. 序列&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_set&#34;&gt;2.5. Set&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_map&#34;&gt;2.6. Map&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_类_对象_继承_特质&#34;&gt;3. 类、对象、继承、特质&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_类&#34;&gt;3.1. 类&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_类的定义&#34;&gt;3.1.1. 类的定义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_构造器&#34;&gt;3.1.2. 构造器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_对象&#34;&gt;3.2. 对象&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_单例对象&#34;&gt;3.2.1. 单例对象&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_伴生对象&#34;&gt;3.2.2. 伴生对象&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_apply方法&#34;&gt;3.2.3. apply方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_应用程序对象&#34;&gt;3.2.4. 应用程序对象&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_继承&#34;&gt;3.3. 继承&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_扩展类&#34;&gt;3.3.1. 扩展类&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_重写方法&#34;&gt;3.3.2. 重写方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_类型检查和转换&#34;&gt;3.3.3. 类型检查和转换&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_超类的构造&#34;&gt;3.3.4. 超类的构造&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_模式匹配和样例类&#34;&gt;4. 模式匹配和样例类&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_匹配字符串&#34;&gt;4.1. 匹配字符串&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_匹配类型&#34;&gt;4.2. 匹配类型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_匹配数组_元组&#34;&gt;4.3. 匹配数组、元组&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_样例类&#34;&gt;4.4. 样例类&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_option类型&#34;&gt;4.5. Option类型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_偏函数&#34;&gt;4.6. 偏函数&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_单机版_wordcount&#34;&gt;5. 单机版 wordcount&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_scala基础&#34;&gt;1. Scala基础&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_声明变量&#34;&gt;1.1. 声明变量&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object VariableDemo {
  def main(args: Array[String]) {
    //使用val定义的变量值是不可变的，相当于java里用final修饰的变量
    val i = 1
    //使用var定义的变量是可变得，在Scala中鼓励使用val
    var s = &#34;hello&#34;
    //Scala编译器会自动推断变量的类型，必要的时候可以指定类型
    //变量名在前，类型在后
    val str: String = &#34;itcast&#34;
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_常用类型&#34;&gt;1.2. 常用类型&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;Scala&lt;/strong&gt; 和 &lt;strong&gt;Java&lt;/strong&gt; 一样，有7种数值类型 &lt;strong&gt;Byte&lt;/strong&gt; 、 &lt;strong&gt;Char&lt;/strong&gt; 、 &lt;strong&gt;Short&lt;/strong&gt; 、 &lt;strong&gt;Int&lt;/strong&gt; 、 &lt;strong&gt;Long&lt;/strong&gt; 、 &lt;strong&gt;Float&lt;/strong&gt; 和 &lt;strong&gt;Double&lt;/strong&gt; （无包装类型）和一个 &lt;strong&gt;Boolean&lt;/strong&gt; 类型&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_条件表达式&#34;&gt;1.3. 条件表达式&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala的的条件表达式比较简洁，例如：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ConditionDemo {
  def main(args: Array[String]) {
    val x = 1
    //判断x的值，将结果赋给y
    val y = if (x &amp;gt; 0) 1 else -1
    //打印y的值
    println(y)

    //支持混合类型表达式
    val z = if (x &amp;gt; 1) 1 else &#34;error&#34;
    //打印z的值
    println(z)

    //如果缺失else，相当于if (x &amp;gt; 2) 1 else ()
    val m = if (x &amp;gt; 2) 1
    println(m)

    //在scala中每个表达式都有值，scala中有个Unit类，写做(),相当于Java中的void
    val n = if (x &amp;gt; 2) 1 else ()
    println(n)

    //if和else if
    val k = if (x &amp;lt; 0) 0
    else if (x &amp;gt;= 1) 1 else -1
    println(k)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_块表达式&#34;&gt;1.4. 块表达式&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object BlockExpressionDemo {
  def main(args: Array[String]) {
    val x = 0
    //在scala中{}中课包含一系列表达式，块中最后一个表达式的值就是块的值
    //下面就是一个块表达式
    val result = {
      if (x &amp;lt; 0){
        -1
      } else if(x &amp;gt;= 1) {
        1
      } else {
        &#34;error&#34;
      }
    }
    //result的值就是块表达式的结果
    println(result)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_循环&#34;&gt;1.5. 循环&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在scala中有for循环和while循环，用for循环比较多
for循环语法结构：for (i &amp;#8592; 表达式/数组/集合)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ForDemo {
  def main(args: Array[String]) {
    //for(i &amp;lt;- 表达式),表达式1 to 10返回一个Range（区间）
    //每次循环将区间中的一个值赋给i
    for (i &amp;lt;- 1 to 10)
      println(i)

    //for(i &amp;lt;- 数组)
    val arr = Array(&#34;a&#34;, &#34;b&#34;, &#34;c&#34;)
    for (i &amp;lt;- arr)
      println(i)

    //高级for循环
    //每个生成器都可以带一个条件，注意：if前面没有分号
    for(i &amp;lt;- 1 to 3; j &amp;lt;- 1 to 3 if i != j)
      print((10 * i + j) + &#34; &#34;)
    println()

    //for推导式：如果for循环的循环体以yield开始，则该循环会构建出一个集合
    //每次迭代生成集合中的一个值
    val v = for (i &amp;lt;- 1 to 10) yield i * 10
    println(v)

  }

}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_调用方法和函数&#34;&gt;1.6. 调用方法和函数&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala中的+ - * / %等操作符的作用与Java一样，位操作符 &amp;amp; | ^ &amp;gt;&amp;gt; &amp;lt;&amp;lt;也一样。只是有
一点特别的：这些操作符实际上是方法。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;例如：
　　a + b
是如下方法调用的简写：
　　a. +(b)
a 方法 b可以写成 a.方法(b)&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_定义方法和函数&#34;&gt;1.7. 定义方法和函数&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_定义方法&#34;&gt;1.7.1. 定义方法&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162650.png&#34; alt=&#34;2017 04 04 162650&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;方法的返回值类型可以不写，编译器可以自动推断出来，但是对于递归函数，必须指定返回类型&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_定义函数&#34;&gt;1.7.2. 定义函数&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162701.png&#34; alt=&#34;2017 04 04 162701&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_方法和函数的区别&#34;&gt;1.7.3. 方法和函数的区别&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在函数式编程语言中，函数是“头等公民”，它可以像任何其他数据类型一样被传递和操作&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+
image::/src/img/scala/2017-04-04_162709.png[]
---
案例：首先定义一个方法，再定义一个函数，然后将函数传递到方法里面&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object MethodAndFunctionDemo {
  //定义一个方法
  //方法m2参数要求是一个函数，函数的参数必须是两个Int类型
  //返回值类型也是Int类型
  def m1(f: (Int, Int) =&amp;gt; Int) : Int = {
    f(2, 6)
  }

  //定义一个函数f1，参数是两个Int类型，返回值是一个Int类型
  val f1 = (x: Int, y: Int) =&amp;gt; x + y
  //再定义一个函数f2
  val f2 = (m: Int, n: Int) =&amp;gt; m * n

  //main方法
  def main(args: Array[String]) {

    //调用m1方法，并传入f1函数
    val r1 = m1(f1)
    println(r1)

    //调用m1方法，并传入f2函数
    val r2 = m1(f2)
    println(r2)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_将方法转换成函数_神奇的下划线&#34;&gt;1.7.4. 将方法转换成函数（神奇的下划线）&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162717.png&#34; alt=&#34;2017 04 04 162717&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_数组_映射_元组_集合&#34;&gt;2. 数组、映射、元组、集合&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_数组&#34;&gt;2.1. 数组&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_定长数组和变长数组&#34;&gt;2.1.1. 定长数组和变长数组&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.mutable.ArrayBuffer
object ArrayDemo {

  def main(args: Array[String]) {

    //初始化一个长度为8的定长数组，其所有元素均为0
    val arr1 = new Array[Int](8)
    //直接打印定长数组，内容为数组的hashcode值
    println(arr1)
    //将数组转换成数组缓冲，就可以看到原数组中的内容了
    //toBuffer会将数组转换长数组缓冲
    println(arr1.toBuffer)

    //注意：如果new，相当于调用了数组的apply方法，直接为数组赋值
    //初始化一个长度为1的定长数组
    val arr2 = Array[Int](10)
    println(arr2.toBuffer)

    //定义一个长度为3的定长数组
    val arr3 = Array(&#34;hadoop&#34;, &#34;storm&#34;, &#34;spark&#34;)
    //使用()来访问元素
    println(arr3(2))

    //////////////////////////////////////////////////
    //变长数组（数组缓冲）
    //如果想使用数组缓冲，需要导入import scala.collection.mutable.ArrayBuffer包
    val ab = ArrayBuffer[Int]()
    //向数组缓冲的尾部追加一个元素
    //+=尾部追加元素
    ab += 1
    //追加多个元素
    ab += (2, 3, 4, 5)
    //追加一个数组++=
    ab ++= Array(6, 7)
    //追加一个数组缓冲
    ab ++= ArrayBuffer(8,9)
    //打印数组缓冲ab

    //在数组某个位置插入元素用insert
    ab.insert(0, -1, 0)
    //删除数组某个位置的元素用remove
    ab.remove(8, 2)
    println(ab)

  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_遍历数组&#34;&gt;2.1.2. 遍历数组&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1.增强for循环
2.好用的until会生成脚标，0 until 10 包含0不包含10&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162727.png&#34; alt=&#34;2017 04 04 162727&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ForArrayDemo {

  def main(args: Array[String]) {
    //初始化一个数组
    val arr = Array(1,2,3,4,5,6,7,8)
    //增强for循环
    for(i &amp;lt;- arr)
      println(i)

    //好用的until会生成一个Range
    //reverse是将前面生成的Range反转
    for(i &amp;lt;- (0 until arr.length).reverse)
      println(arr(i))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_数组转换&#34;&gt;2.1.3. 数组转换&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;yield关键字将原始的数组进行转换会产生一个新的数组，原始的数组不变&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162736.png&#34; alt=&#34;2017 04 04 162736&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ArrayYieldDemo {
  def main(args: Array[String]) {
    //定义一个数组
    val arr = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)
    //将偶数取出乘以10后再生成一个新的数组
    val res = for (e &amp;lt;- arr if e % 2 == 0) yield e * 10
    println(res.toBuffer)

    //更高级的写法,用着更爽
    //filter是过滤，接收一个返回值为boolean的函数
    //map相当于将数组中的每一个元素取出来，应用传进去的函数
    val r = arr.filter(_ % 2 == 0).map(_ * 10)
    println(r.toBuffer)

  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_数组常用算法&#34;&gt;2.1.4. 数组常用算法&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中，数组上的某些方法对数组进行相应的操作非常方便！&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162743.png&#34; alt=&#34;2017 04 04 162743&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_映射&#34;&gt;2.2. 映射&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中，把哈希表这种数据结构叫做映射&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_构建映射&#34;&gt;2.2.1. 构建映射&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162749.png&#34; alt=&#34;2017 04 04 162749&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_获取和修改映射中的值&#34;&gt;2.2.2. 获取和修改映射中的值&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162755.png&#34; alt=&#34;2017 04 04 162755&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;好用的getOrElse&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162802.png&#34; alt=&#34;2017 04 04 162802&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注意：在Scala中，有两种Map，一个是immutable包下的Map，该Map中的内容不可变；另一个是mutable包下的Map，该Map中的内容可变
例子：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162812.png&#34; alt=&#34;2017 04 04 162812&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注意：通常我们在创建一个集合是会用val这个关键字修饰一个变量（相当于java中的final），那么就意味着该变量的引用不可变，该引用中的内容是不是可变，取决于这个引用指向的集合的类型&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_元组&#34;&gt;2.3. 元组&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;映射是K/V对偶的集合，对偶是元组的最简单形式，元组可以装着多个不同类型的值。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_创建元组&#34;&gt;2.3.1. 创建元组&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162819.png&#34; alt=&#34;2017 04 04 162819&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_获取元组中的值&#34;&gt;2.3.2. 获取元组中的值&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162828.png&#34; alt=&#34;2017 04 04 162828&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_将对偶的集合转换成映射&#34;&gt;2.3.3. 将对偶的集合转换成映射&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162835.png&#34; alt=&#34;2017 04 04 162835&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_拉链操作&#34;&gt;2.3.4. 拉链操作&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;zip命令可以将多个值绑定在一起&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162841.png&#34; alt=&#34;2017 04 04 162841&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注意：如果两个数组的元素个数不一致，拉链操作后生成的数组的长度为较小的那个数组的元素个数&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_集合&#34;&gt;2.4. 集合&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Scala的集合有三大类&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;序列Seq、集Set、映射Map，所有的集合都扩展自Iterable特质
在Scala中集合有可变（mutable）和不可变（immutable）两种类型，immutable类型的集合初始化后就不能改变了（注意与val修饰的变量进行区别）&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_序列&#34;&gt;2.4.1. 序列&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;不可变的序列 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;code&gt;import scala.collection.immutable._&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;在Scala中列表要么为空（Nil表示空列表）要么是一个head元素加上一个tail列表。
9 :: List(5, 2)  :: 操作符是将给定的头和尾创建一个新的列表
注意：:: 操作符是右结合的，如9 :: 5 :: 2 :: Nil相当于 9 :: (5 :: (2 :: Nil))&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ImmutListDemo {

  def main(args: Array[String]) {
    //创建一个不可变的集合
    val lst1 = List(1,2,3)
    //将0插入到lst1的前面生成一个新的List
    val lst2 = 0 :: lst1
    val lst3 = lst1.::(0)
    val lst4 = 0 +: lst1
    val lst5 = lst1.+:(0)

    //将一个元素添加到lst1的后面产生一个新的集合
    val lst6 = lst1 :+ 3

    val lst0 = List(4,5,6)
    //将2个list合并成一个新的List
    val lst7 = lst1 ++ lst0
    //将lst1插入到lst0前面生成一个新的集合
    val lst8 = lst1 ++: lst0

    //将lst0插入到lst1前面生成一个新的集合
    val lst9 = lst1.:::(lst0)

    println(lst9)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;可变的序列 &lt;code&gt;import scala.collection.mutable._&lt;/code&gt; &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.mutable.ListBuffer

object MutListDemo extends App{
  //构建一个可变列表，初始有3个元素1,2,3
  val lst0 = ListBuffer[Int](1,2,3)
  //创建一个空的可变列表
  val lst1 = new ListBuffer[Int]
  //向lst1中追加元素，注意：没有生成新的集合
  lst1 += 4
  lst1.append(5)

  //将lst1中的元素最近到lst0中， 注意：没有生成新的集合
  lst0 ++= lst1

  //将lst0和lst1合并成一个新的ListBuffer 注意：生成了一个集合
  val lst2= lst0 ++ lst1

  //将元素追加到lst0的后面生成一个新的集合
  val lst3 = lst0 :+ 5
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_set&#34;&gt;2.5. Set&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;不可变的Set&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.immutable.HashSet

object ImmutSetDemo extends App{
  val set1 = new HashSet[Int]()
  //将元素和set1合并生成一个新的set，原有set不变
  val set2 = set1 + 4
  //set中元素不能重复
  val set3 = set1 ++ Set(5, 6, 7)
  val set0 = Set(1,3,4) ++ set1
  println(set0.getClass)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;可变的Set&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.mutable

object MutSetDemo extends App{
  //创建一个可变的HashSet
  val set1 = new mutable.HashSet[Int]()
  //向HashSet中添加元素
  set1 += 2
  //add等价于+=
  set1.add(4)
  set1 ++= Set(1,3,5)
  println(set1)
  //删除一个元素
  set1 -= 5
  set1.remove(2)
  println(set1)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_map&#34;&gt;2.6. Map&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.mutable

object MutMapDemo extends App{
  val map1 = new mutable.HashMap[String, Int]()
  //向map中添加数据
  map1(&#34;spark&#34;) = 1
  map1 += ((&#34;hadoop&#34;, 2))
  map1.put(&#34;storm&#34;, 3)
  println(map1)

  //从map中移除元素
  map1 -= &#34;spark&#34;
  map1.remove(&#34;hadoop&#34;)
  println(map1)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_类_对象_继承_特质&#34;&gt;3. 类、对象、继承、特质&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala的类与Java、C++的类比起来更简洁，学完之后你会更爱Scala！！！&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_类&#34;&gt;3.1. 类&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_类的定义&#34;&gt;3.1.1. 类的定义&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;//在Scala中，类并不用声明为public。
//Scala源文件中可以包含多个类，所有这些类都具有公有可见性。
class Person {
  //用val修饰的变量是只读属性，有getter但没有setter
  //（相当与Java中用final修饰的变量）
  val id = &#34;9527&#34;

  //用var修饰的变量既有getter又有setter
  var age: Int = 18

  //类私有字段,只能在类的内部使用
  private var name: String = &#34;唐伯虎&#34;

  //对象私有字段,访问权限更加严格的，Person类的方法只能访问到当前对象的字段
  private[this] val pet = &#34;小强&#34;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_构造器&#34;&gt;3.1.2. 构造器&lt;/h4&gt;
&lt;div class=&#34;admonitionblock important&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-important&#34; title=&#34;Important&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
主构造器会执行类定义中的所有语句
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/**
  *每个类都有主构造器，主构造器的参数直接放置类名后面，与类交织在一起
  */
class Student(val name: String, val age: Int){
  //主构造器会执行类定义中的所有语句
  println(&#34;执行主构造器&#34;)

  try {
    println(&#34;读取文件&#34;)
    throw new IOException(&#34;io exception&#34;)
  } catch {
    case e: NullPointerException =&amp;gt; println(&#34;打印异常Exception : &#34; + e)
    case e: IOException =&amp;gt; println(&#34;打印异常Exception : &#34; + e)
  } finally {
    println(&#34;执行finally部分&#34;)
  }

  private var gender = &#34;male&#34;

  //用this关键字定义辅助构造器
  def this(name: String, age: Int, gender: String){
    //每个辅助构造器必须以主构造器或其他的辅助构造器的调用开始
    this(name, age)
    println(&#34;执行辅助构造器&#34;)
    this.gender = gender
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/**
  *构造器参数可以不带val或var，如果不带val或var的参数至少被一个方法所使用，
  *那么它将会被提升为字段
  */
//在类名后面加private就变成了私有的
class Queen private(val name: String, prop: Array[String], private var age: Int = 18){

  println(prop.size)

  //prop被下面的方法使用后，prop就变成了不可变得对象私有字段，等同于private[this] val prop
  //如果没有被方法使用该参数将不被保存为字段，仅仅是一个可以被主构造器中的代码访问的普通参数
  def description = name + &#34; is &#34; + age + &#34; years old with &#34; + prop.toBuffer
}

object Queen{
  def main(args: Array[String]) {
    //私有的构造器，只有在其伴生对象中使用
    val q = new Queen(&#34;hatano&#34;, Array(&#34;蜡烛&#34;, &#34;皮鞭&#34;), 20)
    println(q.description())
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_对象&#34;&gt;3.2. 对象&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_单例对象&#34;&gt;3.2.1. 单例对象&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中没有静态方法和静态字段，但是可以使用object这个语法结构来达到同样的目的&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;存放工具方法和常量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;高效共享单个不可变的实例&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;单例模式&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.mutable.ArrayBuffer

object SingletonDemo {
  def main(args: Array[String]) {
    //单例对象，不需要new，用【类名.方法】调用对象中的方法
    val session = SessionFactory.getSession()
    println(session)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object SessionFactory{
  //该部分相当于java中的静态块
  var counts = 5
  val sessions = new ArrayBuffer[Session]()
  while(counts &amp;gt; 0){
    sessions += new Session
    counts -= 1
  }

  //在object中的方法相当于java中的静态方法
  def getSession(): Session ={
    sessions.remove(0)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class Session{

}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_伴生对象&#34;&gt;3.2.2. 伴生对象&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala的类中，与类名相同的对象叫做伴生对象，类和伴生对象之间可以相互访问私有的方法和属性&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class Dog {
  val id = 1
  private var name = &#34;itcast&#34;

  def printName(): Unit ={
    //在Dog类中可以访问伴生对象Dog的私有属性
    println(Dog.CONSTANT + name )
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/**
  * 伴生对象
  */
object Dog {

  //伴生对象中的私有属性
  private val CONSTANT = &#34;汪汪汪 : &#34;

  def main(args: Array[String]) {
    val p = new Dog
    //访问私有的字段name
    p.name = &#34;123&#34;
    p.printName()
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_apply方法&#34;&gt;3.2.3. apply方法&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;通常我们会在类的伴生对象中定义apply方法，当遇到类名(参数1,&amp;#8230;&amp;#8203;参数n)时apply方法会被调用&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ApplyDemo {
  def main(args: Array[String]) {
    //调用了Array伴生对象的apply方法
    //def apply(x: Int, xs: Int*): Array[Int]
    //arr1中只有一个元素5
    val arr1 = Array(5)
    println(arr1.toBuffer)

    //new了一个长度为5的array，数组里面包含5个null
    var arr2 = new Array(5)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_应用程序对象&#34;&gt;3.2.4. 应用程序对象&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala程序都必须从一个对象的main方法开始，可以通过扩展App特质，不写main方法。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object AppObjectDemo extends App{
  //不用写main方法
  println(&#34;I love you Scala&#34;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_继承&#34;&gt;3.3. 继承&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_扩展类&#34;&gt;3.3.1. 扩展类&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中扩展类的方式和Java一样都是使用extends关键字&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_重写方法&#34;&gt;3.3.2. 重写方法&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中重写一个非抽象的方法必须使用override修饰符&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_类型检查和转换&#34;&gt;3.3.3. 类型检查和转换&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;Scala
Java
obj.isInstanceOf[C]
obj instanceof C
obj.asInstanceOf[C]
(C)obj
classOf[C]
C.class&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_超类的构造&#34;&gt;3.3.4. 超类的构造&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ClazzDemo {
  def main(args: Array[String]) {
    //val h = new Human
    //println(h.fight)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;trait Flyable{
  def fly(): Unit ={
    println(&#34;I can fly&#34;)
  }

  def fight(): String
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;abstract class Animal {
  def run(): Int
  val name: String
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class Human extends Animal with Flyable{

  val name = &#34;abc&#34;

  //打印几次&#34;ABC&#34;?
  val t1,t2,(a, b, c) = {
    println(&#34;ABC&#34;)
    (1,2,3)
  }

  println(a)
  println(t1._1)

  //在Scala中重写一个非抽象方法必须用override修饰
  override def fight(): String = {
    &#34;fight with 棒子&#34;
  }
  //在子类中重写超类的抽象方法时，不需要使用override关键字，写了也可以
  def run(): Int = {
    1
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_模式匹配和样例类&#34;&gt;4. 模式匹配和样例类&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala有一个十分强大的模式匹配机制，可以应用到很多场合：如switch语句、类型检查等。
并且Scala还提供了样例类，对模式匹配进行了优化，可以快速进行匹配&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_匹配字符串&#34;&gt;4.1. 匹配字符串&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.util.Random

object CaseDemo01 extends App{
  val arr = Array(&#34;YoshizawaAkiho&#34;, &#34;YuiHatano&#34;, &#34;AoiSola&#34;)
  val name = arr(Random.nextInt(arr.length))
  name match {
    case &#34;YoshizawaAkiho&#34; =&amp;gt; println(&#34;吉泽老师...&#34;)
    case &#34;YuiHatano&#34; =&amp;gt; println(&#34;波多老师...&#34;)
    case _ =&amp;gt; println(&#34;真不知道你们在说什么...&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_匹配类型&#34;&gt;4.2. 匹配类型&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.util.Random

object CaseDemo01 extends App{
  //val v = if(x &amp;gt;= 5) 1 else if(x &amp;lt; 2) 2.0 else &#34;hello&#34;
  val arr = Array(&#34;hello&#34;, 1, 2.0, CaseDemo)
  val v = arr(Random.nextInt(4))
  println(v)
  v match {
    case x: Int =&amp;gt; println(&#34;Int &#34; + x)
    case y: Double if(y &amp;gt;= 0) =&amp;gt; println(&#34;Double &#34;+ y)
    case z: String =&amp;gt; println(&#34;String &#34; + z)
    case _ =&amp;gt; throw new Exception(&#34;not match exception&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注意：case y: Double if(y &amp;gt;= 0) &amp;#8658; &amp;#8230;&amp;#8203;
模式匹配的时候还可以添加守卫条件。如不符合守卫条件，将掉入case _中&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_匹配数组_元组&#34;&gt;4.3. 匹配数组、元组&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object CaseDemo03 extends App{

  val arr = Array(1, 3, 5)
  arr match {
    case Array(1, x, y) =&amp;gt; println(x + &#34; &#34; + y)
    case Array(0) =&amp;gt; println(&#34;only 0&#34;)
    case Array(0, _*) =&amp;gt; println(&#34;0 ...&#34;)
    case _ =&amp;gt; println(&#34;something else&#34;)
  }

  val lst = List(3, -1)
  lst match {
    case 0 :: Nil =&amp;gt; println(&#34;only 0&#34;)
    case x :: y :: Nil =&amp;gt; println(s&#34;x: $x y: $y&#34;)
    case 0 :: tail =&amp;gt; println(&#34;0 ...&#34;)
    case _ =&amp;gt; println(&#34;something else&#34;)
  }

  val tup = (2, 3, 7)
  tup match {
    case (1, x, y) =&amp;gt; println(s&#34;1, $x , $y&#34;)
    case (_, z, 5) =&amp;gt; println(z)
    case  _ =&amp;gt; println(&#34;else&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;admonitionblock important&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-important&#34; title=&#34;Important&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
在Scala中列表要么为空（Nil表示空列表）要么是一个head元素加上一个tail列表。&lt;br&gt;
9 :: List(5, 2)  :: 操作符是将给定的头和尾创建一个新的列表&lt;br&gt;
:: 操作符是右结合的，如9 :: 5 :: 2 :: Nil相当于 9 :: (5 :: (2 :: Nil))
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_样例类&#34;&gt;4.4. 样例类&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中样例类是一中特殊的类，可用于模式匹配。case class是多例的，后面要跟构造参数，case object是单例的&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.util.Random

case class SubmitTask(id: String, name: String)
case class HeartBeat(time: Long)
case object CheckTimeOutTask

object CaseDemo04 extends App{
  val arr = Array(CheckTimeOutTask, HeartBeat(12333), SubmitTask(&#34;0001&#34;, &#34;task-0001&#34;))

  arr(Random.nextInt(arr.length)) match {
    case SubmitTask(id, name) =&amp;gt; {
      println(s&#34;$id, $name&#34;)//前面需要加上s, $id直接取id的值
    }
    case HeartBeat(time) =&amp;gt; {
      println(time)
    }
    case CheckTimeOutTask =&amp;gt; {
      println(&#34;check&#34;)
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_option类型&#34;&gt;4.5. Option类型&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中Option类型样例类用来表示可能存在或也可能不存在的值(Option的子类有Some和None)。Some包装了某个值，None表示没有值&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object OptionDemo {
  def main(args: Array[String]) {
    val map = Map(&#34;a&#34; -&amp;gt; 1, &#34;b&#34; -&amp;gt; 2)
    val v = map.get(&#34;b&#34;) match {
      case Some(i) =&amp;gt; i
      case None =&amp;gt; 0
    }
    println(v)
    //更好的方式
    val v1 = map.getOrElse(&#34;c&#34;, 0)
    println(v1)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_偏函数&#34;&gt;4.6. 偏函数&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;被包在花括号内没有match的一组case语句是一个偏函数，它是PartialFunction[A, B]的一个实例，A代表参数类型，B代表返回类型，常用作输入模式匹配&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object PartialFuncDemo  {

  def func1: PartialFunction[String, Int] = {
    case &#34;one&#34; =&amp;gt; 1
    case &#34;two&#34; =&amp;gt; 2
    case _ =&amp;gt; -1
  }

  def func2(num: String) : Int = num match {
    case &#34;one&#34; =&amp;gt; 1
    case &#34;two&#34; =&amp;gt; 2
    case _ =&amp;gt; -1
  }

  def main(args: Array[String]) {
    println(func1(&#34;one&#34;))
    println(func2(&#34;one&#34;))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_单机版_wordcount&#34;&gt;5. 单机版 wordcount&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val lines = List(&#34;hello tom hello jerry&#34;,&#34;hello tom kitty hello hello&#34;)

lines.flatMap(_.split(&#34; &#34;)) &lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
    .map((_,1)) &lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
    .groupBy(_._1) &lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
    .map(t =&amp;gt; (t._1,t._2.size)) &lt;i class=&#34;conum&#34; data-value=&#34;4&#34;&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
    .toList.sortBy(_._2) &lt;i class=&#34;conum&#34; data-value=&#34;5&#34;&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;colist arabic&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;res2: List[String] = List(hello, tom, hello, jerry, hello, tom, kitty, hello, hello)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;res4: List[(String, Int)] = Listhello,1), (tom,1), (hello,1), (jerry,1), (hello,1), (tom,1), (kitty,1), (hello,1), (hello,1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;res5: scala.collection.immutable.Map[String,List[(String, Int)]] = Map(tom &amp;#8594; Listtom,1), (tom,1, kitty &amp;#8594; Listkitty,1, jerry &amp;#8594; Listjerry,1, hello &amp;#8594; Listhello,1), (hello,1), (hello,1), (hello,1), (hello,1)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;4&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;res6: scala.collection.immutable.Map[String,Int] = Map(tom &amp;#8594; 2, kitty &amp;#8594; 1, jerry &amp;#8594; 1, hello &amp;#8594; 5)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;5&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;res7: List[(String, Int)] = Listkitty,1), (jerry,1), (tom,2), (hello,5&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt; lines.flatMap(_.split(&#34; &#34;)).map((_,1)).groupBy(_._1).mapValues(_.foldLeft(0)(_+_._2))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Kafka负载均衡-自定义Partition-文件存储机制</title>
      <link>/post/bigdata/storm/Kafka%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-%E8%87%AA%E5%AE%9A%E4%B9%89Partition-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/</link>
      <pubDate>Sat, 01 Apr 2017 09:31:48 +0000</pubDate>
      
      <guid>/post/bigdata/storm/Kafka%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-%E8%87%AA%E5%AE%9A%E4%B9%89Partition-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;kafka&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka整体结构图&#34;&gt;1. Kafka整体结构图&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_consumer与topic关系&#34;&gt;2. Consumer与topic关系&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka消息的分发&#34;&gt;3. Kafka消息的分发&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_consumer的负载均衡&#34;&gt;4. Consumer的负载均衡&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka文件存储机制&#34;&gt;5. kafka文件存储机制&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka文件存储基本结构&#34;&gt;5.1. Kafka文件存储基本结构&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka_partition_segment&#34;&gt;5.2. Kafka Partition Segment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka_查找message&#34;&gt;5.3. Kafka 查找message&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_查找segment_file&#34;&gt;5.3.1. 查找segment file&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka自定义partition&#34;&gt;6. Kafka自定义Partition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka整体结构图&#34;&gt;1. Kafka整体结构图&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka名词解释和工作方式&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Producer &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;消息生产者，就是向kafka broker发消息的客户端。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Consumer &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;消息消费者，向kafka broker取消息的客户端&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Topic &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;咋们可以理解为一个队列。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Consumer Group （CG）&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;这是 &lt;strong&gt;kafka&lt;/strong&gt; 用来实现一个 &lt;strong&gt;topic&lt;/strong&gt; 消息的广播（发给所有的 &lt;strong&gt;consumer&lt;/strong&gt; ）和单播（发给任意一个 &lt;strong&gt;consumer&lt;/strong&gt; ）的手段。一个 &lt;strong&gt;topic&lt;/strong&gt; 可以有多个 &lt;strong&gt;CG&lt;/strong&gt; 。&lt;strong&gt;topic&lt;/strong&gt; 的消息会复制（不是真的复制，是概念上的）到所有的 &lt;strong&gt;CG&lt;/strong&gt; ，但每个 &lt;strong&gt;partion&lt;/strong&gt; 只会把消息发给该 &lt;strong&gt;CG&lt;/strong&gt; 中的一个 &lt;strong&gt;consumer&lt;/strong&gt; 。如果需要实现广播，只要每个 &lt;strong&gt;consumer&lt;/strong&gt; 有一个独立的 &lt;strong&gt;CG&lt;/strong&gt; 就可以了。要实现单播只要所有的 &lt;strong&gt;consumer&lt;/strong&gt; 在同一个 &lt;strong&gt;CG&lt;/strong&gt; 。用 &lt;strong&gt;CG&lt;/strong&gt; 还可以将 &lt;strong&gt;consumer&lt;/strong&gt; 进行自由的分组而不需要多次发送消息到不同的 &lt;strong&gt;topic&lt;/strong&gt; 。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Broker &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;一台 &lt;strong&gt;kafka&lt;/strong&gt; 服务器就是一个 &lt;strong&gt;broker&lt;/strong&gt; 。一个集群由多个 &lt;strong&gt;broker&lt;/strong&gt; 组成。一个 &lt;strong&gt;broker&lt;/strong&gt; 可以容纳多个 &lt;strong&gt;topic&lt;/strong&gt; 。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Partition&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;为了实现扩展性，一个非常大的 &lt;strong&gt;topic&lt;/strong&gt; 可以分布到多个 &lt;strong&gt;broker&lt;/strong&gt; （即服务器）上，一个 &lt;strong&gt;topic&lt;/strong&gt; 可以分为多个 &lt;strong&gt;partition&lt;/strong&gt; ，每个 &lt;strong&gt;partition&lt;/strong&gt; 是一个有序的队列。 &lt;strong&gt;partition&lt;/strong&gt; 中的每条消息都会被分配一个有序的 &lt;strong&gt;id&lt;/strong&gt; （ &lt;strong&gt;offset&lt;/strong&gt; ）。 &lt;strong&gt;kafka&lt;/strong&gt; 只保证按一个 &lt;strong&gt;partition&lt;/strong&gt; 中的顺序将消息发给 &lt;strong&gt;consumer&lt;/strong&gt; ，不保证一个 &lt;strong&gt;topic&lt;/strong&gt; 的整体（多个 &lt;strong&gt;partition&lt;/strong&gt; 间）的顺序。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Offset&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;strong&gt;kafka&lt;/strong&gt; 的存储文件都是按照 &lt;strong&gt;offset.kafka&lt;/strong&gt; 来命名，用 &lt;strong&gt;offset&lt;/strong&gt; 做名字的好处是方便查找。例如你想找位于2049的位置，只要找到 &lt;strong&gt;2048.kafka&lt;/strong&gt; 的文件即可。当然 &lt;strong&gt;the first offset&lt;/strong&gt; 就是 &lt;strong&gt;00000000000.kafka&lt;/strong&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_consumer与topic关系&#34;&gt;2. Consumer与topic关系&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;本质上 &lt;strong&gt;kafka&lt;/strong&gt; 只支持 &lt;strong&gt;Topic&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;每个 &lt;strong&gt;group&lt;/strong&gt; 中可以有多个 &lt;strong&gt;consumer&lt;/strong&gt; ，每个 &lt;strong&gt;consumer&lt;/strong&gt; 属于一个 &lt;strong&gt;consumer&lt;/strong&gt;   &lt;strong&gt;group&lt;/strong&gt; ；
　　通常情况下，一个 &lt;strong&gt;group&lt;/strong&gt; 中会包含多个 &lt;strong&gt;consumer&lt;/strong&gt; ，这样不仅可以提高 &lt;strong&gt;topic&lt;/strong&gt; 中消息的并发消费能力，而且还能提高&#34;故障容错&#34;性，如果 &lt;strong&gt;group&lt;/strong&gt; 中的某个 &lt;strong&gt;consumer&lt;/strong&gt; 失效那么其消费的 &lt;strong&gt;partitions&lt;/strong&gt; 将会有其他 &lt;strong&gt;consumer&lt;/strong&gt; 自动接管。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于 &lt;strong&gt;Topic&lt;/strong&gt; 中的一条特定的消息，只会被订阅此 &lt;strong&gt;Topic&lt;/strong&gt; 的每个 &lt;strong&gt;group&lt;/strong&gt; 中的其中一个 &lt;strong&gt;consumer&lt;/strong&gt; 消费，此消息不会发送给一个 &lt;strong&gt;group&lt;/strong&gt; 的多个 &lt;strong&gt;consumer&lt;/strong&gt; ；
　　那么一个 &lt;strong&gt;group&lt;/strong&gt; 中所有的 &lt;strong&gt;consumer&lt;/strong&gt; 将会交错的消费整个 &lt;strong&gt;Topic&lt;/strong&gt; ，每个 &lt;strong&gt;group&lt;/strong&gt; 中 &lt;strong&gt;consumer&lt;/strong&gt; 消息消费互相独立，我们可以认为一个 &lt;strong&gt;group&lt;/strong&gt; 是一个&#34;订阅&#34;者。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;kafka&lt;/strong&gt; 中,一个 &lt;strong&gt;partition&lt;/strong&gt; 中的消息只会被 &lt;strong&gt;group&lt;/strong&gt; 中的一个 &lt;strong&gt;consumer&lt;/strong&gt; 消费(同一时刻)；
一个 &lt;strong&gt;Topic&lt;/strong&gt; 中的每个 &lt;strong&gt;partions&lt;/strong&gt; ，只会被一个&#34;订阅者&#34;中的一个 &lt;strong&gt;consumer&lt;/strong&gt; 消费，不过一个 &lt;strong&gt;consumer&lt;/strong&gt; 可以同时消费多个 &lt;strong&gt;partitions&lt;/strong&gt; 中的消息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;kafka&lt;/strong&gt; 的设计原理决定,对于一个 &lt;strong&gt;topic&lt;/strong&gt; ，同一个 &lt;strong&gt;group&lt;/strong&gt; 中不能有多于 &lt;strong&gt;partitions&lt;/strong&gt; 个数的 &lt;strong&gt;consumer&lt;/strong&gt; 同时消费，否则将意味着某些 &lt;strong&gt;consumer&lt;/strong&gt; 将无法得到消息。
　　 &lt;strong&gt;kafka&lt;/strong&gt; 只能保证一个 &lt;strong&gt;partition&lt;/strong&gt; 中的消息被某个 &lt;strong&gt;consumer&lt;/strong&gt; 消费时是顺序的；事实上，从 &lt;strong&gt;Topic&lt;/strong&gt; 角度来说,当有多个 &lt;strong&gt;partitions&lt;/strong&gt; 时,消息仍不是全局有序的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka消息的分发&#34;&gt;3. Kafka消息的分发&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;Producer&lt;/strong&gt; 客户端负责消息的分发&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;kafka&lt;/strong&gt; 集群中的任何一个 &lt;strong&gt;broker&lt;/strong&gt; 都可以向 &lt;strong&gt;producer&lt;/strong&gt; 提供 &lt;strong&gt;metadata&lt;/strong&gt; 信息,这些 &lt;strong&gt;metadata&lt;/strong&gt; 中包含&#34;集群中存活的 &lt;strong&gt;servers&lt;/strong&gt; 列表 &lt;strong&gt;&#34;/&#34;partitions leader&lt;/strong&gt; 列表&#34;等信息；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当 &lt;strong&gt;producer&lt;/strong&gt; 获取到 &lt;strong&gt;metadata&lt;/strong&gt; 信息之后,  &lt;strong&gt;producer&lt;/strong&gt; 将会和 &lt;strong&gt;Topic&lt;/strong&gt; 下所有 &lt;strong&gt;partition&lt;/strong&gt;   &lt;strong&gt;leader&lt;/strong&gt; 保持 &lt;strong&gt;socket&lt;/strong&gt; 连接；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;消息由 &lt;strong&gt;producer&lt;/strong&gt; 直接通过 &lt;strong&gt;socket&lt;/strong&gt; 发送到 &lt;strong&gt;broker&lt;/strong&gt; ，中间不会经过任何&#34;路由层&#34;，事实上，消息被路由到哪个 &lt;strong&gt;partition&lt;/strong&gt; 上由 &lt;strong&gt;producer&lt;/strong&gt; 客户端决定；
　　比如可以采用&#34; &lt;strong&gt;random&lt;/strong&gt; &#34;&#34; &lt;strong&gt;key&lt;/strong&gt; - &lt;strong&gt;hash&lt;/strong&gt; &#34;&#34;轮询&#34;等,如果一个 &lt;strong&gt;topic&lt;/strong&gt; 中有多个 &lt;strong&gt;partitions&lt;/strong&gt; ,那么在 &lt;strong&gt;producer&lt;/strong&gt; 端实现&#34;消息均衡分发&#34;是必要的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;producer&lt;/strong&gt; 端的配置文件中,开发者可以指定 &lt;strong&gt;partition&lt;/strong&gt; 路由的方式。&lt;/p&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　Producer消息发送的应答机制
　　设置发送数据是否需要服务端的反馈,有三个值0,1,-1
　　0: producer不会等待broker发送ack
　　1: 当leader接收到消息之后发送ack
　　-1: 当所有的follower都同步消息成功后发送ack
    request.required.acks=0&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_consumer的负载均衡&#34;&gt;4. Consumer的负载均衡&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;当一个 &lt;strong&gt;group&lt;/strong&gt; 中,有 &lt;strong&gt;consumer&lt;/strong&gt; 加入或者离开时,会触发 &lt;strong&gt;partitions&lt;/strong&gt; 均衡.均衡的最终目的,是提升 &lt;strong&gt;topic&lt;/strong&gt; 的并发消费能力，步骤如下：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;假如 &lt;strong&gt;topic1&lt;/strong&gt; ,具有如下 &lt;strong&gt;partitions: P0,P1,P2,P3&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加入 &lt;strong&gt;group&lt;/strong&gt; 中,有如下 &lt;strong&gt;consumer: C1,C2&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;首先根据 &lt;strong&gt;partition&lt;/strong&gt; 索引号对 &lt;strong&gt;partitions&lt;/strong&gt; 排序:  &lt;strong&gt;P0,P1,P2,P3&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据 &lt;strong&gt;consumer.id&lt;/strong&gt; 排序:  &lt;strong&gt;C0,C1&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算倍数:  &lt;strong&gt;M = [P0,P1,P2,P3].size / [C0,C1].size&lt;/strong&gt; ,本例值 &lt;strong&gt;M=2&lt;/strong&gt; (向上取整)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;然后依次分配 &lt;strong&gt;partitions&lt;/strong&gt; :  &lt;strong&gt;C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104025.png&#34; alt=&#34;2017 04 01 104025&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka文件存储机制&#34;&gt;5. kafka文件存储机制&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka文件存储基本结构&#34;&gt;5.1. Kafka文件存储基本结构&lt;/h3&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;Kafka&lt;/strong&gt; 文件存储中，同一个 &lt;strong&gt;topic&lt;/strong&gt; 下有多个不同 &lt;strong&gt;partition&lt;/strong&gt; ，每个 &lt;strong&gt;partition&lt;/strong&gt; 为一个目录， &lt;strong&gt;partiton&lt;/strong&gt; 命名规则为 &lt;strong&gt;topic&lt;/strong&gt; 名称+有序序号，第一个 &lt;strong&gt;partiton&lt;/strong&gt; 序号从0开始，序号最大值为 &lt;strong&gt;partitions&lt;/strong&gt; 数量减1。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个 &lt;strong&gt;partion&lt;/strong&gt; (目录)相当于一个巨型文件被平均分配到多个大小相等 &lt;strong&gt;segment&lt;/strong&gt; (段)数据文件中。但每个段 &lt;strong&gt;segment file&lt;/strong&gt; 消息数量不一定相等，这种特性方便 &lt;strong&gt;old&lt;/strong&gt;   &lt;strong&gt;segment&lt;/strong&gt;   &lt;strong&gt;file&lt;/strong&gt; 快速被删除。默认保留7天的数据。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104135.png&#34; alt=&#34;2017 04 01 104135&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个 &lt;strong&gt;partiton&lt;/strong&gt; 只需要支持顺序读写就行了， &lt;strong&gt;segment&lt;/strong&gt; 文件生命周期由服务端配置参数决定。（什么时候创建，什么时候删除）&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104143.png&#34; alt=&#34;2017 04 01 104143&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;数据有序的讨论？&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;一个 &lt;strong&gt;partition&lt;/strong&gt; 的数据是否是有序的？&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;间隔性有序，不连续
针对一个 &lt;strong&gt;topic&lt;/strong&gt; 里面的数据，只能做到 &lt;strong&gt;partition&lt;/strong&gt; 内部有序，不能做到全局有序。
特别加入消费者的场景后，如何保证消费者消费的数据全局有序的？伪命题。
只有一种情况下才能保证全局有序？就是只有一个 &lt;strong&gt;partition&lt;/strong&gt; 。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka_partition_segment&#34;&gt;5.2. Kafka Partition Segment&lt;/h3&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Segment file&lt;/strong&gt; 组成：由2大部分组成，分别为 &lt;strong&gt;index&lt;/strong&gt;   &lt;strong&gt;file&lt;/strong&gt; 和 &lt;strong&gt;data&lt;/strong&gt;   &lt;strong&gt;file&lt;/strong&gt; ，此2个文件一一对应，成对出现，后缀 &lt;strong&gt;.index&lt;/strong&gt; 和 &lt;strong&gt;.log&lt;/strong&gt; 分别表示为 &lt;strong&gt;segment&lt;/strong&gt; 索引文件、数据文件。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104152.png&#34; alt=&#34;2017 04 01 104152&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Segment&lt;/strong&gt; 文件命名规则： &lt;strong&gt;partion&lt;/strong&gt; 全局的第一个 &lt;strong&gt;segment&lt;/strong&gt; 从0开始，后续每个 &lt;strong&gt;segment&lt;/strong&gt; 文件名为上一个 &lt;strong&gt;segment&lt;/strong&gt; 文件最后一条消息的 &lt;strong&gt;offset&lt;/strong&gt; 值。数值最大为64位 &lt;strong&gt;long&lt;/strong&gt; 大小，19位数字字符长度，没有数字用0填充。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中 &lt;strong&gt;message&lt;/strong&gt; 的物理偏移地址。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104159.png&#34; alt=&#34;2017 04 01 104159&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;3，497：当前log文件中的第几条信息，存放在磁盘上的那个地方&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;上述图中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。
其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;segment data file由许多message组成， qq物理结构如下：
关键字
解释说明&lt;/p&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;8 byte offset &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;4 byte message size &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;message大小&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;4 byte CRC32 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;用crc32校验message&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;1 byte “magic&#34; &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示本次发布Kafka服务程序协议版本号&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;1 byte “attributes&#34; &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示为独立版本、或标识压缩类型、或编码类型。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;4 byte key length &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示key的长度,当key为-1时，K byte key字段不填&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;K byte key &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;可选&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;value bytes payload &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示实际消息数据。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka_查找message&#34;&gt;5.3. Kafka 查找message&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;读取offset=368776的message，需要通过下面2个步骤查找。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104213.png&#34; alt=&#34;2017 04 01 104213&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_查找segment_file&#34;&gt;5.3.1. 查找segment file&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0
　　00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1
　　00000000000000737337.index的起始偏移量为737338=737337 + 1
　　其他后续文件依次类推。
以起始偏移量命名并排序这些文件，只要根据offset **二分查找**文件列表，就可以快速定位到具体文件。当offset=368776时定位到00000000000000368769.index和对应log文件。
5.3.2、通过segment file查找message
当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址
　　然后再通过00000000000000368769.log顺序查找直到offset=368776为止。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka自定义partition&#34;&gt;6. Kafka自定义Partition&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;见代码&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Kafka集群安装部署、Kafka生产者、Kafka消费者</title>
      <link>/post/bigdata/storm/kafka/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/storm/kafka/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;kafka&lt;/div&gt;
&lt;ul class=&#34;sectlevel0&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka是什么&#34;&gt;Kafka是什么&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_jms是什么&#34;&gt;JMS是什么&lt;/a&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_jms的基础&#34;&gt;1. JMS的基础&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_jms消息传输模型&#34;&gt;2. JMS消息传输模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_jms核心组件&#34;&gt;3. JMS核心组件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_常见的类jms消息服务器&#34;&gt;4. 常见的类JMS消息服务器&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_jms消息服务器_activemq&#34;&gt;4.1. JMS消息服务器 ActiveMQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_分布式消息中间件_metamorphosis&#34;&gt;4.2. 分布式消息中间件 Metamorphosis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_分布式消息中间件_rocketmq&#34;&gt;4.3. 分布式消息中间件 RocketMQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_其他mq&#34;&gt;4.4. 其他MQ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_为什么需要消息队列_重要&#34;&gt;为什么需要消息队列（重要）&lt;/a&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_用户注册的一般流程&#34;&gt;1. 用户注册的一般流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_用户注册的并行执行&#34;&gt;2. 用户注册的并行执行&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_用户注册的最终一致&#34;&gt;3. 用户注册的最终一致&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka核心组件&#34;&gt;Kafka核心组件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka集群部署&#34;&gt;Kafka集群部署&lt;/a&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_集群部署的基本流程&#34;&gt;1. 集群部署的基本流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_集群部署的基础环境准备&#34;&gt;2. 集群部署的基础环境准备&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka集群部署_2&#34;&gt;3. Kafka集群部署&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_下载安装包&#34;&gt;3.1. 下载安装包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_解压安装包&#34;&gt;3.2. 解压安装包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_修改配置文件&#34;&gt;3.3. 修改配置文件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_分发安装包&#34;&gt;3.4. 分发安装包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_再次修改配置文件_重要&#34;&gt;3.5. 再次修改配置文件（重要）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_启动集群&#34;&gt;3.6. 启动集群&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka生产者java_api&#34;&gt;Kafka生产者Java API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka消费者java_api&#34;&gt;Kafka消费者Java API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_kafka是什么&#34; class=&#34;sect0&#34;&gt;Kafka是什么&lt;/h1&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。
　　
　　KAFKA + STORM +REDIS
[disc]　
* Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。
* Kafka最初是由LinkedIn开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。
* Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。
* Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。
* 无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_jms是什么&#34; class=&#34;sect0&#34;&gt;JMS是什么&lt;/h1&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_jms的基础&#34;&gt;1. JMS的基础&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;    JMS是什么：JMS是Java提供的一套技术规范
　　JMS干什么用：用来异构系统 集成通信，缓解系统瓶颈，提高系统的伸缩性增强系统用户体验，使得系统模块化和组件化变得可行并更加灵活
　　通过什么方式：生产消费者模式（生产者、服务器、消费者）&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152744.png&#34; alt=&#34;2017 03 28 152744&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　jdk，kafka，activemq……&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_jms消息传输模型&#34;&gt;2. JMS消息传输模型&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）
点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;发布/订阅模式（一对多，数据生产后，推送给所有订阅者）
发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即时当前订阅者不可用，处于离线状态。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152816.png&#34; alt=&#34;2017 03 28 152816&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;queue.put（object）  数据生产
queue.take(object)    数据消费&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_jms核心组件&#34;&gt;3. JMS核心组件&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;Destination：消息发送的目的地，也就是前面说的Queue和Topic。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Message [m1]：从字面上就可以看出是被发送的消息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Producer： 消息的生产者，要发送一个消息，必须通过这个生产者来发送。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MessageConsumer： 与生产者相对应，这是消息的消费者或接收者，通过它来接收一个消息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152836.png&#34; alt=&#34;2017 03 28 152836&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;通过与ConnectionFactory可以获得一个connection
通过connection可以获得一个session会话。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_常见的类jms消息服务器&#34;&gt;4. 常见的类JMS消息服务器&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_jms消息服务器_activemq&#34;&gt;4.1. JMS消息服务器 ActiveMQ&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　ActiveMQ 是Apache出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持JMS1.1和J2EE 1.4规范的。
　　主要特点：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;多种语言和协议编写客户端。语言: Java, C, C++, C#, Ruby, Perl, Python, PHP。应用协议: OpenWire,Stomp REST,WS Notification,XMPP,AMQP&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;完全支持JMS1.1和J2EE 1.4规范 (持久化,XA消息,事务)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对Spring的支持,ActiveMQ可以很容易内嵌到使用Spring的系统里面去,而且也支持Spring2.0的特性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过了常见J2EE服务器(如 Geronimo,JBoss 4, GlassFish,WebLogic)的测试,其中通过JCA 1.5 resource adaptors的配置,可以让ActiveMQ可以自动的部署到任何兼容J2EE 1.4 商业服务器上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持多种传送协议:in-VM,TCP,SSL,NIO,UDP,JGroups,JXTA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持通过JDBC和journal提供高速的消息持久化&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从设计上保证了高性能的集群,客户端-服务器,点对点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持Ajax&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持与Axis的整合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以很容易得调用内嵌JMS provider,进行测试&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_分布式消息中间件_metamorphosis&#34;&gt;4.2. 分布式消息中间件 Metamorphosis&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　Metamorphosis (MetaQ) 是一个高性能、高可用、可扩展的分布式消息中间件，类似于LinkedIn的Kafka，具有消息存储顺序写、吞吐量大和支持本地和XA事务等特性，适用于大吞吐量、顺序消息、广播和日志数据传输等场景，在淘宝和支付宝有着广泛的应用，现已开源。
　　主要特点：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;生产者、服务器和消费者都可分布&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;消息存储顺序写&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性能极高,吞吐量大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持消息顺序&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持本地和XA事务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;客户端pull，随机读,利用sendfile系统调用，zero-copy ,批量拉数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持消费端事务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持消息广播模式&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持异步发送消息&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持http协议&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持消息重试和recover&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据迁移、扩容对用户透明&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;消费状态保存在客户端&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持同步和异步复制两种HA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持group commit&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_分布式消息中间件_rocketmq&#34;&gt;4.3. 分布式消息中间件 RocketMQ&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点：
* 能够保证严格的消息顺序
* 提供丰富的消息拉取模式
* 高效的订阅者水平扩展能力
* 实时的消息订阅机制
* 亿级消息堆积能力
* Metaq3.0 版本改名，产品名称改为RocketMQ&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_其他mq&#34;&gt;4.4. 其他MQ&lt;/h3&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;.NET消息中间件 DotNetMQ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于HBase的消息队列 HQueue&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go 的 MQ 框架 KiteQ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AMQP消息服务器 RabbitMQ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MemcacheQ 是一个基于 MemcacheDB 的消息队列服务器。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_为什么需要消息队列_重要&#34; class=&#34;sect0&#34;&gt;为什么需要消息队列（重要）&lt;/h1&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;消息系统的核心作用就是三点：解耦，异步和并行
以用户注册的案列来说明消息系统的作用&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_用户注册的一般流程&#34;&gt;1. 用户注册的一般流程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152848.png&#34; alt=&#34;2017 03 28 152848&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;问题：随着后端流程越来越多，每步流程都需要额外的耗费很多时间，从而会导致用户更长的等待延迟。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_用户注册的并行执行&#34;&gt;2. 用户注册的并行执行&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152855.png&#34; alt=&#34;2017 03 28 152855&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;问题：系统并行的发起了4个请求，4个请求中，如果某一个环节执行1分钟，其他环节再快，用户也需要等待1分钟。如果其中一个环节异常之后，整个服务挂掉了。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152902.png&#34; alt=&#34;2017 03 28 152902&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_用户注册的最终一致&#34;&gt;3. 用户注册的最终一致&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152910.png&#34; alt=&#34;2017 03 28 152910&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1、 保证主流程的正常执行、执行成功之后，发送MQ消息出去。
2、 需要这个destination的其他系统通过消费数据再执行，最终一致。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152916.png&#34; alt=&#34;2017 03 28 152916&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_kafka核心组件&#34; class=&#34;sect0&#34;&gt;Kafka核心组件&lt;/h1&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;Topic ：消息根据Topic进行归类&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Producer：发送消息者&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consumer：消息接受者&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;broker：每个kafka实例(server)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zookeeper：依赖集群保存meta信息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152924.png&#34; alt=&#34;2017 03 28 152924&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h1 id=&#34;_kafka集群部署&#34; class=&#34;sect0&#34;&gt;Kafka集群部署&lt;/h1&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_集群部署的基本流程&#34;&gt;1. 集群部署的基本流程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　下载安装包、解压安装包、修改配置文件、分发安装包、启动集群&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_集群部署的基础环境准备&#34;&gt;2. 集群部署的基础环境准备&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　安装前的准备工作（zk集群已经部署完毕）&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;关闭防火墙
　　chkconfig iptables off  &amp;amp;&amp;amp; setenforce 0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建用户
　　groupadd realtime &amp;amp;&amp;amp;　useradd realtime　&amp;amp;&amp;amp; usermod -a -G realtime realtime&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建工作目录并赋权
　　mkdir /export
　　mkdir /export/servers
　　chmod 755 -R /export&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;切换到realtime用户下
　　su realtime&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka集群部署_2&#34;&gt;3. Kafka集群部署&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_下载安装包&#34;&gt;3.1. 下载安装包&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　http://kafka.apache.org/downloads.html
　　在linux中使用wget命令下载安装包
    wget http://mirrors.hust.edu.cn/apache/kafka/0.8.2.2/kafka_2.11-0.8.2.2.tgz&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_解压安装包&#34;&gt;3.2. 解压安装包&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　tar -zxvf /export/software/kafka_2.11-0.8.2.2.tgz -C /export/servers/
　　cd /export/servers/
　　ln -s kafka_2.11-0.8.2.2 kafka&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_修改配置文件&#34;&gt;3.3. 修改配置文件&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cp   /export/servers/kafka/config/server.properties
　　/export/servers/kafka/config/server.properties.bak
　　vi /export/servers/kafka/config/server.properties
　　输入以下内容：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152939.png&#34; alt=&#34;2017 03 28 152939&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_分发安装包&#34;&gt;3.4. 分发安装包&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　scp -r /export/servers/kafka_2.11-0.8.2.2 kafka02:/export/servers
　　然后分别在各机器上创建软连
　　cd /export/servers/
　　ln -s kafka_2.11-0.8.2.2 kafka&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_再次修改配置文件_重要&#34;&gt;3.5. 再次修改配置文件（重要）&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　依次修改各服务器上配置文件的的broker.id，分别是0,1,2不得重复。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_启动集群&#34;&gt;3.6. 启动集群&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　依次在各节点上启动kafka
　　bin/kafka-server-start.sh config/server.properties
　　
== Kafka常用操作命令&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;查看当前服务器中的所有topic
　　bin/kafka-topics.sh --list --zookeeper zk01:2181&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建topic
　　./kafka-topics.sh --create --zookeeper mini1:2181 --replication-factor 1 --partitions 3 --topic first&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;删除topic
　　sh bin/kafka-topics.sh --delete --zookeeper zk01:2181 --topic test
需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过shell命令发送消息
　　kafka-console-producer.sh --broker-list kafka01:9092 --topic itheima&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过shell消费消息
　　sh bin/kafka-console-consumer.sh --zookeeper zk01:2181 --from-beginning --topic test1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看消费位置
　　sh kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zk01:2181 --group testGroup&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看某个Topic的详情
　　sh kafka-topics.sh --topic test --describe --zookeeper zk01:2181&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_kafka生产者java_api&#34; class=&#34;sect0&#34;&gt;Kafka生产者Java API&lt;/h1&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152953.png&#34; alt=&#34;2017 03 28 152953&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h1 id=&#34;_kafka消费者java_api&#34; class=&#34;sect0&#34;&gt;Kafka消费者Java API&lt;/h1&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_153002.png&#34; alt=&#34;2017 03 28 153002&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　
　　StreamMessage：Java 数据流消息，用标准流操作来顺序的填充和读取。
　　MapMessage：一个Map类型的消息；名称为 string 类型，而值为 Java 的基本类型。
　　TextMessage：普通字符串消息，包含一个String。
　　ObjectMessage：对象消息，包含一个可序列化的Java 对象
　　BytesMessage：二进制数组消息，包含一个byte[]。
　　XMLMessage:  一个XML类型的消息。
最常用的是TextMessage和ObjectMessage。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Storm目录树、任务提交、消息容错</title>
      <link>/post/bigdata/storm/Storm%E7%9B%AE%E5%BD%95%E6%A0%91%E3%80%81%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E3%80%81%E6%B6%88%E6%81%AF%E5%AE%B9%E9%94%99/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/storm/Storm%E7%9B%AE%E5%BD%95%E6%A0%91%E3%80%81%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E3%80%81%E6%B6%88%E6%81%AF%E5%AE%B9%E9%94%99/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Storm&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_storm程序的并发机制&#34;&gt;1. Storm程序的并发机制&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_概念&#34;&gt;1.1. 概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_配置并行度&#34;&gt;1.2. 配置并行度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm通信机制&#34;&gt;2. Storm通信机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_worker进程间通信&#34;&gt;3. Worker进程间通信&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_worker进程间通信分析&#34;&gt;4. Worker进程间通信分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_worker进程间技术_netty_zeromq&#34;&gt;5. Worker进程间技术(Netty、ZeroMQ)&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_netty&#34;&gt;5.1. Netty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_zeromq&#34;&gt;5.2. ZeroMQ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_worker_内部通信技术_disruptor&#34;&gt;6. Worker 内部通信技术(Disruptor)&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_disruptor的来历&#34;&gt;6.1. Disruptor的来历&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_disruptor是什么&#34;&gt;6.2. Disruptor是什么&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_disruptor主要特点&#34;&gt;6.3. Disruptor主要特点&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm组件本地目录树&#34;&gt;7. Storm组件本地目录树&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm_zookeeper目录树&#34;&gt;8. Storm zookeeper目录树&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm_任务提交的过程&#34;&gt;9. Storm 任务提交的过程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm_消息容错机制&#34;&gt;10. Storm 消息容错机制&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_总体介绍&#34;&gt;10.1. 总体介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_基本实现&#34;&gt;10.2. 基本实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_可靠性配置&#34;&gt;10.3. 可靠性配置&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm程序的并发机制&#34;&gt;1. Storm程序的并发机制&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_概念&#34;&gt;1.1. 概念&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Workers (JVMs)&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在一个物理节点上可以运行一个或多个独立的JVM 进程。一个Topology可以包含一个或多个worker(并行的跑在不同的物理机上), 所以worker process就是执行一个topology的子集, 并且worker只能对应于一个topology&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Executors (threads)&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在一个worker JVM进程中运行着多个Java线程。一个executor线程可以执行一个或多个tasks。但一般默认每个executor只执行一个task。一个worker可以包含一个或多个executor, 每个component (spout或bolt)至少对应于一个executor, 所以可以说executor执行一个compenent的子集, 同时一个executor只能对应于一个component。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Tasks(bolt/spout instances&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Task就是具体的处理逻辑对象，每一个Spout和Bolt会被当作很多task在整个集群里面执行。每一个task对应到一个线程，而stream grouping则是定义怎么从一堆task发射tuple到另外一堆task。你可以调用TopologyBuilder.setSpout和TopBuilder.setBolt来设置并行度 — 也就是有多少个task。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_配置并行度&#34;&gt;1.2. 配置并行度&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;对于并发度的配置, 在storm里面可以在多个地方进行配置, 优先级为：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;　　defaults.yaml &amp;lt; storm.yaml &amp;lt; topology-specific configuration
　　&amp;lt; internal component-specific configuration &amp;lt; external component-specific configuration&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;worker processes的数目, 可以通过配置文件和代码中配置, worker就是执行进程, 所以考虑并发的效果, 数目至少应该大亍machines的数目&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;executor的数目, component的并发线程数，只能在代码中配置(通过setBolt和setSpout的参数), 例如, setBolt(&#34;green-bolt&#34;, new GreenBolt(), 2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;tasks的数目, 可以不配置, 默认和executor1:1, 也可以通过setNumTasks()配置&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;Topology的worker数通过config设置，即执行该topology的worker（java）进程数。它可以通过 storm rebalance 命令任意调整。
Config conf = newConfig();
conf.setNumWorkers(2);//用2个worker
topologyBuilder.setSpout(&#34;blue-spout&#34;,newBlueSpout(),2);//设置2个并发度
topologyBuilder.setBolt(&#34;green-bolt&#34;,newGreenBolt(),2).setNumTasks(4).shuffleGrouping(&#34;blue-spout&#34;);//设置2个并发度，4个任务
topologyBuilder.setBolt(&#34;yellow-bolt&#34;,newYellowBolt(),6).shuffleGrouping(&#34;green-bolt&#34;);//设置6个并发度
StormSubmitter.submitTopology(&#34;mytopology&#34;, conf, topologyBuilder.createTopology());&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163004.png&#34; alt=&#34;2017 03 28 163004&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;3个组件的并发度加起来是10，就是说拓扑一共有10个executor，一共有2个worker，每个worker产生10 / 2 = 5条线程。
绿色的bolt配置成2个executor和4个task。为此每个executor为这个bolt运行2个task。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;动态的改变并行度
　　Storm支持在不 restart topology 的情况下, 动态的改变(增减) worker processes 的数目和 executors 的数目, 称为rebalancing. 通过Storm web UI，或者通过storm rebalance命令实现：
storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm通信机制&#34;&gt;2. Storm通信机制&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;Worker间的通信经常需要通过网络跨节点进行，Storm使用ZeroMQ或Netty(0.9以后默认使用)作为进程间通信的消息框架。
Worker进程内部通信：不同worker的thread通信使用LMAX Disruptor来完成。
不同topologey之间的通信，Storm不负责，需要自己想办法实现，例如使用kafka等；&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_worker进程间通信&#34;&gt;3. Worker进程间通信&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　worker进程间消息传递机制，消息的接收和处理的大概流程见下图&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163014.png&#34; alt=&#34;2017 03 28 163014&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;对于worker进程来说，为了管理流入和传出的消息，每个worker进程有一个独立的接收线程[m1](对配置的TCP端口supervisor.slots.ports进行监听);
对应Worker接收线程，每个worker存在一个独立的发送线程[m2]，它负责从worker的transfer-queue[m3]中读取消息，并通过网络发送给其他worker&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个executor有自己的incoming-queue[m4]和outgoing-queue[m5]。
Worker接收线程将收到的消息通过task编号传递给对应的executor(一个或多个)的incoming-queues;
每个executor有单独的线程分别来处理spout/bolt的业务逻辑，业务逻辑输出的中间数据会存放在outgoing-queue中，当executor的outgoing-queue中的tuple达到一定的阀值，executor的发送线程将批量获取outgoing-queue中的tuple,并发送到transfer-queue中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个worker进程控制一个或多个executor线程，用户可在代码中进行配置。其实就是我们在代码中设置的并发度个数。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_worker进程间通信分析&#34;&gt;4. Worker进程间通信分析&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163023.png&#34; alt=&#34;2017 03 28 163023&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1、 Worker接受线程通过网络接受数据，并根据Tuple中包含的taskId，匹配到对应的executor；然后根据executor找到对应的incoming-queue，将数据存发送到incoming-queue队列中。
2、 业务逻辑执行现成消费incoming-queue的数据，通过调用Bolt的execute(xxxx)方法，将Tuple作为参数传输给用户自定义的方法
3、 业务逻辑执行完毕之后，将计算的中间数据发送给outgoing-queue队列，当outgoing-queue中的tuple达到一定的阀值，executor的发送线程将批量获取outgoing-queue中的tuple,并发送到Worker的transfer-queue中
4、 Worker发送线程消费transfer-queue中数据，计算Tuple的目的地，连接不同的node+port将数据通过网络传输的方式传送给另一个的Worker。
5、 另一个worker执行以上步骤1的操作。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_worker进程间技术_netty_zeromq&#34;&gt;5. Worker进程间技术(Netty、ZeroMQ)&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_netty&#34;&gt;5.1. Netty&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　Netty是一个NIO client-server(客户端服务器)框架，使用Netty可以快速开发网络应用，例如服务器和客户端协议。Netty提供了一种新的方式来使开发网络应用程序，这种新的方式使得它很容易使用和有很强的扩展性。Netty的内部实现时很复杂的，但是Netty提供了简单易用的api从网络处理代码中解耦业务逻辑。Netty是完全基于NIO实现的，所以整个Netty都是异步的。
　　书籍：Netty权威指南&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_zeromq&#34;&gt;5.2. ZeroMQ&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ZeroMQ是一种基于消息队列的多线程网络库，其对套接字类型、连接处理、帧、甚至路由的底层细节进行抽象，提供跨越多种传输协议的套接字。ZeroMQ是网络通信中新的一层，介于应用层和传输层之间（按照TCP/IP划分），其是一个可伸缩层，可并行运行，分散在分布式系统间。
ZeroMQ定位为：一个简单好用的传输层，像框架一样的一个socket library，他使得Socket编程更加简单、简洁和性能更高。是一个消息处理队列库，可在多个线程、内核和主机盒之间弹性伸缩。ZMQ的明确目标是“成为标准网络协议栈的一部分，之后进入Linux内核”。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_worker_内部通信技术_disruptor&#34;&gt;6. Worker 内部通信技术(Disruptor)&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_disruptor的来历&#34;&gt;6.1. Disruptor的来历&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;一个公司的业务与技术的关系，一般可以分为三个阶段。第一个阶段就是跟着业务跑。第二个阶段是经历了几年的时间，才达到的驱动业务阶段。第三个阶段，技术引领业务的发展乃至企业的发展。所以我们在学习Disruptor这个技术时，不得不提LMAX这个机构，因为Disruptor这门技术就是由LMAX公司开发并开源的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LMAX是在英国注册并受到FSA监管（监管号码为509778）的外汇黄金交易所。LMAX也是欧洲第一家也是唯一一家采用多边交易设施Multilateral Trading Facility（MTF）拥有交易所牌照和经纪商牌照的欧洲顶级金融公司&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LAMX拥有最迅捷的交易平台，顶级技术支持。LMAX交易所使用“（MTF）分裂器Disruptor”技术，可以在极短时间内（一般在3百万秒之一内）处理订单，在一个线程里每秒处理6百万订单。所有订单均为撮合成交形式，无一例外。多边交易设施（MTF）曾经用来设计伦敦证券交易 所（london Stock Exchange）、德国证券及衍生工具交易所（Deutsche Borse）和欧洲证券交易所（Euronext）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2011年LMAX凭借该技术获得了金融行业技术评选大赛的最佳交易系统奖和甲骨文“公爵杯”创新编程框架奖。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_disruptor是什么&#34;&gt;6.2. Disruptor是什么&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1、 简单理解：Disruptor是一个Queue。Disruptor是实现了“队列”的功能，而且是一个有界队列。而队列的应用场景自然就是“生产者-消费者”模型。
2、 在JDK中Queue有很多实现类，包括不限于ArrayBlockingQueue、LinkBlockingQueue，这两个底层的数据结构分别是数组和链表。数组查询快，链表增删快，能够适应大多数应用场景。
3、 但是ArrayBlockingQueue、LinkBlockingQueue都是线程安全的。涉及到线程安全，就会有synchronized、lock等关键字，这就意味着CPU会打架。
4、 Disruptor一种线程之间信息无锁的交换方式（使用CAS（Compare And Swap/Set）操作）。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_disruptor主要特点&#34;&gt;6.3. Disruptor主要特点&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1、 没有竞争=没有锁=非常快。
2、 所有访问者都记录自己的序号的实现方式，允许多个生产者与多个消费者共享相同的数据结构。
3、 在每个对象中都能跟踪序列号（ring buffer，claim Strategy，生产者和消费者），加上神奇的cache line padding，就意味着没有为伪共享和非预期的竞争。
2.4.2、 Disruptor 核心技术点
　　Disruptor可以看成一个事件监听或消息机制，在队列中一边生产者放入消息，另外一边消费者并行取出处理.
　　底层是单个数据结构：一个ring buffer。
　　每个生产者和消费者都有一个次序计算器，以显示当前缓冲工作方式。
　　每个生产者消费者能够操作自己的次序计数器的能够读取对方的计数器，生产者能够读取消费者的计算器确保其在没有锁的情况下是可写的。
　　
　　核心组件
* Ring Buffer 环形的缓冲区，负责对通过 Disruptor 进行交换的数据（事件）进行存储和更新。
* Sequence 通过顺序递增的序号来编号管理通过其进行交换的数据（事件），对数据(事件)的处理过程总是沿着序号逐个递增处理。
* RingBuffer底层是个数组，次序计算器是一个64bit long 整数型，平滑增长。
　　&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163035.png&#34; alt=&#34;2017 03 28 163035&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1、 接受数据并写入到脚标31的位置，之后会沿着序号一直写入，但是不会绕过消费者所在的脚标。
2、 Joumaler和replicator同时读到24的位置，他们可以批量读取数据到30
3、消费逻辑线程读到了14的位置，但是没法继续读下去，因为他的sequence暂停在15的位置上，需要等到他的sequence给他序号。如果sequence能正常工作，就能读取到30的数据。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm组件本地目录树&#34;&gt;7. Storm组件本地目录树&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163042.png&#34; alt=&#34;2017 03 28 163042&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm_zookeeper目录树&#34;&gt;8. Storm zookeeper目录树&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163050.png&#34; alt=&#34;2017 03 28 163050&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm_任务提交的过程&#34;&gt;9. Storm 任务提交的过程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163058.png&#34; alt=&#34;2017 03 28 163058&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;TopologyMetricsRunnable.TaskStartEvent[oldAssignment=&amp;lt;null&amp;gt;,newAssignment=Assignment[masterCodeDir=C:\Users\MAOXIA~1\AppData\Local\Temp\\e73862a8-f7e7-41f3-883d-af494618bc9f\nimbus\stormdist\double11-1-1458909887,nodeHost={61ce10a7-1e78-4c47-9fb3-c21f43a331ba=192.168.1.106},taskStartTimeSecs={1=1458909910, 2=1458909910, 3=1458909910, 4=1458909910, 5=1458909910, 6=1458909910, 7=1458909910, 8=1458909910},workers=[ResourceWorkerSlot[hostname=192.168.1.106,memSize=0,cpu=0,tasks=[1, 2, 3, 4, 5, 6, 7, 8],jvm=&amp;lt;null&amp;gt;,nodeId=61ce10a7-1e78-4c47-9fb3-c21f43a331ba,port=6900]],timeStamp=1458909910633,type=Assign],task2Component=&amp;lt;null&amp;gt;,clusterName=&amp;lt;null&amp;gt;,topologyId=double11-1-1458909887,timestamp=0]&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163734.png&#34; alt=&#34;2017 03 28 163734&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm_消息容错机制&#34;&gt;10. Storm 消息容错机制&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_总体介绍&#34;&gt;10.1. 总体介绍&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在storm中，可靠的信息处理机制是从spout开始的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一个提供了可靠的处理机制的spout需要记录他发射出去的tuple，当下游bolt处理tuple或者子tuple失败时spout能够重新发射。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Storm通过调用Spout的nextTuple()发送一个tuple。为实现可靠的消息处理，首先要给每个发出的tuple带上唯一的ID，并且将ID作为参数传递给SoputOutputCollector的emit()方法：collector.emit(new Values(&#34;value1&#34;,&#34;value2&#34;), msgId); messageid就是用来标示唯一的tupke的，而rootid是随机生成的
给每个tuple指定ID告诉Storm系统，无论处理成功还是失败，spout都要接收tuple树上所有节点返回的通知。如果处理成功，spout的ack()方法将会对编号是msgId的消息应答确认；如果处理失败或者超时，会调用fail()方法。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_基本实现&#34;&gt;10.2. 基本实现&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Storm 系统中有一组叫做&#34;acker&#34;的特殊的任务，它们负责跟踪DAG（有向无环图）中的每个消息。
acker任务保存了spout id到一对值的映射。第一个值就是spout的任务id，通过这个id，acker就知道消息处理完成时该通知哪个spout任务。第二个值是一个64bit的数字，我们称之为&#34;ack val&#34;， 它是树中所有消息的随机id的异或计算结果。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ack val表示了整棵树的的状态，无论这棵树多大，只需要这个固定大小的数字就可以跟踪整棵树。当消息被创建和被应答的时候都会有相同的消息id发送过来做异或。 每当acker发现一棵树的ack val值为0的时候，它就知道这棵树已经被完全处理了&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163743.png&#34; alt=&#34;2017 03 28 163743&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163751.png&#34; alt=&#34;2017 03 28 163751&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163757.png&#34; alt=&#34;2017 03 28 163757&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163804.png&#34; alt=&#34;2017 03 28 163804&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163811.png&#34; alt=&#34;2017 03 28 163811&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_可靠性配置&#34;&gt;10.3. 可靠性配置&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;有三种方法可以去掉消息的可靠性：
将参数Config.TOPOLOGY_ACKERS设置为0，通过此方法，当Spout发送一个消息的时候，它的ack方法将立刻被调用；
Spout发送一个消息时，不指定此消息的messageID。当需要关闭特定消息可靠性的时候，可以使用此方法；
最后，如果你不在意某个消息派生出来的子孙消息的可靠性，则此消息派生出来的子消息在发送时不要做锚定，即在emit方法中不指定输入消息。因为这些子孙消息没有被锚定在任何tuple tree中，因此他们的失败不会引起任何spout重新发送消息。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;一个worker进程运行一个专用的接收线程来负责将外部发送过来的消息移动到对应的executor线程的incoming-queue中
transfer-queue的大小由参数topology.transfer.buffer.size来设置。transfer-queue的每个元素实际上代表一个tuple的集合
transfer-queue的大小由参数topology.transfer.buffer.size来设置。
executor的incoming-queue的大小用户可以自定义配置。
executor的outgoing-queue的大小用户可以自定义配置&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>storm集群部署,单词计数,Stream Grouping</title>
      <link>/post/bigdata/storm/storm%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2,%E5%8D%95%E8%AF%8D%E8%AE%A1%E6%95%B0,Stream%20Grouping/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/storm/storm%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2,%E5%8D%95%E8%AF%8D%E8%AE%A1%E6%95%B0,Stream%20Grouping/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;storm&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_集群部署的基本流程&#34;&gt;1. 集群部署的基本流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_集群部署的基础环境准备&#34;&gt;2. 集群部署的基础环境准备&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm集群部署&#34;&gt;3. Storm集群部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm常用操作命令&#34;&gt;4. Storm常用操作命令&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm集群的进程及日志熟悉&#34;&gt;5. Storm集群的进程及日志熟悉&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm源码下载及目录熟悉&#34;&gt;6. Storm源码下载及目录熟悉&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm单词技术案例_重点掌握&#34;&gt;7. Storm单词技术案例（重点掌握）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_集群部署的基本流程&#34;&gt;1. 集群部署的基本流程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;集群部署的流程：下载安装包、解压安装包、修改配置文件、分发安装包、启动集群
注意：
    所有的集群上都需要配置hosts&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vi  /etc/hosts
192.168.239.128 storm01 zk01 hadoop01
192.168.239.129 storm02 zk02 hadoop02
192.168.239.130 storm03 zk03 hadoop03&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_集群部署的基础环境准备&#34;&gt;2. 集群部署的基础环境准备&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;安装前的准备工作（zk集群已经部署完毕）&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;关闭防火墙&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;chkconfig iptables off  &amp;amp;&amp;amp; setenforce 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建用户&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;groupadd realtime &amp;amp;&amp;amp;　useradd realtime　&amp;amp;&amp;amp; usermod -a -G realtime realtime&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建工作目录并赋权&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mkdir /export
mkdir /export/servers
chmod 755 -R /export&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;切换到realtime用户下&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su realtime&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm集群部署&#34;&gt;3. Storm集群部署&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;下载安装包&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;wget http://124.202.164.6/files/1139000006794ECA/apache.fayea.com/storm/apache-storm-0.9.5/apache-storm-0.9.5.tar.gz&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压安装包&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf apache-storm-0.9.5.tar.gz -C /export/servers/
cd /export/servers/
ln -s apache-storm-0.9.5 storm&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改配置文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mv /export/servers/storm/conf/storm.yaml /export/servers/storm/conf/storm.yaml.bak
vi /export/servers/storm/conf/storm.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;输入以下内容：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-01.png&#34; alt=&#34;2017 03 28 01&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分发安装包&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;scp -r /export/servers/apache-storm-0.9.5 storm02:/export/servers
#然后分别在各机器上创建软连接
cd /export/servers/
ln -s apache-storm-0.9.5 storm&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动集群&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#在nimbus.host所属的机器上启动 nimbus服务
cd /export/servers/storm/bin/
nohup ./storm nimbus &amp;amp;
#在nimbus.host所属的机器上启动ui服务
cd /export/servers/storm/bin/
nohup ./storm ui &amp;amp;
#在其它个点击上启动supervisor服务
cd /export/servers/storm/bin/
nohup ./storm supervisor &amp;amp;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看集群
访问nimbus.host:/8080，即可看到storm的ui界面。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-02.png&#34; alt=&#34;2017 03 28 02&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm常用操作命令&#34;&gt;4. Storm常用操作命令&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;有许多简单且有用的命令可以用来管理拓扑，它们可以提交、杀死、禁用、再平衡拓扑。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;提交任务命令格式&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;storm jar 【jar路径】 【拓扑包名.拓扑类名】 【拓扑名称】&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/storm jar examples/storm-starter/storm-starter-topologies-0.9.6.jar storm.starter.WordCountTopology wordcount&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;杀死任务命令格式&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;storm kill 【拓扑名称】 -w 10（执行kill命令时可以通过-w [等待秒数]指定拓扑停用以后的等待时间）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;storm kill topology-name -w 10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;停用任务命令格式&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;storm deactivte  【拓扑名称】&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;storm deactivte topology-name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;我们能够挂起或停用运行中的拓扑。当停用拓扑时，所有已分发的元组都会得到处理，但是spouts的nextTuple方法不会被调用。销毁一个拓扑，可以使用kill命令。它会以一种安全的方式销毁一个拓扑，首先停用拓扑，在等待拓扑消息的时间段内允许拓扑完成当前的数据流。&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;启用任务命令格式&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;storm activate【拓扑名称】&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;storm activate topology-name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;重新部署任务命令格式&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;storm rebalance  【拓扑名称】&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;storm rebalance topology-name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;再平衡使你重分配集群任务。这是个很强大的命令。比如，你向一个运行中的集群增加了节点。再平衡命令将会停用拓扑，然后在相应超时时间之后重分配工人，并重启拓扑。&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm集群的进程及日志熟悉&#34;&gt;5. Storm集群的进程及日志熟悉&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;部署成功之后，启动storm集群。
依次启动集群的各种角色&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看nimbus的日志信息
在nimbus的服务器上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /export/servers/storm/logs
tail -100f /export/servers/storm/logs/nimbus.log&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看ui运行日志信息
在ui的服务器上，一般和nimbus一个服务器&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /export/servers/storm/logs
tail -100f /export/servers/storm/logs/ui.log&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看supervisor运行日志信息
在supervisor服务上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /export/servers/storm/logs
tail -100f /export/servers/storm/logs/supervisor.log&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看supervisor上worker运行日志信息
在supervisor服务上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /export/servers/storm/logs
tail -100f /export/servers/storm/logs/worker-6702.log&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-03.png&#34; alt=&#34;2017 03 28 03&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;(该worker正在运行wordcount程序)&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm源码下载及目录熟悉&#34;&gt;6. Storm源码下载及目录熟悉&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;在Storm官方网站上寻找源码地址&lt;br&gt;
&lt;a href=&#34;http://storm.apache.org/downloads.html&#34; class=&#34;bare&#34;&gt;http://storm.apache.org/downloads.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;点击文字标签进入github&lt;br&gt;
点击Apache/storm文字标签，进入github
      &lt;a href=&#34;https://github.com/apache/storm&#34; class=&#34;bare&#34;&gt;https://github.com/apache/storm&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拷贝storm源码地址&lt;br&gt;
在网页右侧，拷贝storm源码地址&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用Subversion客户端下载&lt;br&gt;
&lt;a href=&#34;https://github.com/apache/storm/tags/v0.9.5&#34; class=&#34;bare&#34;&gt;https://github.com/apache/storm/tags/v0.9.5&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Storm源码目录分析（重要）&lt;br&gt;
扩展包中的三个项目，使storm能与hbase、hdfs、kafka交互&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm单词技术案例_重点掌握&#34;&gt;7. Storm单词技术案例（重点掌握）&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;功能说明&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;设计一个topology，来实现对文档里面的单词出现的频率进行统计。
整个topology分为三个部分：
RandomSentenceSpout：
    数据源，在已知的英文句子中，随机发送一条句子出去。
SplitSentenceBolt：
    负责将单行文本记录（句子）切分成单词
WordCountBolt：
    负责对单词的频率进行累加&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;项目主要流程&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-04.png&#34; alt=&#34;2017 03 28 04&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RandomSentenceSpout的实现及生命周期&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-05.png&#34; alt=&#34;2017 03 28 05&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SplitSentenceBolt的实现及生命周期&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-06.png&#34; alt=&#34;2017 03 28 06&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WordCountBolt的实现及生命周期&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-07.png&#34; alt=&#34;2017 03 28 07&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stream Grouping详解&lt;br&gt;
Storm里面有7种类型的stream grouping&lt;/p&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Shuffle Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;随机分组， 随机派发stream里面的tuple，保证每个bolt接收到的tuple数目大致相同。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Fields Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;按字段分组，比如按userid来分组，具有同样userid的tuple会被分到相同的Bolts里的一个task，而不同的userid则会被分配到不同的bolts里的task。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;All Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;广播发送，对于每一个tuple，所有的bolts都会收到。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Global Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;全局分组， 这个tuple被分配到storm中的一个bolt的其中一个task。再具体一点就是分配给id值最低的那个task。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Non Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;不分组，这stream grouping个分组的意思是说stream不关心到底谁会收到它的tuple。目前这种分组和Shuffle grouping是一样的效果， 有一点不同的是storm会把这个bolt放到这个bolt的订阅者同一个线程里面去执行。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Direct Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;直接分组， 这是一种比较特别的分组方法，用这种分组意味着消息的发送者指定由消息接收者的哪个task处理这个消息。只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来获取处理它的消息的task的id （OutputCollector.emit方法也会返回task的id）。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Local or shuffle grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;如果目标bolt有一个或者多个task在同一个工作进程中，tuple将会被随机发生给这些tasks。否则，和普通的Shuffle Grouping行为一致。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>storm</title>
      <link>/post/bigdata/storm/storm/</link>
      <pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/storm/storm/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;storm 简介&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_离线计算是什么&#34;&gt;1. 离线计算是什么？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_流式计算是什么&#34;&gt;2. 流式计算是什么&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_离线计算与实时计算的区别&#34;&gt;3. 离线计算与实时计算的区别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm是什么&#34;&gt;4. Storm是什么？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm与hadoop的区别&#34;&gt;5. Storm与Hadoop的区别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm应用场景及行业案例&#34;&gt;6. Storm应用场景及行业案例&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_典型案列&#34;&gt;6.1. 典型案列&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm核心组件_重要&#34;&gt;7. Storm核心组件（重要）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm编程模型_重要&#34;&gt;8. Storm编程模型（重要）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_流式计算一般架构图_重要&#34;&gt;9. 流式计算一般架构图（重要）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_离线计算是什么&#34;&gt;1. 离线计算是什么？&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;    离线计算：批量获取数据、批量传输数据、周期性批量计算数据、数据展示
    代表技术：Sqoop批量导入数据、HDFS批量存储数据、MapReduce批量计算数据、Hive批量计算数据、***任务调度
1，hivesql
2、调度平台
3、Hadoop集群运维
4、数据清洗（脚本语言）
5、元数据管理
6、数据稽查
7、数据仓库模型架构&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_流式计算是什么&#34;&gt;2. 流式计算是什么&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;流式计算：数据实时产生、数据实时传输、数据实时计算、实时展示
代表技术：Flume实时获取数据、Kafka/metaq实时数据存储、Storm/JStorm实时数据计算、Redis实时结果缓存、持久化存储(mysql)。
一句话总结：将源源不断产生的数据实时收集并实时计算，尽可能快的得到计算结果&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_离线计算与实时计算的区别&#34;&gt;3. 离线计算与实时计算的区别&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;最大的区别：实时收集、实时计算、实时展示&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm是什么&#34;&gt;4. Storm是什么？&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;Flume实时采集，低延迟
Kafka消息队列，低延迟
Storm实时计算，低延迟
Redis实时存储，低延迟

Storm用来实时处理数据，特点：低延迟、高可用、分布式、可扩展、数据不丢失。提供简单容易理解的接口，便于开发。


海量数据？数据类型很多，产生数据的终端很多，处理数据能力增强&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm与hadoop的区别&#34;&gt;5. Storm与Hadoop的区别&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;   Storm用于实时计算，Hadoop用于离线计算。
   Storm处理的数据保存在内存中，源源不断；Hadoop处理的数据保存在文件系统中，一批一批。
   Storm的数据通过网络传输进来；Hadoop的数据保存在磁盘中。
   Storm与Hadoop的编程模型相似

Job：任务名称
JobTracker：项目经理
TaskTracker：开发组长、产品经理
Child:负责开发的人员
Mapper/Reduce:开发人员中的两种角色，一种是服务器开发、一种是客户端开发

Topology:任务名称
Nimbus:项目经理
Supervisor:开组长、产品经理
Worker:开人员
Spout/Bolt：开人员中的两种角色，一种是服务器开发、一种是客户端开发&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm应用场景及行业案例&#34;&gt;6. Storm应用场景及行业案例&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;        Storm用来实时计算源源不断产生的数据，如同流水线生产。
6.1、运用场景
   日志分析
从海量日志中分析出特定的数据，并将分析的结果存入外部存储器用来辅佐决策。
   管道系统
将一个数据从一个系统传输到另外一个系统，比如将数据库同步到Hadoop
   消息转化器
将接受到的消息按照某种格式进行转化，存储到另外一个系统如消息中间件&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_典型案列&#34;&gt;6.1. 典型案列&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;   一淘-实时分析系统：实时分析用户的属性，并反馈给搜索引擎
最初，用户属性分析是通过每天在云梯上定时运行的MR job来完成的。为了满足实时性的要求，希望能够实时分析用户的行为日志，将最新的用户属性反馈给搜索引擎，能够为用户展现最贴近其当前需求的结果。
   携程-网站性能监控：实时分析系统监控携程网的网站性能
利用HTML5提供的performance标准获得可用的指标，并记录日志。Storm集群实时分析日志和入库。使用DRPC聚合成报表，通过历史数据对比等判断规则，触发预警事件。
   阿里妈妈-用户画像：实时计算用户的兴趣数据
为了更加精准投放广告，阿里妈妈后台计算引擎需要维护每个用户的兴趣点（理想状态是，你对什么感兴趣，就向你投放哪类广告）。用户兴趣主要基于用户的历史行为、用户的实时查询、用户的实时点击、用户的地理信息而得，其中实时查询、实时点击等用户行为都是实时数据。考虑到系统的实时性，阿里妈妈使用Storm维护用户兴趣数据，并在此基础上进行受众定向的广告投放。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm核心组件_重要&#34;&gt;7. Storm核心组件（重要）&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-26-3.png&#34; alt=&#34;2017 03 26 3&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;   Nimbus：负责资源分配和任务调度。
   Supervisor：负责接受nimbus分配的任务，启动和停止属于自己管理的worker进程。---通过配置文件设置当前supervisor上启动多少个worker。
   Worker：运行具体处理组件逻辑的进程。Worker运行的任务类型只有两种，一种是Spout任务，一种是Bolt任务。
   Task：worker中每一个spout/bolt的线程称为一个task. 在storm0.8之后，task不再与物理线程对应，不同spout/bolt的task可能会共享一个物理线程，该线程称为executor。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm编程模型_重要&#34;&gt;8. Storm编程模型（重要）&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-26-2.png&#34; alt=&#34;2017 03 26 2&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;   Topology：Storm中运行的一个实时应用程序的名称。（拓扑）
   Spout：在一个topology中获取源数据流的组件。
通常情况下spout会从外部数据源中读取数据，然后转换为topology内部的源数据。
   Bolt：接受数据然后执行处理的组件,用户可以在其中执行自己想要的操作。
   Tuple：一次消息传递的基本单元，理解为一组消息就是一个Tuple。
   Stream：表示数据的流向。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_流式计算一般架构图_重要&#34;&gt;9. 流式计算一般架构图（重要）&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-26-1.png&#34; alt=&#34;2017 03 26 1&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;   其中flume用来获取数据。
   Kafka用来临时保存数据。
   Strom用来计算数据。
   Redis是个内存数据库，用来保存数据。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>CentOS7</title>
      <link>/post/linux/CentOS7/</link>
      <pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/linux/CentOS7/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;CentOS7&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_centos7&#34;&gt;1. CentOS7&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_安装_docker&#34;&gt;1.1. 安装 docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_centos7&#34;&gt;1. CentOS7&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装_docker&#34;&gt;1.1. 安装 docker&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hadoop-docker</title>
      <link>/post/bigdata/hadoop/docker/hadoop-docker/</link>
      <pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/hadoop/docker/hadoop-docker/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Dockerfiles&#34; class=&#34;bare&#34;&gt;https://github.com/HariSekhon/Dockerfiles&lt;/a&gt;
&lt;a href=&#34;https://hub.docker.com/r/harisekhon&#34; class=&#34;bare&#34;&gt;https://hub.docker.com/r/harisekhon&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>