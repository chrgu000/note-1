{
  "version": 1,
  "width": 93,
  "height": 22,
  "title": null,
  "duration": 188.437714,
  "env": {
    "TERM": "xterm",
    "SHELL": null
  },
  "stdout": [
    [
      0.01983,
      "\u001b]0;@d2d9acf07561:/\u0007"
    ],
    [
      0.000268,
      "\u001b[?1034h"
    ],
    [
      6.2e-05,
      "[root@d2d9acf07561 /]# "
    ],
    [
      13.983654,
      "s"
    ],
    [
      6.7e-05,
      "s"
    ],
    [
      5.2e-05,
      "h"
    ],
    [
      0.000705,
      " "
    ],
    [
      4.4e-05,
      "root@196.168.1.34"
    ],
    [
      1.061604,
      "\r\n"
    ],
    [
      0.709919,
      "Last login: Mon Apr 10 14:25:28 2017 from dishui.io\r\r\n"
    ],
    [
      0.035428,
      "\u001b]0;root@localhost:~\u0007\u001b[?1034h[root@localhost ~]# "
    ],
    [
      6.617652,
      "docker exec -it spark-master /bin/bash \\\r\n> && cd $SPARK_HOME \\\r\n> && bin/spark-shell --master spark://master:7077"
    ],
    [
      1.238885,
      "\r\n"
    ],
    [
      0.173377,
      "root@master:/# "
    ],
    [
      11.285939,
      "\r\u001b[Kroot@master:/# cd $SP"
    ],
    [
      0.002854,
      "ARK_HOME \\\r\n> && bin/spark-s"
    ],
    [
      0.227898,
      "hell --master spark://master:7077"
    ],
    [
      0.900494,
      "\r\n"
    ],
    [
      7.554844,
      "Welcome to\r\n      ____              __\r\n     / __/__  ___ _____/ /__\r\n    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.5.2\r\n      /_/\r\n\r\n"
    ],
    [
      0.004106,
      "Using Scala version 2.10.4 (OpenJDK 64-Bit Server VM, Java 1.8.0_102)\r\nType in expressions to have them evaluated.\r\nType :help for more information.\r\n"
    ],
    [
      3.180764,
      "17/04/10 07:17:20 INFO SparkILoop: Created spark context.."
    ],
    [
      0.001545,
      "\r\n"
    ],
    [
      0.000922,
      "Spark context available as sc."
    ],
    [
      0.010669,
      "\r\n"
    ],
    [
      2.004143,
      "17/04/10 07:17:22 INFO SparkILoop: Created sql context..\r\nSQL context available as sqlContext.\r\n"
    ],
    [
      4.302329,
      "\r\nscala> "
    ],
    [
      5.708049,
      "def fu"
    ],
    [
      0.002981,
      "nc1(index: Int, iter: Iterat"
    ],
    [
      0.019557,
      "or[(I"
    ],
    [
      0.00311,
      "nt)]) : Iterator[String] = {\r\n"
    ],
    [
      0.006477,
      "     | "
    ],
    [
      0.005375,
      "  iter."
    ],
    [
      0.001145,
      "t"
    ],
    [
      0.002089,
      "oList.map(x =>"
    ],
    [
      0.002004,
      " "
    ],
    [
      0.000454,
      "\"[partID:\" +  index + "
    ],
    [
      0.00251,
      "\", val: "
    ],
    [
      0.001243,
      "\" +"
    ],
    [
      0.00172,
      " x +"
    ],
    [
      0.000681,
      " \""
    ],
    [
      0.000384,
      "]\")."
    ],
    [
      0.001028,
      "it"
    ],
    [
      0.005411,
      "erator\r\n"
    ],
    [
      0.003805,
      "     | "
    ],
    [
      0.002714,
      "}"
    ],
    [
      1.021179,
      "\r\n"
    ],
    [
      2.258957,
      "func1: (index: Int, iter: Iterator[Int])Iterator[String]\r\n\r\nscala> "
    ],
    [
      9.386669,
      "val rdd"
    ],
    [
      0.003982,
      "1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)"
    ],
    [
      0.873414,
      "\r\n"
    ],
    [
      0.969874,
      "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:21\r\n\r\nscala> "
    ],
    [
      6.18606,
      "rdd1.mapP"
    ],
    [
      0.006266,
      "artitionsWithIndex(func1).collect"
    ],
    [
      2.119671,
      "\r\n"
    ],
    [
      1.011496,
      "\r[Stage 0:>                                                          (0 + 0) / 2]"
    ],
    [
      0.798615,
      "\r[Stage 0:>                                                          (0 + 1) / 2]"
    ],
    [
      0.631109,
      "\r[Stage 0:=============================>                             (1 + 1) / 2]"
    ],
    [
      0.001114,
      "\r                                                                                \r"
    ],
    [
      0.006858,
      "res0: Array[String] = Array([partID:0, val: 1], [partID:0, val: 2], [partID:0, val: 3], [partID:0, val: 4], [partID:1, val: 5], [partID:1, val: 6], [partID:1, val: 7], [partID:1, val: 8], [partID:1, val: 9])\r\n\r\nscala> "
    ],
    [
      7.848169,
      "rdd1.a"
    ],
    [
      0.002695,
      "ggr"
    ],
    [
      0.002988,
      "egate(0)(_+_, _+_)"
    ],
    [
      0.910558,
      "\r\n"
    ],
    [
      0.686152,
      "res1: Int = 45\r\n\r\nscala> "
    ],
    [
      5.650566,
      "rdd1.a"
    ],
    [
      0.008124,
      "ggregate(0)(math.max(_, _), _ + _)"
    ],
    [
      0.92351,
      "\r\n"
    ],
    [
      0.579685,
      "res2: Int = 13\r\n\r\nscala> "
    ],
    [
      7.364037,
      "rdd1."
    ],
    [
      0.010438,
      "aggregate(5)(math.max(_, _), _ + _)"
    ],
    [
      0.891508,
      "\r\n"
    ],
    [
      0.433408,
      "res3: Int = 19\r\n\r\nscala> "
    ],
    [
      8.42058,
      "val rd"
    ],
    [
      0.00379,
      "d2 "
    ],
    [
      0.001971,
      "="
    ],
    [
      0.002291,
      " sc.paralleliz"
    ],
    [
      0.003341,
      "e(List(\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"),2)"
    ],
    [
      0.871541,
      "\r\n"
    ],
    [
      0.328149,
      "rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at <console>:21\r\n\r\nscala> "
    ],
    [
      5.188897,
      "def func"
    ],
    [
      0.003954,
      "2(index: Int, iter: Iterator[(String)]) : Iterator[String] = {"
    ],
    [
      0.001795,
      "\r\n     |   ite"
    ],
    [
      0.002351,
      "r.to"
    ],
    [
      0.006152,
      "List.map(x => \"[partID:\" +  in"
    ],
    [
      0.001158,
      "dex + "
    ],
    [
      0.006081,
      "\", val: \" + x + \"]\").iterator\r\n     | }"
    ],
    [
      0.880499,
      "\r\n"
    ],
    [
      0.388645,
      "func2: (index: Int, iter: Iterator[String])Iterator[String]\r\n\r\nscala> "
    ],
    [
      5.218964,
      "rdd"
    ],
    [
      0.002319,
      "2.a"
    ],
    [
      0.00315,
      "ggregat"
    ],
    [
      0.005157,
      "e(\"\")(_ + _, _ + _)"
    ],
    [
      0.792354,
      "\r\n"
    ],
    [
      0.404589,
      "res4: String = abcdef\r\n\r\nscala> "
    ],
    [
      4.797023,
      "rdd2.aggre"
    ],
    [
      0.005348,
      "ga"
    ],
    [
      0.003416,
      "te(\"=\")(_ + _, _ + "
    ],
    [
      0.007881,
      "_)"
    ],
    [
      0.793657,
      "\r\n"
    ],
    [
      0.399024,
      "res5: String = ==abc=def\r\n\r\nscala> "
    ],
    [
      6.054506,
      "val rdd3 = "
    ],
    [
      0.002697,
      "sc.parallelize(List("
    ],
    [
      0.014125,
      "\"12\",\"2"
    ],
    [
      0.003848,
      "3\",\"345\",\"4567\"),2)"
    ],
    [
      0.768379,
      "\r\n"
    ],
    [
      0.280614,
      "rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[3] at parallelize at <console>:21\r\n\r\nscala> "
    ],
    [
      6.490941,
      "rdd3.aggr"
    ],
    [
      0.002119,
      "egate("
    ],
    [
      0.001534,
      "\"\")((x,y) => math.max"
    ],
    [
      0.00206,
      "(x.length, y.len"
    ],
    [
      0.005599,
      "g"
    ],
    [
      0.005501,
      "th).toString, (x,y) => x "
    ],
    [
      0.014904,
      "+ y)"
    ],
    [
      0.837827,
      "\r\n"
    ],
    [
      0.384123,
      "res6: String = 24\r\n\r\nscala> "
    ],
    [
      5.365481,
      "val"
    ],
    [
      0.000865,
      " "
    ],
    [
      0.001812,
      "rdd4 = sc.p"
    ],
    [
      0.004705,
      "arallelize(List(\"12\",\"23\",\"345\",\"\"),2)"
    ],
    [
      0.79868,
      "\r\n"
    ],
    [
      0.256686,
      "rdd4: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at <console>:21\r\n\r\nscala> "
    ],
    [
      5.785738,
      "rdd4"
    ],
    [
      0.002647,
      ".aggreg"
    ],
    [
      0.008721,
      "ate(\"\""
    ],
    [
      0.003205,
      ")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y)"
    ],
    [
      0.777184,
      "\r\n"
    ],
    [
      0.364618,
      "res7: String = 10\r\n\r\nscala> "
    ],
    [
      5.45887,
      "val rdd5 "
    ],
    [
      0.000338,
      "= s"
    ],
    [
      0.002994,
      "c.parallelize(List(\"12\",\"23\""
    ],
    [
      6.5e-05,
      ",\""
    ],
    [
      0.000378,
      "\",\"345\")"
    ],
    [
      0.007536,
      ",2)"
    ],
    [
      0.860622,
      "\r\n"
    ],
    [
      0.244952,
      "rdd5: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at parallelize at <console>:21\r\n\r\nscala> "
    ],
    [
      6.329275,
      "rdd5.aggregat"
    ],
    [
      0.012102,
      "e(\"\")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y)"
    ],
    [
      1.046761,
      "\r\n"
    ],
    [
      0.330062,
      "res8: String = 11\r\n\r\nscala> "
    ],
    [
      4.184258,
      "Stopping spark context.\r\n"
    ],
    [
      0.931446,
      "root@master:/usr/local/spark-1.5.2# "
    ],
    [
      0.552468,
      "exit\r\n"
    ],
    [
      0.016511,
      "-bash: bin/spark-shell: No such file or directory\r\n\u001b]0;root@localhost:~\u0007[root@localhost ~]# "
    ],
    [
      0.906125,
      "logout\r\nConnection to 196.168.1.34 closed.\r\r\n"
    ],
    [
      0.000518,
      "\u001b]0;@d2d9acf07561:/\u0007"
    ],
    [
      0.091754,
      "[root@d2d9acf07561 /]# "
    ],
    [
      1.000261,
      "exit"
    ],
    [
      3.3e-05,
      "\r\n"
    ]
  ],
  "command": "bash"
}