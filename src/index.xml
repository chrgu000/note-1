<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Src-rsses on 零零碎碎</title>
    <link>/src/index.xml</link>
    <description>Recent content in Src-rsses on 零零碎碎</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <atom:link href="/src/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title></title>
      <link>/src/java/alter-page/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/java/alter-page/</guid>
      <description>&lt;%@ page language=&#34;java&#34; import=&#34;java.util.*&#34; pageEncoding=&#34;UTF-8&#34;%&gt;
&lt;%@ page isELIgnored=&#34;false&#34;%&gt;
&lt;%@ taglib uri=&#34;http://java.sun.com/jsp/jstl/core&#34; prefix=&#34;c&#34; %&gt;
&lt;%@ taglib prefix=&#34;fn&#34; uri=&#34;http://java.sun.com/jsp/jstl/functions&#34;%&gt;
&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;en&#34;&gt;
&lt;head&gt;
	&lt;meta charset=&#34;UTF-8&#34;&gt;
	&lt;title&gt;Document&lt;/title&gt;
	&lt;style&gt;

		*{margin: 0;padding: 0;
		  font-family:&#34;微软雅黑&#34;,&#39;Microsoft YaHei&#39;,Arial, Helvetica, sans-serif;font-size: 12px;
		}
		input,button,textarea,select,optgroup,option{font-family:inherit; font-size:inherit;font-style:inherit;font-weight:inherit;outline:none;}
		input,button,textarea,select{*font-size:100%;-webkit-box-sizing:content-box;box-sizing:content-box;resize:none;}
		a{text-decoration: none;color:#000;}
		.fl{
			float: left;
		}
		.fr{
			float: right;
		}
		.m-box{
			width: inherit;
			height: inherit;
			border: 4px solid #e4e4e4;
			position: absolute;
			top: 50%;
			left: 50%;
			background: white;
			margin-top: -68px;
			margin-left: -125px;
		}
		.m-top{
			border: 1px solid #d9d9d9;
			height: 31px;
			background: #eeeeee;
			line-height: 31px;
		}
		.m-top-txt{
			margin-left: 6px;
			color: #666666;
		}
		.m-top-clo{
			line-height: 31px;
			padding:0 9px;
			color: #666666;
		}
		.m-con-txt{
			line-height: 76px;
			font-size: 14px;
			text-align: center;
		}
		.m-btn{
			padding-top: 12px;
			text-align: center;
		}
		.m-btn a{
			padding: 8px 36px;
			line-height: 100%;
			border: 1px solid #c9c9c9;
			margin: 0 10px;
			background: #f4f4f4;
		}
		.inp{
			padding: 5px;
			height: 14px;
			width: 80px;
			margin-left: 10px;
			line-height: 100%;
		}
		.m-con-txt-error{width:100px; margin-left: 120px; position:absolute;top:90px;left:0;}
		input.error { border: 1px solid red; }
		label.error {display: block;color: red}
	&lt;/style&gt;
&lt;/head&gt;
&lt;body style=&#34;width: 240px;height: 120px;&#34;&gt;
		&lt;div class=&#34;m-box&#34;&gt;
			&lt;div class=&#34;m-top fix&#34;&gt;
				&lt;span class=&#39;m-top-txt fl&#39;&gt;现货市场缓存更新&lt;/span&gt;
			&lt;/div&gt;
			&lt;div class=&#34;m-con&#34;&gt;
				&lt;p class=&#34;${param.result == &#39;1&#39;?&#39;m-con-txt&#39;:&#39;m-con-txt-error&#39;}&#34;&gt;${param.result == &#39;1&#39;?&#39;更新成功&#39;:&#39;更新失败&#39; }!&lt;/p&gt;
			&lt;/div&gt;
		&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/activemq/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/activemq/README/</guid>
      <description>

&lt;h1 id=&#34;tomcat&#34;&gt;Tomcat&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://tomcat.apache.org&#34;&gt;Tomcat&lt;/a&gt; is is an open source implementation of the Java Servlet, JavaServer Pages, Java Expression Language and Java WebSocket technologies.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This chart creates a &lt;a href=&#34;http://tomcat.apache.org&#34;&gt;tomcat application server&lt;/a&gt; Deployment, plus http Services for the server.
The chart offers an optimization for application updates running in a servlet container-type engines like tomcat and &lt;a href=&#34;http://jbossas.jboss.org&#34;&gt;Jboss&lt;/a&gt;. The chart uses the WAR, EAR, and other deployable components outside of the Servlet engine as sidecar container so application upgrades requires the sidecar container image only to be updated and not the Servlet engine as if both would run at the same image.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 1.8+&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;provider-specific-prerequisites&#34;&gt;Provider-specific Prerequisites&lt;/h2&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release stable/tomcat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command deploys a tomcat dedicated server with sane defaults.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: List all releases using &lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the &lt;code&gt;my-release&lt;/code&gt; deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes all the Kubernetes components associated with the chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The following table lists the configurable parameters of the tomcat chart and their default values.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.webarchive.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sidecar image source repository name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ananwaresystems/webarchive&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.webarchive.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;webarchive&lt;/code&gt; release tag.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.tomcat.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Tomact image source repository name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;tomcat&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.tomcat.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;tomcat&lt;/code&gt; release tag.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;7.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;deploy.directory&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Webarchive deployment directory&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/usr/local/tomcat/webapps&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Tomcat service name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;http&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.externalPort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes service port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;80&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.internalPort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Tomcat front port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;8080&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes service type&lt;/td&gt;
&lt;td&gt;&lt;code&gt;LoadBalancer&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;CPU/Memory resource requests/limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Node affinity&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Node tolerations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Refer to &lt;a href=&#34;values.yaml&#34;&gt;values.yaml&lt;/a&gt; for the full run-down on defaults. These are a mixture of Kubernetes and tomcat-related directives that map to environment variables.&lt;/p&gt;

&lt;p&gt;Specify each parameter using the &lt;code&gt;--set key=value[,key=value]&lt;/code&gt; argument to &lt;code&gt;helm install&lt;/code&gt;. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release \
  --set Values.someval=My Server,ImageTag=1.0 \
    stable/tomcat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above command deploys Tomcat dedicated with a server name of &lt;code&gt;My Server&lt;/code&gt; and docker-tomcat image version &lt;code&gt;1.0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release -f values.yaml stable/stable
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: You can use the default &lt;a href=&#34;values.yaml&#34;&gt;values.yaml&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;persistence&#34;&gt;Persistence&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;credits&#34;&gt;Credits&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/javaweb-tomcat-sidecar&#34;&gt;Java Web Application with Tomcat and Sidecar Container&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/elastic-stack/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/elastic-stack/README/</guid>
      <description>

&lt;h1 id=&#34;elastic-stack-helm-chart&#34;&gt;Elastic-stack Helm Chart&lt;/h1&gt;

&lt;p&gt;This chart installs an elasticsearch cluster with kibana and logstash by default.
You can optionally disable logstash and install Fluentd if you prefer. It also optionally installs filebeat, nginx-ldapauth-proxy and elasticsearch-curator.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites-details&#34;&gt;Prerequisites Details&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 1.8+&lt;/li&gt;
&lt;li&gt;PV dynamic provisioning support on the underlying infrastructure&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;chart-details&#34;&gt;Chart Details&lt;/h2&gt;

&lt;p&gt;This chart will do the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implemented a dynamically scalable elasticsearch cluster using Kubernetes StatefulSets/Deployments&lt;/li&gt;
&lt;li&gt;Multi-role deployment: master, client (coordinating) and data nodes&lt;/li&gt;
&lt;li&gt;Statefulset Supports scaling down without degrading the cluster&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release stable/elastic-stack
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deleting-the-charts&#34;&gt;Deleting the Charts&lt;/h2&gt;

&lt;p&gt;Delete the Helm deployment as normal&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deletion of the StatefulSet doesn&amp;rsquo;t cascade to deleting associated PVCs. To delete them:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete pvc -l release=my-release,component=data
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;Each requirement is configured with the options provided by that Chart.
Please consult the relevant charts for their configuration options.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/elastic-stack/charts/elasticsearch/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/elastic-stack/charts/elasticsearch/README/</guid>
      <description>

&lt;h1 id=&#34;elasticsearch-helm-chart&#34;&gt;Elasticsearch Helm Chart&lt;/h1&gt;

&lt;p&gt;This chart uses a standard Docker image of Elasticsearch (docker.elastic.co/elasticsearch/elasticsearch-oss) and uses a service pointing to the master&amp;rsquo;s transport port for service discovery.
Elasticsearch does not communicate with the Kubernetes API, hence no need for RBAC permissions.&lt;/p&gt;

&lt;h2 id=&#34;warning-for-previous-users&#34;&gt;Warning for previous users&lt;/h2&gt;

&lt;p&gt;If you are currently using an earlier version of this Chart you will need to redeploy your Elasticsearch clusters. The discovery method used here is incompatible with using RBAC.
If you are upgrading to Elasticsearch 6 from the 5.5 version used in this chart before, please note that your cluster needs to do a full cluster restart.
The simplest way to do that is to delete the installation (keep the PVs) and install this chart again with the new version.
If you want to avoid doing that upgrade to Elasticsearch 5.6 first before moving on to Elasticsearch 6.0.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites-details&#34;&gt;Prerequisites Details&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 1.6+&lt;/li&gt;
&lt;li&gt;PV dynamic provisioning support on the underlying infrastructure&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;statefulsets-details&#34;&gt;StatefulSets Details&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34;&gt;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;statefulsets-caveats&#34;&gt;StatefulSets Caveats&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#limitations&#34;&gt;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#limitations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;todo&#34;&gt;Todo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Implement TLS/Auth/Security&lt;/li&gt;
&lt;li&gt;Smarter upscaling/downscaling&lt;/li&gt;
&lt;li&gt;Solution for memory locking&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;chart-details&#34;&gt;Chart Details&lt;/h2&gt;

&lt;p&gt;This chart will do the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Implemented a dynamically scalable elasticsearch cluster using Kubernetes StatefulSets/Deployments&lt;/li&gt;
&lt;li&gt;Multi-role deployment: master, client (coordinating) and data nodes&lt;/li&gt;
&lt;li&gt;Statefulset Supports scaling down without degrading the cluster&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release stable/elasticsearch
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deleting-the-charts&#34;&gt;Deleting the Charts&lt;/h2&gt;

&lt;p&gt;Delete the Helm deployment as normal&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deletion of the StatefulSet doesn&amp;rsquo;t cascade to deleting associated PVCs. To delete them:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete pvc -l release=my-release,component=data
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The following table lists the configurable parameters of the elasticsearch chart and their default values.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;appVersion&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Application Version (Elasticsearch)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;6.5.3&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container image name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker.elastic.co/elasticsearch/elasticsearch-oss&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;6.5.1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;initImage.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Init container image name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;busybox&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;initImage.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Init container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;latest&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;initImage.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Init container pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Always&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cluster.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Cluster name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;elasticsearch&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cluster.xpackEnable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Writes the X-Pack configuration options to the configuration file&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cluster.config&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional cluster config appended&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cluster.keystoreSecret&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of secret holding secure config options in an es keystore&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cluster.env&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Cluster environment variables&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{MINIMUM_MASTER_NODES: &amp;quot;2&amp;quot;}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cluster.additionalJavaOpts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Cluster parameters to be added to &lt;code&gt;ES_JAVA_OPTS&lt;/code&gt; environment variable&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client component name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;client&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.replicas&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client node replicas (deployment)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client node resources requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{} - cpu limit must be an integer&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client priorityClass&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.heapSize&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client node heap size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;512m&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client Deployment annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Node labels for client pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client tolerations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.serviceAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client Service annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.serviceType&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client service type&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client loadBalancerIP&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client loadBalancerSourceRanges&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.antiAffinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client anti-affinity policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;soft&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.nodeAffinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client node affinity policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.ingress.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enable Client Ingress&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.ingress.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client Ingress annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.ingress.hosts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client Ingress Hostnames&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;client.ingress.tls&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Client Ingress TLS configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.exposeHttp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Expose http port 9200 on master Pods for monitoring, etc&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master component name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.replicas&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master node replicas (deployment)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master node resources requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{} - cpu limit must be an integer&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master priorityClass&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master Deployment annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Node labels for master pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master tolerations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.heapSize&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master node heap size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;512m&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master component name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master persistent enabled/disabled&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master statefulset PVC template name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;data&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master persistent volume size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;4Gi&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.storageClass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master persistent volume Class&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.accessMode&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master persistent Access Mode&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ReadWriteOnce&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.antiAffinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master anti-affinity policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;soft&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.nodeAffinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master node affinity policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.updateStrategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Master node update strategy policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{type: &amp;quot;onDelete&amp;quot;}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.exposeHttp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Expose http port 9200 on data Pods for monitoring, etc&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.replicas&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data node replicas (statefulset)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data node resources requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{} - cpu limit must be an integer&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data priorityClass&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.heapSize&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data node heap size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1536m&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;`data.hooks.drain.enabled&lt;/td&gt;
&lt;td&gt;Data nodes: Enable drain pre-stop and post-start hook&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.persistence.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data persistent enabled/disabled&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.persistence.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data statefulset PVC template name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;data&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.persistence.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data persistent volume size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;30Gi&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.persistence.storageClass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data persistent volume Class&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.persistence.accessMode&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data persistent Access Mode&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ReadWriteOnce&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data StatefulSet annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Node labels for data pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data tolerations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.terminationGracePeriodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data termination grace period (seconds)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;3600&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.antiAffinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data anti-affinity policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;soft&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.nodeAffinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data node affinity policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data.updateStrategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Data node update strategy policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{type: &amp;quot;onDelete&amp;quot;}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extraInitContainers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional init container passed through the tpl&lt;/td&gt;
&lt;td&gt;``&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podSecurityPolicy.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specify pod annotations in the pod security policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podSecurityPolicy.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specify if a pod security policy must be created&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.client.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create the client service account&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.client.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the client service account to use or create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ elasticsearch.client.fullname }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.master.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create the master service account&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.master.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the master service account to use or create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ elasticsearch.master.fullname }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.data.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create the data service account&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.data.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the data service account to use or create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ elasticsearch.data.fullname }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Specify each parameter using the &lt;code&gt;--set key=value[,key=value]&lt;/code&gt; argument to &lt;code&gt;helm install&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In terms of Memory resources you should make sure that you follow that equation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;${role}HeapSize &amp;lt; ${role}MemoryRequests &amp;lt; ${role}MemoryLimits&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The YAML value of cluster.config is appended to elasticsearch.yml file for additional customization (&amp;ldquo;script.inline: on&amp;rdquo; for example to allow inline scripting)&lt;/p&gt;

&lt;h1 id=&#34;deep-dive&#34;&gt;Deep dive&lt;/h1&gt;

&lt;h2 id=&#34;application-version&#34;&gt;Application Version&lt;/h2&gt;

&lt;p&gt;This chart aims to support Elasticsearch v2 to v6 deployments by specifying the &lt;code&gt;values.yaml&lt;/code&gt; parameter &lt;code&gt;appVersion&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;version-specific-features&#34;&gt;Version Specific Features&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Memory Locking &lt;em&gt;(variable renamed)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Ingest Node &lt;em&gt;(v5)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;X-Pack Plugin &lt;em&gt;(v5)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Upgrade paths &amp;amp; more info: &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;mlocking&#34;&gt;Mlocking&lt;/h2&gt;

&lt;p&gt;This is a limitation in kubernetes right now. There is no way to raise the
limits of lockable memory, so that these memory areas won&amp;rsquo;t be swapped. This
would degrade performance heavily. The issue is tracked in
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/3595&#34;&gt;kubernetes/#3595&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[WARN ][bootstrap] Unable to lock JVM Memory: error=12,reason=Cannot allocate memory
[WARN ][bootstrap] This can result in part of the JVM being swapped out.
[WARN ][bootstrap] Increase RLIMIT_MEMLOCK, soft limit: 65536, hard limit: 65536
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;minimum-master-nodes&#34;&gt;Minimum Master Nodes&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;The minimum_master_nodes setting is extremely important to the stability of your cluster. This setting helps prevent split brains, the existence of two masters in a single cluster.&lt;/p&gt;

&lt;p&gt;When you have a split brain, your cluster is at danger of losing data. Because the master is considered the supreme ruler of the cluster, it decides when new indices can be created, how shards are moved, and so forth. If you have two masters, data integrity becomes perilous, since you have two nodes that think they are in charge.&lt;/p&gt;

&lt;p&gt;This setting tells Elasticsearch to not elect a master unless there are enough master-eligible nodes available. Only then will an election take place.&lt;/p&gt;

&lt;p&gt;This setting should always be configured to a quorum (majority) of your master-eligible nodes. A quorum is (number of master-eligible nodes / 2) + 1&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;More info: &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/guide/1.x/_important_configuration_changes.html#_minimum_master_nodes&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/guide/1.x/_important_configuration_changes.html#_minimum_master_nodes&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;client-and-coordinating-nodes&#34;&gt;Client and Coordinating Nodes&lt;/h1&gt;

&lt;p&gt;Elasticsearch v5 terminology has updated, and now refers to a &lt;code&gt;Client Node&lt;/code&gt; as a &lt;code&gt;Coordinating Node&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;More info: &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/5.5/modules-node.html#coordinating-node&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/reference/5.5/modules-node.html#coordinating-node&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;enabling-elasticsearch-interal-monitoring&#34;&gt;Enabling elasticsearch interal monitoring&lt;/h2&gt;

&lt;p&gt;Requires version 6.3+ and standard non &lt;code&gt;oss&lt;/code&gt; repository defined. Starting with 6.3 Xpack is partially free and enabled by default. You need to set a new config to enable the collection of these internal metrics. (&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/6.3/monitoring-settings.html&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/reference/6.3/monitoring-settings.html&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;To do this through this helm chart override with the three following changes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;image.repository: docker.elastic.co/elasticsearch/elasticsearch
cluster.xpackEnable: true
cluster.env.XPACK_MONITORING_ENABLED: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: to see these changes you will need to update your kibana repo to &lt;code&gt;image.repository: docker.elastic.co/kibana/kibana&lt;/code&gt; instead of the &lt;code&gt;oss&lt;/code&gt; version&lt;/p&gt;

&lt;h2 id=&#34;select-right-storage-class-for-ssd-volumes&#34;&gt;Select right storage class for SSD volumes&lt;/h2&gt;

&lt;h3 id=&#34;gce-kubernetes-1-5&#34;&gt;GCE + Kubernetes 1.5&lt;/h3&gt;

&lt;p&gt;Create StorageClass for SSD-PD&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f - &amp;lt;&amp;lt;EOF
kind: StorageClass
apiVersion: extensions/v1beta1
metadata:
  name: ssd
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create cluster with Storage class &lt;code&gt;ssd&lt;/code&gt; on Kubernetes 1.5+&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install stable/elasticsearch --name my-release --set data.persistence.storageClass=ssd,data.storage=100Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;usage-of-the-tpl-function&#34;&gt;Usage of the &lt;code&gt;tpl&lt;/code&gt; Function&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;tpl&lt;/code&gt; function allows us to pass string values from &lt;code&gt;values.yaml&lt;/code&gt; through the templating engine. It is used for the following values:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;extraInitContainers&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is important that these values be configured as strings. Otherwise, installation will fail.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/elastic-stack/charts/logstash/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/elastic-stack/charts/logstash/README/</guid>
      <description>

&lt;h1 id=&#34;logstash&#34;&gt;Logstash&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/products/logstash&#34;&gt;Logstash&lt;/a&gt; is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash.”&lt;/p&gt;

&lt;h2 id=&#34;tl-dr&#34;&gt;TL;DR;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/logstash
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install --name my-release stable/logstash
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the &lt;code&gt;my-release&lt;/code&gt; deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes nearly all the Kubernetes components associated with the
chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h2&gt;

&lt;h3 id=&#34;release-and-tune-this-chart-once-per-logstash-pipeline&#34;&gt;Release and tune this chart once per Logstash pipeline&lt;/h3&gt;

&lt;p&gt;To achieve multiple pipelines with this chart, current best practice is to
maintain one pipeline per chart release. In this way configuration is
simplified and pipelines are more isolated from one another.&lt;/p&gt;

&lt;h3 id=&#34;default-pipeline-beats-input-elasticsearch-output&#34;&gt;Default Pipeline: Beats Input -&amp;gt; Elasticsearch Output&lt;/h3&gt;

&lt;p&gt;Current best practice for ELK logging is to ship logs from hosts using Filebeat
to logstash where persistent queues are enabled. Filebeat supports structured
(e.g. JSON) and unstructured (e.g. log lines) log shipment.&lt;/p&gt;

&lt;h3 id=&#34;load-beats-generated-index-template-into-elasticsearch&#34;&gt;Load Beats-generated index template into Elasticsearch&lt;/h3&gt;

&lt;p&gt;To best utilize the combination of Beats, Logstash and Elasticsearch,
load Beats-generated index templates into Elasticsearch as described &lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-template.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On a remote-to-Kubernetes Linux instance you might run the following command to
load that instance&amp;rsquo;s Beats-generated index template into Elasticsearch
(Elasticsearch hostname will vary).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filebeat setup --template -E output.logstash.enabled=false \
  -E &#39;output.elasticsearch.hosts=[&amp;quot;elasticsearch.cluster.local:9200&amp;quot;]&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;links&#34;&gt;Links&lt;/h3&gt;

&lt;p&gt;Please review the following links that expound on current best practices.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/blog/structured-logging-filebeat&#34;&gt;https://www.elastic.co/blog/structured-logging-filebeat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-template.html&#34;&gt;https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-template.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/deploying-and-scaling.html&#34;&gt;https://www.elastic.co/guide/en/logstash/current/deploying-and-scaling.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/persistent-queues.html&#34;&gt;https://www.elastic.co/guide/en/logstash/current/persistent-queues.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The following table lists the configurable parameters of the chart and its default values.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replicaCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Number of replicas&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podDisruptionBudget&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod disruption budget&lt;/td&gt;
&lt;td&gt;&lt;code&gt;maxUnavailable: 1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;updateStrategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Update strategy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;type: RollingUpdate&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container image name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker.elastic.co/logstash/logstash-oss&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;6.5.3&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Service type (ClusterIP, NodePort or LoadBalancer)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Service annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.ports&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ports exposed by service&lt;/td&gt;
&lt;td&gt;beats&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The load balancer IP for the service&lt;/td&gt;
&lt;td&gt;unset&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The cluster IP for the service&lt;/td&gt;
&lt;td&gt;unset&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.nodePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The nodePort for the service&lt;/td&gt;
&lt;td&gt;unset&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.externalTrafficPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Set externalTrafficPolicy&lt;/td&gt;
&lt;td&gt;unset&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ports&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ports exposed by logstash container&lt;/td&gt;
&lt;td&gt;beats&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enables Ingress&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.hosts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress accepted hostnames&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[&amp;quot;logstash.cluster.local&amp;quot;]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.tls&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress TLS configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod resource requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;priorityClassName&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Node selector&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Tolerations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;affinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Affinity or Anti-Affinity&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podLabels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod labels&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;livenessProbe&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Liveness probe settings for logstash container&lt;/td&gt;
&lt;td&gt;(see &lt;code&gt;values.yaml&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;readinessProbe&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Readiness probe settings for logstash container&lt;/td&gt;
&lt;td&gt;(see &lt;code&gt;values.yaml&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enable persistence&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.storageClass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Storage class for PVCs&lt;/td&gt;
&lt;td&gt;unset&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.accessMode&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Access mode for PVCs&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ReadWriteOnce&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Size for PVCs&lt;/td&gt;
&lt;td&gt;&lt;code&gt;2Gi&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;volumeMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Volume mounts to configure for logstash container&lt;/td&gt;
&lt;td&gt;(see &lt;code&gt;values.yaml&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;volumes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Volumes to configure for logstash container&lt;/td&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Duration the pod needs to terminate gracefully&lt;/td&gt;
&lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exporter.logstash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus logstash-exporter settings&lt;/td&gt;
&lt;td&gt;(see &lt;code&gt;values.yaml&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exporter.logstash.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enables Prometheus logstash-exporter&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;elasticsearch.host&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ElasticSearch hostname&lt;/td&gt;
&lt;td&gt;&lt;code&gt;elasticsearch-client.default.svc.cluster.local&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;elasticsearch.port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ElasticSearch port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;9200&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;config&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Logstash configuration key-values&lt;/td&gt;
&lt;td&gt;(see &lt;code&gt;values.yaml&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;patterns&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Logstash patterns configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;inputs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Logstash inputs configuration&lt;/td&gt;
&lt;td&gt;beats&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Logstash filters configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;outputs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Logstash outputs configuration&lt;/td&gt;
&lt;td&gt;elasticsearch&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/elasticsearch/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/elasticsearch/README/</guid>
      <description>

&lt;h1 id=&#34;elasticsearch-helm-chart&#34;&gt;Elasticsearch Helm Chart&lt;/h1&gt;

&lt;p&gt;This functionality is in alpha status and may be changed or removed completely in a future release. Elastic will take a best effort approach to fix any issues, but alpha features are not subject to the support SLA of official GA features.&lt;/p&gt;

&lt;p&gt;This helm chart is a lightweight way to configure and run our official &lt;a href=&#34;https://www.elastic.co/guide/en/kibana/current/docker.html&#34;&gt;Elasticsearch docker image&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; &amp;gt;= 2.8.0&lt;/li&gt;
&lt;li&gt;Kubernetes 1.&lt;sup&gt;8&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt;.&lt;sup&gt;9&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt;.&lt;sup&gt;10&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt;.11.&lt;/li&gt;
&lt;li&gt;Minimum cluster requirements include the following to run this chart with default settings. All of these settings are configurable.

&lt;ul&gt;
&lt;li&gt;Three Kubernetes nodes to respect the default &amp;ldquo;hard&amp;rdquo; affinity settings&lt;/li&gt;
&lt;li&gt;1GB of RAM for the JVM heap&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;usage-notes-and-getting-started&#34;&gt;Usage notes and getting started&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This repo includes a number of &lt;a href=&#34;./examples&#34;&gt;example&lt;/a&gt; configurations which can be used as a reference. They are also used in the automated testing of this chart&lt;/li&gt;
&lt;li&gt;Automated testing of this chart is currently only run against GKE (Google Kubernetes Engine). If you are using a different Kubernetes provider you will likely need to adjust the &lt;code&gt;storageClassName&lt;/code&gt; in the &lt;code&gt;volumeClaimTemplate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The default storage class for GKE is &lt;code&gt;standard&lt;/code&gt; which by default will give you &lt;code&gt;pd-ssd&lt;/code&gt; type persistent volumes. This is network attached storage and will not perform as well as local storage. If you are using Kubernetes version 1.10 or greater you can use &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/local-ssd&#34;&gt;Local PersistentVolumes&lt;/a&gt; for increased performance&lt;/li&gt;
&lt;li&gt;The chart deploys a statefulset and by default will do an automated rolling update of your cluster. It does this by waiting for the cluster health to become green after each instance is updated. If you prefer to update manually you can set &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#on-delete&#34;&gt;&lt;code&gt;updateStrategy: OnDelete&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It is important to verify that the JVM heap size in &lt;code&gt;esJavaOpts&lt;/code&gt; and to set the CPU/Memory &lt;code&gt;resources&lt;/code&gt; to something suitable for your cluster&lt;/li&gt;
&lt;li&gt;To simplify chart and maintenance each set of node groups is deployed as a separate helm release. Take a look at the &lt;a href=&#34;./examples/multi&#34;&gt;multi&lt;/a&gt; example to get an idea for how this works. Without doing this it isn&amp;rsquo;t possible to resize persistent volumes in a statefulset. By setting it up this way it makes it possible to add more nodes with a new storage size then drain the old ones. It also solves the problem of allowing the user to determine which node groups to update first when doing upgrades or changes.&lt;/li&gt;
&lt;li&gt;We have designed this chart to be very un-opinionated about how to configure Elasticsearch. It exposes ways to set environment variables and mount secrets inside of the container. Doing this makes it much easier for this chart to support multiple versions with minimal changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installing&#34;&gt;Installing&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Add the elastic helm charts repo
&lt;code&gt;
helm repo add elastic https://helm.elastic.co
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Install it
&lt;code&gt;
helm install --name elasticsearch elastic/elasticsearch --version 6.5.3-alpha1
&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;clusterName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;This will be used as the Elasticsearch &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.name.html&#34;&gt;cluster.name&lt;/a&gt; and should be unique per cluster in the namespace&lt;/td&gt;
&lt;td&gt;&lt;code&gt;elasticsearch&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeGroup&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;This is the name that will be used for each group of nodes in the cluster. The name will be &lt;code&gt;clusterName-nodeGroup-X&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;masterService&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The service name used for &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#unicast&#34;&gt;discovery.zen.ping.unicast.hosts&lt;/a&gt; to connect to the masters&lt;/td&gt;
&lt;td&gt;&lt;code&gt;elasticsearch-master&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;roles&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A hash map with the &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html&#34;&gt;specific roles&lt;/a&gt; for the node group&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master: true&lt;/code&gt;&lt;br&gt;&lt;code&gt;data: true&lt;/code&gt;&lt;br&gt;&lt;code&gt;ingest: true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replicas&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes replica count for the statefulset (i.e. how many pods)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;minimumMasterNodes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The value for &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/discovery-settings.html#minimum_master_nodes&#34;&gt;discovery.zen.minimum_master_nodes&lt;/a&gt;. Should be set to &lt;code&gt;(replicas / 2) + 1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extraEnvs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Extra &lt;a href=&#34;https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/#using-environment-variables-inside-of-your-config&#34;&gt;environment variables&lt;/a&gt; which will be appended to the &lt;code&gt;env:&lt;/code&gt; definition for the container&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;secretMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Allows you easily mount a secret as a file inside the statefulset. Useful for mounting certificates and other secrets. See &lt;a href=&#34;./values.yaml&#34;&gt;values.yaml&lt;/a&gt; for an example&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The Elasticsearch docker image&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker.elastic.co/elasticsearch/elasticsearch&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;imageTag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The Elasticsearch docker image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;6.5.3&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;imagePullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/containers/images/#updating-images&#34;&gt;imagePullPolicy&lt;/a&gt; value&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;esJavaOpts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/jvm-options.html&#34;&gt;Java options&lt;/a&gt; for Elasticsearch. This is where you should configure the &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html&#34;&gt;jvm heap size&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;-Xmx1g -Xms1g&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Allows you to set the &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/&#34;&gt;resources&lt;/a&gt; for the statefulset&lt;/td&gt;
&lt;td&gt;&lt;code&gt;requests.cpu: 100m&lt;/code&gt;&lt;br&gt;&lt;code&gt;requests.memory: 2Gi&lt;/code&gt;&lt;br&gt;&lt;code&gt;limits.cpu: 1000m&lt;/code&gt;&lt;br&gt;&lt;code&gt;limits.memory: 2Gi&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;networkHost&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Value for the &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/network.host.html&#34;&gt;network.host Elasticsearch setting&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;volumeClaimTemplate&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configuration for the &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-storage&#34;&gt;volumeClaimTemplate for statefulsets&lt;/a&gt;. You will want to adjust the storage (default &lt;code&gt;30Gi&lt;/code&gt;) and the &lt;code&gt;storageClassName&lt;/code&gt; if you are using a different storage class&lt;/td&gt;
&lt;td&gt;&lt;code&gt;accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]&lt;/code&gt;&lt;br&gt;&lt;code&gt;storageClassName: standard&lt;/code&gt;&lt;br&gt;&lt;code&gt;resources.requests.storage: 30Gi&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;antiAffinityTopologyKey&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity&#34;&gt;anti-affinity topology key&lt;/a&gt;. By default this will prevent multiple Elasticsearch nodes from running on the same Kubernetes node&lt;/td&gt;
&lt;td&gt;&lt;code&gt;kubernetes.io/hostname&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;antiAffinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Setting this to hard enforces the &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity&#34;&gt;anti-affinity rules&lt;/a&gt;. If it is set to soft it will be done &amp;ldquo;best effort&amp;rdquo;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;hard&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podManagementPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;By default Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies&#34;&gt;deploys statefulsets serially&lt;/a&gt;. This deploys them in parralel so that they can discover eachother&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Parallel&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;protocol&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The protocol that will be used for the readinessProbe. Change this to &lt;code&gt;https&lt;/code&gt; if you have &lt;code&gt;xpack.security.http.ssl.enabled&lt;/code&gt; set&lt;/td&gt;
&lt;td&gt;&lt;code&gt;http&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;httpPort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The http port that Kubernetes will use for the healthchecks and the service. If you change this you will also need to set &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-http.html#_settings_2&#34;&gt;http.port&lt;/a&gt; in &lt;code&gt;extraEnvs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;9200&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;transportPort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The transport port that Kubernetes will use for the service. If you change this you will also need to set &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html#_tcp_transport&#34;&gt;transport port configuration&lt;/a&gt; in &lt;code&gt;extraEnvs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;9300&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;updateStrategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The &lt;a href=&#34;https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets&#34;&gt;updateStrategy&lt;/a&gt; for the statefulset. By default Kubernetes will wait for the cluster to be green after upgrading each pod. Setting this to &lt;code&gt;OnDelete&lt;/code&gt; will allow you to manually delete each pod during upgrades&lt;/td&gt;
&lt;td&gt;&lt;code&gt;RollingUpdate&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;maxUnavailable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The &lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget&#34;&gt;maxUnavailable&lt;/a&gt; value for the pod disruption budget. By default this will prevent Kubernetes from having more than 1 unhealthy pod in the node group&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;fsGroup&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The Group ID (GID) for &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/&#34;&gt;securityContext.fsGroup&lt;/a&gt; so that the Elasticsearch user can read from the persistent volume&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1000&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;terminationGracePeriod&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods&#34;&gt;terminationGracePeriod&lt;/a&gt; in seconds used when trying to stop the pod&lt;/td&gt;
&lt;td&gt;&lt;code&gt;120&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;sysctlVmMaxMapCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets the &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html#vm-max-map-count&#34;&gt;sysctl vm.max_map_count&lt;/a&gt; needed for Elasticsearch&lt;/td&gt;
&lt;td&gt;&lt;code&gt;262144&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;readinessProbe&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configuration for the &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/&#34;&gt;readinessProbe&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;failureThreshold: 3&lt;/code&gt;&lt;br&gt;&lt;code&gt;initialDelaySeconds: 10&lt;/code&gt;&lt;br&gt;&lt;code&gt;periodSeconds: 10&lt;/code&gt;&lt;br&gt;&lt;code&gt;successThreshold: 3&lt;/code&gt;&lt;br&gt;&lt;code&gt;timeoutSeconds: 5&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;imagePullSecrets&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configuration for &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-pod-that-uses-your-secret&#34;&gt;imagePullSecrets&lt;/a&gt; so that you can use a private registry for your image&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configurable &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector&#34;&gt;nodeSelector&lt;/a&gt; so that you can target specific nodes for your Elasticsearch cluster&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configurable &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/&#34;&gt;tolerations&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configurable &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34;&gt;ingress&lt;/a&gt; to expose the Elasticsearch service. See &lt;a href=&#34;./values.yaml&#34;&gt;&lt;code&gt;values.yaml&lt;/code&gt;&lt;/a&gt; for an example&lt;/td&gt;
&lt;td&gt;&lt;code&gt;enabled: false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;try-it-out&#34;&gt;Try it out&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&#34;./examples&#34;&gt;examples/&lt;/a&gt; you will find some example configurations. These examples are used for the automated testing of this helm chart&lt;/p&gt;

&lt;h3 id=&#34;default&#34;&gt;Default&lt;/h3&gt;

&lt;p&gt;To deploy a cluster with all default values and run the integration tests&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd examples/default
make
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;multi&#34;&gt;Multi&lt;/h3&gt;

&lt;p&gt;A cluster with dedicated node types&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd examples/multi
make
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;security&#34;&gt;Security&lt;/h3&gt;

&lt;p&gt;A cluster with X-Pack security enabled&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Generate SSL certificates following the &lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/6.3/configuring-tls.html#node-certificates&#34;&gt;official docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Make sure you have a copy of your &lt;a href=&#34;https://www.elastic.co/subscriptions&#34;&gt;license&lt;/a&gt; handy.&lt;/li&gt;
&lt;li&gt;Create Kubernetes secrets for authentication credentials, X-Pack license and certificates
&lt;code&gt;
kubectl create secret generic elastic-credentials  --from-literal=password=changeme --from-literal=username=elastic
kubectl create secret generic elastic-license --from-file=license.json
kubectl create secret generic elastic-certificates --from-file=elastic-certificates.p12
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Deploy!
&lt;code&gt;
cd examples/security
make
&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Attach into one of the containers&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl exec -ti $(kubectl get pods -l release=helm-es-security -o name | awk -F&#39;/&#39; &#39;{ print $NF }&#39; | head -n 1) bash
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Install the X-Pack license&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -XPUT &#39;http://localhost:9200/_xpack/license&#39; -H &amp;quot;Content-Type: application/json&amp;quot; -d @/usr/share/elasticsearch/config/license/license.json
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Test that authentication is now enabled&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl &#39;http://localhost:9200/&#39; # This one will fail
curl -u elastic:changeme &#39;http://localhost:9200/&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Install some test data to play around with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://download.elastic.co/demos/kibana/gettingstarted/logs.jsonl.gz &amp;amp;&amp;amp; gunzip logs.jsonl.gz &amp;amp;&amp;amp; curl -u elastic:changeme -H &#39;Content-Type: application/x-ndjson&#39; -XPOST &#39;localhost:9200/_bulk?pretty&#39; --data-binary @logs.jsonl
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;minikube&#34;&gt;Minikube&lt;/h3&gt;

&lt;p&gt;This chart also works successfully on minikube in addition to typical hosted Kubernetes environments.
An example &lt;code&gt;values.yaml&lt;/code&gt; file for minikube is provided under &lt;code&gt;examples/&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In order to properly support the required persistent volume claims for the Elasticsearch &lt;code&gt;StatefulSet&lt;/code&gt;, the &lt;code&gt;default-storageclass&lt;/code&gt; and &lt;code&gt;storage-provisioner&lt;/code&gt; minikube addons must be enabled.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minikube addons enable default-storageclass
minikube addons enable storage-provisioner
cd examples/minikube
make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that if &lt;code&gt;helm&lt;/code&gt; or &lt;code&gt;kubectl&lt;/code&gt; timeouts occur, you may consider creating a minikube VM with more CPU cores or memory allocated.&lt;/p&gt;

&lt;h2 id=&#34;clustering-and-node-discovery&#34;&gt;Clustering and Node Discovery&lt;/h2&gt;

&lt;p&gt;This chart facilitates Elasticsearch node discovery and services by creating two &lt;code&gt;Service&lt;/code&gt; definitions in Kubernetes, one with the name &lt;code&gt;$clusterName-$nodeGroup&lt;/code&gt; and another named &lt;code&gt;$clusterName-$nodeGroup-headless&lt;/code&gt;.
Only &lt;code&gt;Ready&lt;/code&gt; pods are a part of the &lt;code&gt;$clusterName-$nodeGroup&lt;/code&gt; service, while all pods (&lt;code&gt;Ready&lt;/code&gt; or not) are a part of &lt;code&gt;$clusterName-$nodeGroup-headless&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The chart value for &lt;code&gt;masterService&lt;/code&gt; is used to populate &lt;code&gt;discovery.zen.ping.unicast.hosts&lt;/code&gt;, which Elasticsearch nodes will use to contact master nodes and form a cluster.
Therefore, to add a group of nodes to an existing cluster, setting &lt;code&gt;masterService&lt;/code&gt; to the desired &lt;code&gt;Service&lt;/code&gt; name of the related cluster is sufficient.&lt;/p&gt;

&lt;p&gt;For an example of deploying both a group master nodes and data nodes using multiple releases of this chart, see the accompanying values files in &lt;code&gt;examples/multi&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;testing&#34;&gt;Testing&lt;/h2&gt;

&lt;p&gt;This chart uses &lt;a href=&#34;https://docs.pytest.org/en/latest/&#34;&gt;pytest&lt;/a&gt; to test the templating logic. The dependencies for testing can be installed from the &lt;a href=&#34;../requirements.txt&#34;&gt;&lt;code&gt;requirements.txt&lt;/code&gt;&lt;/a&gt; in the parent directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install -r ../requirements.txt
make test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also use &lt;code&gt;helm template&lt;/code&gt; to look at the YAML being generated&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make template
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is possible to run all of the tests and linting inside of a docker container&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;make test
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/grafana/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/grafana/README/</guid>
      <description>

&lt;h1 id=&#34;grafana-helm-chart&#34;&gt;Grafana Helm Chart&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Installs the web dashboarding system &lt;a href=&#34;http://grafana.org/&#34;&gt;Grafana&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tl-dr&#34;&gt;TL;DR;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/grafana
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install --name my-release stable/grafana
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the my-release deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes all the Kubernetes components associated with the chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replicas&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Number of nodes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;deploymentStrategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deployment strategy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;RollingUpdate&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;securityContext&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deployment securityContext&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{&amp;quot;runAsUser&amp;quot;: 472, &amp;quot;fsGroup&amp;quot;: 472}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image repository&lt;/td&gt;
&lt;td&gt;&lt;code&gt;grafana/grafana&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image tag. (&lt;code&gt;Must be &amp;gt;= 5.0.0&lt;/code&gt;)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5.2.3&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes service type&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes port where service is exposed&lt;/td&gt;
&lt;td&gt;&lt;code&gt;9000&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Service annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;80&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.labels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Custom labels&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enables Ingress&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.labels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Custom labels&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.hosts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress accepted hostnames&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.tls&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress TLS configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;CPU/Memory resource requests/limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Node labels for pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Toleration labels for pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;affinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Affinity settings for pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Use persistent volume to store data&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Size of persistent volume claim&lt;/td&gt;
&lt;td&gt;&lt;code&gt;10Gi&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.existingClaim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Use an existing PVC to persist data&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.storageClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Type of persistent volume claim&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.accessModes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Persistence access modes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.subPath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Mount a sub dir of the persistent volume&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;schedulerName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Alternate scheduler name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;env&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Extra environment variables passed to pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;envFromSecret&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of a Kubenretes secret (must be manually created in the same namespace) containing values to be added to the environment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extraSecretMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional grafana server secret mounts&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;datasources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configure grafana datasources&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;dashboardProviders&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Configure grafana dashboard providers&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;dashboards&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Dashboards to import&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;dashboardsConfigMaps&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ConfigMaps reference that contains dashboards&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;grafana.ini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Grafana&amp;rsquo;s primary configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ldap.existingSecret&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The name of an existing secret containing the &lt;code&gt;ldap.toml&lt;/code&gt; file, this must have the key &lt;code&gt;ldap-toml&lt;/code&gt;.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ldap.config&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Grafana&amp;rsquo;s LDAP configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deployment annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;sidecar.dashboards.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enabled the cluster wide search for dashboards and adds/updates/deletes them in grafana&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;sidecar.dashboards.label&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Label that config maps with dashboards should have to be added&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;sidecar.datasources.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enabled the cluster wide search for datasources and adds/updates/deletes them in grafana&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;sidecar.datasources.label&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Label that config maps with datasources should have to be added&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;smtp.existingSecret&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The name of an existing secret containing the SMTP credentials, this must have the keys &lt;code&gt;user&lt;/code&gt; and &lt;code&gt;password&lt;/code&gt;.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;sidecar-for-dashboards&#34;&gt;Sidecar for dashboards&lt;/h2&gt;

&lt;p&gt;If the parameter &lt;code&gt;sidecar.dashboards.enabled&lt;/code&gt; is set, a sidecar container is deployed in the grafana pod. This container watches all config maps in the cluster and filters out the ones with a label as defined in &lt;code&gt;sidecar.dashboards.label&lt;/code&gt;. The files defined in those configmaps are written to a folder and accessed by grafana. Changes to the configmaps are monitored and the imported dashboards are deleted/updated. A recommendation is to use one configmap per dashboard, as an reduction of multiple dashboards inside one configmap is currently not properly mirrored in grafana.
Example dashboard config:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: sample-grafana-dashboard
  labels:
     grafana_dashboard: 1
data:
  k8s-dashboard.json: |-
  [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sidecar-for-datasources&#34;&gt;Sidecar for datasources&lt;/h2&gt;

&lt;p&gt;If the parameter &lt;code&gt;sidecar.datasource.enabled&lt;/code&gt; is set, a sidecar container is deployed in the grafana pod. This container watches all config maps in the cluster and filters out the ones with a label as defined in &lt;code&gt;sidecar.datasources.label&lt;/code&gt;. The files defined in those configmaps are written to a folder and accessed by grafana on startup. Using these yaml files, the data sources in grafana can be modified.&lt;/p&gt;

&lt;p&gt;Example datasource config adapted from &lt;a href=&#34;http://docs.grafana.org/administration/provisioning/#example-datasource-config-file&#34;&gt;Grafana&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: sample-grafana-datasource
  labels:
     grafana_datasource: 1
data:
    datasource.yaml: |-
        # config file version
        apiVersion: 1

        # list of datasources that should be deleted from the database
        deleteDatasources:
          - name: Graphite
            orgId: 1

        # list of datasources to insert/update depending
        # whats available in the database
        datasources:
          # &amp;lt;string, required&amp;gt; name of the datasource. Required
        - name: Graphite
          # &amp;lt;string, required&amp;gt; datasource type. Required
          type: graphite
          # &amp;lt;string, required&amp;gt; access mode. proxy or direct (Server or Browser in the UI). Required
          access: proxy
          # &amp;lt;int&amp;gt; org id. will default to orgId 1 if not specified
          orgId: 1
          # &amp;lt;string&amp;gt; url
          url: http://localhost:8080
          # &amp;lt;string&amp;gt; database password, if used
          password:
          # &amp;lt;string&amp;gt; database user, if used
          user:
          # &amp;lt;string&amp;gt; database name, if used
          database:
          # &amp;lt;bool&amp;gt; enable/disable basic auth
          basicAuth:
          # &amp;lt;string&amp;gt; basic auth username
          basicAuthUser:
          # &amp;lt;string&amp;gt; basic auth password
          basicAuthPassword:
          # &amp;lt;bool&amp;gt; enable/disable with credentials headers
          withCredentials:
          # &amp;lt;bool&amp;gt; mark as default datasource. Max one per org
          isDefault:
          # &amp;lt;map&amp;gt; fields that will be converted to json and stored in json_data
          jsonData:
             graphiteVersion: &amp;quot;1.1&amp;quot;
             tlsAuth: true
             tlsAuthWithCACert: true
          # &amp;lt;string&amp;gt; json object of data that will be encrypted.
          secureJsonData:
            tlsCACert: &amp;quot;...&amp;quot;
            tlsClientCert: &amp;quot;...&amp;quot;
            tlsClientKey: &amp;quot;...&amp;quot;
          version: 1
          # &amp;lt;bool&amp;gt; allow users to edit datasources from the UI.
          editable: false

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/heapster/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/heapster/README/</guid>
      <description>

&lt;h1 id=&#34;heapster&#34;&gt;Heapster&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/heapster&#34;&gt;Heapster&lt;/a&gt; enables Container Cluster Monitoring and Performance Analysis. It collects and interprets various signals like compute resource usage, lifecycle events, etc, and exports cluster metrics via REST endpoints.
The Chart can also enable eventer, which can send the kubernetes event logs to a remote location.&lt;/p&gt;

&lt;h2 id=&#34;quickstart&#34;&gt;QuickStart&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install stable/heapster
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release stable/heapster
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the &lt;code&gt;my-release&lt;/code&gt; deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm delete my-release --purge
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes all the Kubernetes components associated with the chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The default configuration values for this chart are listed in &lt;code&gt;values.yaml&lt;/code&gt;.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Repository for container image&lt;/td&gt;
&lt;td&gt;k8s.gcr.io/heapster&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container image tag&lt;/td&gt;
&lt;td&gt;v1.3.0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image pull policy&lt;/td&gt;
&lt;td&gt;IfNotPresent&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Service port name&lt;/td&gt;
&lt;td&gt;api&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Type for the service&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.externalPort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Service external port&lt;/td&gt;
&lt;td&gt;8082&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.internalPort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Service internal port&lt;/td&gt;
&lt;td&gt;8082&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Service annotations, specified as a map&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resources.limits&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Server resource  limits&lt;/td&gt;
&lt;td&gt;limits: {cpu: 100m, memory: 128Mi}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resources.requests&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Server resource requests&lt;/td&gt;
&lt;td&gt;requests: {cpu: 100m, memory: 128Mi}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;command&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Commands for heapster pod&lt;/td&gt;
&lt;td&gt;&amp;rdquo;/heapster &amp;ndash;source=kubernetes.summary_api:&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;rbac.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Bind system:heapster role&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;rbac.serviceAccountName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;existing ServiceAccount to use (ignored if rbac.create=true)&lt;/td&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resizer.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If enabled, scale resources&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;eventer.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If enabled, start eventer&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod Annotations to be added to the heapster Pod&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Node labels for pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The table below is only applicable if &lt;code&gt;resizer.enabled&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;. More information on resizer can be found &lt;a href=&#34;https://github.com/kubernetes/contrib/blob/master/addon-resizer/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resizer.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Repository for container image&lt;/td&gt;
&lt;td&gt;k8s.gcr.io/addon-resizer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resizer.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container image tag&lt;/td&gt;
&lt;td&gt;1.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resizer.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image pull policy&lt;/td&gt;
&lt;td&gt;IfNotPresent&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resizer.resources.limits&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Server resource  limits&lt;/td&gt;
&lt;td&gt;limits: {cpu: 50m, memory: 90Mi}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resizer.resources.requests&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Server resource requests&lt;/td&gt;
&lt;td&gt;requests: {cpu: 50m, memory: 90Mi}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resizer.flags&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Flags for pod nanny command&lt;/td&gt;
&lt;td&gt;Defaults set in values.yaml&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The table below is only applicable if &lt;code&gt;eventer.enabled&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;. More information on eventer can be found
[here]&lt;a href=&#34;https://github.com/kubernetes/heapster/blob/master/docs/overview.md&#34;&gt;https://github.com/kubernetes/heapster/blob/master/docs/overview.md&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;eventer.flags&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Flags for eventer command&lt;/td&gt;
&lt;td&gt;Defaults set in values.yaml&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;eventer.resources.limits&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Server resource  limits&lt;/td&gt;
&lt;td&gt;requests: {}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;eventer.resources.requests&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Server resource requests&lt;/td&gt;
&lt;td&gt;requests: {}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;eventer.resizer.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If enabled, scale resources&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;eventer.resizer.flags&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Flags for pod nanny command for eventer&lt;/td&gt;
&lt;td&gt;Defaults set in values.yaml&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;eventer.resizer.resources.limits&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Server resource limits&lt;/td&gt;
&lt;td&gt;requests: {}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;eventer.resizer.resources.requests&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Server resource requests&lt;/td&gt;
&lt;td&gt;requests: {}&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/kubernetes-dashboard/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/kubernetes-dashboard/README/</guid>
      <description>

&lt;h1 id=&#34;kubernetes-dashboard&#34;&gt;kubernetes-dashboard&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dashboard&#34;&gt;Kubernetes Dashboard&lt;/a&gt; is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.&lt;/p&gt;

&lt;h2 id=&#34;tl-dr&#34;&gt;TL;DR;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This chart bootstraps a &lt;a href=&#34;https://github.com/kubernetes/dashboard&#34;&gt;Kubernetes Dashboard&lt;/a&gt; deployment on a &lt;a href=&#34;http://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt; cluster using the &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; package manager.&lt;/p&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/kubernetes-dashboard --name my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command deploys kubernetes-dashboard on the Kubernetes cluster in the default configuration. The &lt;a href=&#34;#configuration&#34;&gt;configuration&lt;/a&gt; section lists the parameters that can be configured during installation.&lt;/p&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the &lt;code&gt;my-release&lt;/code&gt; deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes all the Kubernetes components associated with the chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;access-control&#34;&gt;Access control&lt;/h2&gt;

&lt;p&gt;It is critical for the Kubernetes cluster to correctly setup access control of Kubernetes Dashboard. See this &lt;a href=&#34;https://github.com/kubernetes/dashboard/wiki/Access-control&#34;&gt;guide&lt;/a&gt; for best practises.&lt;/p&gt;

&lt;p&gt;It is highly recommended to use RBAC with minimal privileges needed for Dashboard to run.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The following table lists the configurable parameters of the kubernetes-dashboard chart and their default values.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Repository for container image&lt;/td&gt;
&lt;td&gt;&lt;code&gt;k8s.gcr.io/kubernetes-dashboard-amd64&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.10.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replicaCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Number of replicas&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extraArgs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional container arguments&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node labels for pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;List of node taints to tolerate (requires Kubernetes &amp;gt;= 1.6)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.externalPort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Dashboard external port&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.internalPort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Dashboard internal port&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specify ingress class&lt;/td&gt;
&lt;td&gt;&lt;code&gt;kubernetes.io/ingress.class: nginx&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enable ingress controller resource&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Path to match against incoming requests. Must begin with a &amp;lsquo;/&amp;rsquo;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.hosts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Dashboard Hostnames&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.tls&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress TLS configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod resource requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;limits: {cpu: 100m, memory: 100Mi}, requests: {cpu: 100m, memory: 100Mi}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;rbac.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create &amp;amp; use RBAC resources&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;rbac.clusterAdminRole&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&amp;ldquo;cluster-admin&amp;rdquo; ClusterRole will be used for dashboard ServiceAccount (&lt;a href=&#34;#access-control&#34;&gt;NOT RECOMMENDED&lt;/a&gt;)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccount.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Whether a new service account name that the agent will use should be created.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccount.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Service account to be used. If not set and serviceAccount.create is &lt;code&gt;true&lt;/code&gt; a name is generated using the fullname template.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Specify each parameter using the &lt;code&gt;--set key=value[,key=value]&lt;/code&gt; argument to &lt;code&gt;helm install&lt;/code&gt;. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/kubernetes-dashboard --name my-release \
  --set=service.externalPort=8080,resources.limits.cpu=200m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, a YAML file that specifies the values for the above parameters can be provided while installing the chart. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/kubernetes-dashboard --name my-release -f values.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: You can use the default &lt;a href=&#34;values.yaml&#34;&gt;values.yaml&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;using-the-dashboard-with-kubectl-proxy&#34;&gt;Using the dashboard with &amp;lsquo;kubectl proxy&amp;rsquo;&lt;/h2&gt;

&lt;p&gt;When running &amp;lsquo;kubectl proxy&amp;rsquo;, the address &lt;code&gt;localhost:8001/ui&lt;/code&gt; automatically expands to &lt;code&gt;http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/&lt;/code&gt;. For this to reach the dashboard, the name of the service must be &amp;lsquo;kubernetes-dashboard&amp;rsquo;, not any other value as set by Helm. You can manually specify this using the value &amp;lsquo;fullnameOverride&amp;rsquo;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fullnameOverride: &#39;kubernetes-dashboard&#39;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/logstash/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/logstash/README/</guid>
      <description>

&lt;h1 id=&#34;logstash&#34;&gt;Logstash&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/products/logstash&#34;&gt;Logstash&lt;/a&gt; is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite “stash.”&lt;/p&gt;

&lt;h2 id=&#34;tl-dr&#34;&gt;TL;DR;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/logstash
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install --name my-release stable/logstash
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the &lt;code&gt;my-release&lt;/code&gt; deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes nearly all the Kubernetes components associated with the
chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;best-practices&#34;&gt;Best Practices&lt;/h2&gt;

&lt;h3 id=&#34;release-and-tune-this-chart-once-per-logstash-pipeline&#34;&gt;Release and tune this chart once per Logstash pipeline&lt;/h3&gt;

&lt;p&gt;To achieve multiple pipelines with this chart, current best practice is to
maintain one pipeline per chart release. In this way configuration is
simplified and pipelines are more isolated from one another.&lt;/p&gt;

&lt;h3 id=&#34;default-pipeline-beats-input-elasticsearch-output&#34;&gt;Default Pipeline: Beats Input -&amp;gt; Elasticsearch Output&lt;/h3&gt;

&lt;p&gt;Current best practice for ELK logging is to ship logs from hosts using Filebeat
to logstash where persistent queues are enabled. Filebeat supports structured
(e.g. JSON) and unstructured (e.g. log lines) log shipment.&lt;/p&gt;

&lt;h3 id=&#34;load-beats-generated-index-template-into-elasticsearch&#34;&gt;Load Beats-generated index template into Elasticsearch&lt;/h3&gt;

&lt;p&gt;To best utilize the combination of Beats, Logstash and Elasticsearch,
load Beats-generated index templates into Elasticsearch as described &lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-template.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On a remote-to-Kubernetes Linux instance you might run the following command to
load that instance&amp;rsquo;s Beats-generated index template into Elasticsearch
(Elasticsearch hostname will vary).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filebeat setup --template -E output.logstash.enabled=false \
  -E &#39;output.elasticsearch.hosts=[&amp;quot;elasticsearch.cluster.local:9200&amp;quot;]&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;links&#34;&gt;Links&lt;/h3&gt;

&lt;p&gt;Please review the following links that expound on current best practices.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/blog/structured-logging-filebeat&#34;&gt;https://www.elastic.co/blog/structured-logging-filebeat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-template.html&#34;&gt;https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-template.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/deploying-and-scaling.html&#34;&gt;https://www.elastic.co/guide/en/logstash/current/deploying-and-scaling.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/persistent-queues.html&#34;&gt;https://www.elastic.co/guide/en/logstash/current/persistent-queues.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The following table lists the configurable parameters of the chart and its default values.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replicaCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Number of replicas&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podDisruptionBudget&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod disruption budget&lt;/td&gt;
&lt;td&gt;&lt;code&gt;maxUnavailable: 1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;updateStrategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Update strategy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;type: RollingUpdate&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container image name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker.elastic.co/logstash/logstash-oss&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;6.5.3&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Service type (ClusterIP, NodePort or LoadBalancer)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Service annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.ports&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ports exposed by service&lt;/td&gt;
&lt;td&gt;beats&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The load balancer IP for the service&lt;/td&gt;
&lt;td&gt;unset&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The cluster IP for the service&lt;/td&gt;
&lt;td&gt;unset&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.nodePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The nodePort for the service&lt;/td&gt;
&lt;td&gt;unset&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;service.externalTrafficPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Set externalTrafficPolicy&lt;/td&gt;
&lt;td&gt;unset&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ports&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ports exposed by logstash container&lt;/td&gt;
&lt;td&gt;beats&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enables Ingress&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.hosts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress accepted hostnames&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[&amp;quot;logstash.cluster.local&amp;quot;]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ingress.tls&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ingress TLS configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod resource requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;priorityClassName&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Node selector&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Tolerations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;affinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Affinity or Anti-Affinity&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podLabels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Pod labels&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;livenessProbe&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Liveness probe settings for logstash container&lt;/td&gt;
&lt;td&gt;(see &lt;code&gt;values.yaml&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;readinessProbe&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Readiness probe settings for logstash container&lt;/td&gt;
&lt;td&gt;(see &lt;code&gt;values.yaml&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enable persistence&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.storageClass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Storage class for PVCs&lt;/td&gt;
&lt;td&gt;unset&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.accessMode&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Access mode for PVCs&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ReadWriteOnce&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Size for PVCs&lt;/td&gt;
&lt;td&gt;&lt;code&gt;2Gi&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;volumeMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Volume mounts to configure for logstash container&lt;/td&gt;
&lt;td&gt;(see &lt;code&gt;values.yaml&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;volumes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Volumes to configure for logstash container&lt;/td&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Duration the pod needs to terminate gracefully&lt;/td&gt;
&lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exporter.logstash&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus logstash-exporter settings&lt;/td&gt;
&lt;td&gt;(see &lt;code&gt;values.yaml&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;exporter.logstash.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enables Prometheus logstash-exporter&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;elasticsearch.host&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ElasticSearch hostname&lt;/td&gt;
&lt;td&gt;&lt;code&gt;elasticsearch-client.default.svc.cluster.local&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;elasticsearch.port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ElasticSearch port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;9200&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;config&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Logstash configuration key-values&lt;/td&gt;
&lt;td&gt;(see &lt;code&gt;values.yaml&lt;/code&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;patterns&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Logstash patterns configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;inputs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Logstash inputs configuration&lt;/td&gt;
&lt;td&gt;beats&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;filters&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Logstash filters configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;outputs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Logstash outputs configuration&lt;/td&gt;
&lt;td&gt;elasticsearch&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/mysql/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/mysql/README/</guid>
      <description>

&lt;h1 id=&#34;mysql&#34;&gt;MySQL&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://MySQL.org&#34;&gt;MySQL&lt;/a&gt; is one of the most popular database servers in the world. Notable users include Wikipedia, Facebook and Google.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This chart bootstraps a single node MySQL deployment on a &lt;a href=&#34;http://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt; cluster using the &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; package manager.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 1.6+ with Beta APIs enabled&lt;/li&gt;
&lt;li&gt;PV provisioner support in the underlying infrastructure&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release stable/mysql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command deploys MySQL on the Kubernetes cluster in the default configuration. The &lt;a href=&#34;#configuration&#34;&gt;configuration&lt;/a&gt; section lists the parameters that can be configured during installation.&lt;/p&gt;

&lt;p&gt;By default a random password will be generated for the root user. If you&amp;rsquo;d like to set your own password change the mysqlRootPassword
in the values.yaml.&lt;/p&gt;

&lt;p&gt;You can retrieve your root password by running the following command. Make sure to replace [YOUR_RELEASE_NAME]:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;printf $(printf &#39;\%o&#39; `kubectl get secret [YOUR_RELEASE_NAME]-mysql -o jsonpath=&amp;quot;{.data.mysql-root-password[*]}&amp;quot;`)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: List all releases using &lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the &lt;code&gt;my-release&lt;/code&gt; deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes all the Kubernetes components associated with the chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The following table lists the configurable parameters of the MySQL chart and their default values.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;mysql&lt;/code&gt; image repository.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;mysql&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;imageTag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;mysql&lt;/code&gt; image tag.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5.7.14&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;imagePullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;existingSecret&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Use Existing secret for Password details&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extraVolumes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional volumes as a string to be passed to the &lt;code&gt;tpl&lt;/code&gt; function&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extraVolumeMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional volumeMounts as a string to be passed to the &lt;code&gt;tpl&lt;/code&gt; function&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;extraInitContainers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional init containers as a string to be passed to the &lt;code&gt;tpl&lt;/code&gt; function&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;mysqlRootPassword&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Password for the &lt;code&gt;root&lt;/code&gt; user. Ignored if existing secret is provided&lt;/td&gt;
&lt;td&gt;Random 10 characters&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;mysqlUser&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Username of new user to create.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;mysqlPassword&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Password for the new user. Ignored if existing secret is provided&lt;/td&gt;
&lt;td&gt;Random 10 characters&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;mysqlDatabase&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name for new database to create.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;livenessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delay before liveness probe is initiated&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;livenessProbe.periodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How often to perform the probe&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;livenessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;When the probe times out&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;livenessProbe.successThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive successes for the probe to be considered successful after having failed.&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;livenessProbe.failureThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive failures for the probe to be considered failed after having succeeded.&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;readinessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delay before readiness probe is initiated&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;readinessProbe.periodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How often to perform the probe&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;readinessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;When the probe times out&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;readinessProbe.successThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive successes for the probe to be considered successful after having failed.&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;readinessProbe.failureThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive failures for the probe to be considered failed after having succeeded.&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create a volume to store data&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Size of persistent volume claim&lt;/td&gt;
&lt;td&gt;8Gi RW&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.storageClass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Type of persistent volume claim&lt;/td&gt;
&lt;td&gt;nil  (uses alpha storage class annotation)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.accessMode&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;ReadWriteOnce or ReadOnly&lt;/td&gt;
&lt;td&gt;ReadWriteOnce&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.existingClaim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of existing persistent volume&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.subPath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Subdirectory of the volume to mount&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Persistent Volume annotations&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Node labels for pod assignment&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Start a side-car prometheus exporter&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.image&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Exporter image&lt;/td&gt;
&lt;td&gt;&lt;code&gt;prom/mysqld-exporter&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.imageTag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Exporter image&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v0.10.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.imagePullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Exporter image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Exporter resource requests/limit&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.livenessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delay before metrics liveness probe is initiated&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.livenessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;When the probe times out&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.readinessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delay before metrics readiness probe is initiated&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.readinessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;When the probe times out&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;CPU/Memory resource requests/limits&lt;/td&gt;
&lt;td&gt;Memory: &lt;code&gt;256Mi&lt;/code&gt;, CPU: &lt;code&gt;100m&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;configurationFiles&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;List of mysql configuration files&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ssl.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Setup and use SSL for MySQL connections&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ssl.secret&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the secret containing the SSL certificates&lt;/td&gt;
&lt;td&gt;mysql-ssl-certs&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ssl.certificates[0].name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the secret containing the SSL certificates&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ssl.certificates[0].ca&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;CA certificate&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ssl.certificates[0].cert&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Server certificate (public key)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ssl.certificates[0].key&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Server key (private key)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;imagePullSecrets&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of Secret resource containing private registry credentials&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;initializationFiles&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;List of SQL files which are run after the container started&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;timezone&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Container and mysqld timezone (TZ env)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt; (UTC depending on image)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Map of annotations to add to the pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Some of the parameters above map to the env variables defined in the &lt;a href=&#34;https://hub.docker.com/_/mysql/&#34;&gt;MySQL DockerHub image&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Specify each parameter using the &lt;code&gt;--set key=value[,key=value]&lt;/code&gt; argument to &lt;code&gt;helm install&lt;/code&gt;. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release \
  --set mysqlRootPassword=secretpassword,mysqlUser=my-user,mysqlPassword=my-password,mysqlDatabase=my-database \
    stable/mysql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above command sets the MySQL &lt;code&gt;root&lt;/code&gt; account password to &lt;code&gt;secretpassword&lt;/code&gt;. Additionally it creates a standard database user named &lt;code&gt;my-user&lt;/code&gt;, with the password &lt;code&gt;my-password&lt;/code&gt;, who has access to a database named &lt;code&gt;my-database&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release -f values.yaml stable/mysql
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: You can use the default &lt;a href=&#34;values.yaml&#34;&gt;values.yaml&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;persistence&#34;&gt;Persistence&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://hub.docker.com/_/mysql/&#34;&gt;MySQL&lt;/a&gt; image stores the MySQL data and configurations at the &lt;code&gt;/var/lib/mysql&lt;/code&gt; path of the container.&lt;/p&gt;

&lt;p&gt;By default a PersistentVolumeClaim is created and mounted into that directory. In order to disable this functionality
you can change the values.yaml to disable persistence and use an emptyDir instead.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;custom-mysql-configuration-files&#34;&gt;Custom MySQL configuration files&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://hub.docker.com/_/mysql/&#34;&gt;MySQL&lt;/a&gt; image accepts custom configuration files at the path &lt;code&gt;/etc/mysql/conf.d&lt;/code&gt;. If you want to use a customized MySQL configuration, you can create your alternative configuration files by passing the file contents on the &lt;code&gt;configurationFiles&lt;/code&gt; attribute. Note that according to the MySQL documentation only files ending with &lt;code&gt;.cnf&lt;/code&gt; are loaded.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;configurationFiles:
  mysql.cnf: |-
    [mysqld]
    skip-host-cache
    skip-name-resolve
    sql-mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION
  mysql_custom.cnf: |-
    [mysqld]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mysql-initialization-files&#34;&gt;MySQL initialization files&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://hub.docker.com/_/mysql/&#34;&gt;MySQL&lt;/a&gt; image accepts *.sh, *.sql and *.sql.gz files at the path &lt;code&gt;/docker-entrypoint-initdb.d&lt;/code&gt;.
These files are being run exactly once for container initialization and ignored on following container restarts.
If you want to use initialization scripts, you can create initialization files by passing the file contents on the &lt;code&gt;initializationFiles&lt;/code&gt; attribute.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;initializationFiles:
  first-db.sql: |-
    CREATE DATABASE IF NOT EXISTS first DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
  second-db.sql: |-
    CREATE DATABASE IF NOT EXISTS second DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ssl&#34;&gt;SSL&lt;/h2&gt;

&lt;p&gt;This chart supports configuring MySQL to use &lt;a href=&#34;https://dev.mysql.com/doc/refman/5.7/en/encrypted-connections.html&#34;&gt;encrypted connections&lt;/a&gt; with TLS/SSL certificates provided by the user. This is accomplished by storing the required Certificate Authority file, the server public key certificate, and the server private key as a Kubernetes secret. The SSL options for this chart support the following use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Manage certificate secrets with helm&lt;/li&gt;
&lt;li&gt;Manage certificate secrets outside of helm&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;manage-certificate-secrets-with-helm&#34;&gt;Manage certificate secrets with helm&lt;/h2&gt;

&lt;p&gt;Include your certificate data in the &lt;code&gt;ssl.certificates&lt;/code&gt; section. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssl:
  enabled: false
  secret: mysql-ssl-certs
  certificates:
  - name: mysql-ssl-certs
    ca: |-
      -----BEGIN CERTIFICATE-----
      ...
      -----END CERTIFICATE-----
    cert: |-
      -----BEGIN CERTIFICATE-----
      ...
      -----END CERTIFICATE-----
    key: |-
      -----BEGIN RSA PRIVATE KEY-----
      ...
      -----END RSA PRIVATE KEY-----
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Make sure your certificate data has the correct formatting in the values file.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;manage-certificate-secrets-outside-of-helm&#34;&gt;Manage certificate secrets outside of helm&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Ensure the certificate secret exist before installation of this chart.&lt;/li&gt;
&lt;li&gt;Set the name of the certificate secret in &lt;code&gt;ssl.secret&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Make sure there are no entries underneath &lt;code&gt;ssl.certificates&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To manually create the certificate secret from local files you can execute:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create secret generic mysql-ssl-certs \
  --from-file=ca.pem=./ssl/certificate-authority.pem \
  --from-file=server-cert.pem=./ssl/server-public-key.pem \
  --from-file=server-key.pem=./ssl/server-private-key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;code&gt;ca.pem&lt;/code&gt;, &lt;code&gt;server-cert.pem&lt;/code&gt;, and &lt;code&gt;server-key.pem&lt;/code&gt; &lt;strong&gt;must&lt;/strong&gt; be used as the key names in this generic secret.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you are using a certificate your configurationFiles must include the three ssl lines under [mysqld]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[mysqld]
    ssl-ca=/ssl/ca.pem
    ssl-cert=/ssl/server-cert.pem
    ssl-key=/ssl/server-key.pem
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/nfs-client-provisioner/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/nfs-client-provisioner/README/</guid>
      <description>

&lt;h1 id=&#34;nfs-client-provisioner&#34;&gt;nfs-client-provisioner&lt;/h1&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client&#34;&gt;NFS client provisioner&lt;/a&gt; is an automatic provisioner for Kubernetes that uses your &lt;em&gt;already configured&lt;/em&gt; NFS server, automatically creating Persistent Volumes.&lt;/p&gt;

&lt;h2 id=&#34;tl-dr&#34;&gt;TL;DR;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/nfs-client-provisioner
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This charts installs custom &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/&#34;&gt;storage class&lt;/a&gt; into a &lt;a href=&#34;http://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt; cluster using the &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; package manager. It also installs a &lt;a href=&#34;https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client&#34;&gt;NFS client provisioner&lt;/a&gt; into the cluster which dynamically creates persistent volumes from single NFS share.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 1.9+&lt;/li&gt;
&lt;li&gt;Existing NFS Share&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install --name my-release --set nfs.server=x.x.x.x --set nfs.path=/exported/path stable/nfs-client-provisioner
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command deploys the given storage class in the default configuration. It can be used afterswards to provision persistent volumes. The &lt;a href=&#34;#configuration&#34;&gt;configuration&lt;/a&gt; section lists the parameters that can be configured during installation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: List all releases using &lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the &lt;code&gt;my-release&lt;/code&gt; deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes all the Kubernetes components associated with the chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The following tables lists the configurable parameters of this chart and their default values.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;replicaCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Number of provisioner instances to deployed&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;strategyType&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specifies the strategy used to replace old Pods by new ones&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Recreate&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Provisioner image&lt;/td&gt;
&lt;td&gt;&lt;code&gt;quay.io/external_storage/nfs-client-provisioner&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Version of provisioner image&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v3.0.2-k8s1.11&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;storageclass.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the storageclass&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nfs-client&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;storageclass.defaultClass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Set as the default StorageClass&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;storageclass.allowVolumeExpansion&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Allow expanding the volume&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;storageclass.reclaimPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Method used to reclaim an obsoleted volume&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Delete&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;storageclass.provisionerName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the provisionerName&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;storageclass.archiveOnDelete&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Archive pvc when deleting&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nfs.server&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Hostname of the NFS server&lt;/td&gt;
&lt;td&gt;null (ip or hostname)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nfs.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Basepath of the mount point to be used&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/ifs/kubernetes&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Resources required (e.g. CPU, memory)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;rbac.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Use Role-based Access Control&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podSecurityPolicy.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Create &amp;amp; use Pod Security Policy resources&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccount.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Should we create a ServiceAccount&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccount.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of the ServiceAccount to use&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/nginx-ingress/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/nginx-ingress/README/</guid>
      <description>

&lt;h1 id=&#34;nginx-ingress&#34;&gt;nginx-ingress&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;nginx-ingress&lt;/a&gt; is an Ingress controller that uses ConfigMap to store the nginx configuration.&lt;/p&gt;

&lt;p&gt;To use, add the &lt;code&gt;kubernetes.io/ingress.class: nginx&lt;/code&gt; annotation to your Ingress resources.&lt;/p&gt;

&lt;h2 id=&#34;tl-dr&#34;&gt;TL;DR;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/nginx-ingress
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This chart bootstraps an nginx-ingress deployment on a &lt;a href=&#34;http://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt; cluster using the &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; package manager.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 1.6+&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install --name my-release stable/nginx-ingress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command deploys nginx-ingress on the Kubernetes cluster in the default configuration. The &lt;a href=&#34;#configuration&#34;&gt;configuration&lt;/a&gt; section lists the parameters that can be configured during installation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: List all releases using &lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the &lt;code&gt;my-release&lt;/code&gt; deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes all the Kubernetes components associated with the chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The following table lists the configurable parameters of the nginx-ingress chart and their default values.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;name of the controller component&lt;/td&gt;
&lt;td&gt;&lt;code&gt;controller&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;controller container image repository&lt;/td&gt;
&lt;td&gt;&lt;code&gt;quay.io/kubernetes-ingress-controller/nginx-ingress-controller&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;controller container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0.18.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;controller container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.config&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;nginx ConfigMap entries&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.hostNetwork&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If the nginx deployment / daemonset should run on the host&amp;rsquo;s network namespace. Do not set this when &lt;code&gt;controller.service.externalIPs&lt;/code&gt; is set and &lt;code&gt;kube-proxy&lt;/code&gt; is used as there will be a port-conflict for port &lt;code&gt;80&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.defaultBackendService&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;default 404 backend service; required only if &lt;code&gt;defaultBackend.enabled = false&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.electionID&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;election ID to use for the status update&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ingress-controller-leader&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.extraEnvs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;any additional environment variables to set in the pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.extraContainers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sidecar containers to add to the controller pod. See &lt;a href=&#34;https://github.com/lemonldap-ng-controller/lemonldap-ng-controller&#34;&gt;LemonLDAP::NG controller&lt;/a&gt; as example&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.extraVolumeMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional volumeMounts to the controller main container&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.extraVolumes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional volumes to the controller pod&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.extraInitContainers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Containers, which are run before the app containers are started&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.ingressClass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;name of the ingress class to route through this controller&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nginx&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.scope.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;limit the scope of the ingress controller&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt; (watch all namespaces)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.scope.namespace&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;namespace to watch for ingress&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt; (use the release namespace)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.extraArgs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional controller container arguments&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.kind&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;install as Deployment or DaemonSet&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.daemonset.useHostPort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If &lt;code&gt;controller.kind&lt;/code&gt; is &lt;code&gt;DaemonSet&lt;/code&gt;, this will enable &lt;code&gt;hostPort&lt;/code&gt; for TCP/80 and TCP/443&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.daemonset.hostPorts.http&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If &lt;code&gt;controller.daemonset.useHostPort&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt; and this is non-empty, it sets the hostPort&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;80&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.daemonset.hostPorts.https&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If &lt;code&gt;controller.daemonset.useHostPort&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt; and this is non-empty, it sets the hostPort&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;443&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node taints to tolerate (requires Kubernetes &amp;gt;=1.6)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.affinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node/pod affinities (requires Kubernetes &amp;gt;=1.6)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.minReadySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;how many seconds a pod needs to be ready before killing the next, during update&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node labels for pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations to be added to pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.podLabels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;labels to add to the pod container metadata&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.replicaCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;desired number of controller pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.minAvailable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;minimum number of available controller pods for PodDisruptionBudget&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;controller pod resource requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;controller priorityClassName&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.lifecycle&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;controller pod lifecycle hooks&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for controller service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.labels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;labels for controller service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.publishService.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;if true, the controller will set the endpoint records on the ingress objects to reflect those on the service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.publishService.pathOverride&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;override of the default publish-service name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;internal controller cluster service IP&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.externalIPs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;controller service external IP addresses. Do not set this when &lt;code&gt;controller.hostNetwork&lt;/code&gt; is set to &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;kube-proxy&lt;/code&gt; is used as there will be a port-conflict for port &lt;code&gt;80&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.externalTrafficPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If &lt;code&gt;controller.service.type&lt;/code&gt; is &lt;code&gt;NodePort&lt;/code&gt; or &lt;code&gt;LoadBalancer&lt;/code&gt;, set this to &lt;code&gt;Local&lt;/code&gt; to enable &lt;a href=&#34;https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typenodeport&#34;&gt;source IP preservation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;Cluster&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.healthCheckNodePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If &lt;code&gt;controller.service.type&lt;/code&gt; is &lt;code&gt;NodePort&lt;/code&gt; or &lt;code&gt;LoadBalancer&lt;/code&gt; and &lt;code&gt;controller.service.externalTrafficPolicy&lt;/code&gt; is set to &lt;code&gt;Local&lt;/code&gt;, set this to &lt;a href=&#34;https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typenodeport&#34;&gt;the managed health-check port the kube-proxy will expose&lt;/a&gt;. If blank, a random port in the &lt;code&gt;NodePort&lt;/code&gt; range will be assigned&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address to assign to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list of IP CIDRs allowed access to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.enableHttp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;if port 80 should be opened for service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.enableHttps&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;if port 443 should be opened for service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.targetPorts.http&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets the targetPort that maps to the Ingress&amp;rsquo; port 80&lt;/td&gt;
&lt;td&gt;&lt;code&gt;80&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.targetPorts.https&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Sets the targetPort that maps to the Ingress&amp;rsquo; port 443&lt;/td&gt;
&lt;td&gt;&lt;code&gt;443&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;type of controller service to create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;LoadBalancer&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.nodePorts.http&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If &lt;code&gt;controller.service.type&lt;/code&gt; is &lt;code&gt;NodePort&lt;/code&gt; and this is non-empty, it sets the nodePort that maps to the Ingress&amp;rsquo; port 80&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.service.nodePorts.https&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If &lt;code&gt;controller.service.type&lt;/code&gt; is &lt;code&gt;NodePort&lt;/code&gt; and this is non-empty, it sets the nodePort that maps to the Ingress&amp;rsquo; port 443&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.livenessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delay before liveness probe is initiated&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.livenessProbe.periodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How often to perform the probe&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.livenessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;When the probe times out&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.livenessProbe.successThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive successes for the probe to be considered successful after having failed.&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.livenessProbe.failureThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive failures for the probe to be considered failed after having succeeded.&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.livenessProbe.port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The port number that the liveness probe will listen on.&lt;/td&gt;
&lt;td&gt;10254&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.readinessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delay before readiness probe is initiated&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.readinessProbe.periodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How often to perform the probe&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.readinessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;When the probe times out&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.readinessProbe.successThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive successes for the probe to be considered successful after having failed.&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.readinessProbe.failureThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive failures for the probe to be considered failed after having succeeded.&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.readinessProbe.port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The port number that the readiness probe will listen on.&lt;/td&gt;
&lt;td&gt;10254&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.stats.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;if &lt;code&gt;true&lt;/code&gt;, enable &amp;ldquo;vts-status&amp;rdquo; page&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.stats.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for controller stats service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.stats.service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;internal controller stats cluster service IP&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.stats.service.externalIPs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;controller service stats external IP addresses&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.stats.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address to assign to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.stats.service.loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list of IP CIDRs allowed access to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.stats.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;type of controller stats service to create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.metrics.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;if &lt;code&gt;true&lt;/code&gt;, enable Prometheus metrics (&lt;code&gt;controller.stats.enabled&lt;/code&gt; must be &lt;code&gt;true&lt;/code&gt; as well)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.metrics.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for Prometheus metrics service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.metrics.service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;cluster IP address to assign to service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.metrics.service.externalIPs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus metrics service external IP addresses&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.metrics.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address to assign to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.metrics.service.loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list of IP CIDRs allowed access to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.metrics.service.servicePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus metrics service port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;9913&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.metrics.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;type of Prometheus metrics service to create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.customTemplate.configMapName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;configMap containing a custom nginx template&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.customTemplate.configMapKey&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;configMap key containing the nginx template&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.headers&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;configMap key:value pairs containing the &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/customization/custom-headers&#34;&gt;custom headers&lt;/a&gt; for Nginx&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;controller.updateStrategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;allows setting of RollingUpdate strategy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If false, controller.defaultBackendService must be provided&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;name of the default backend component&lt;/td&gt;
&lt;td&gt;&lt;code&gt;default-backend&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;default backend container image repository&lt;/td&gt;
&lt;td&gt;&lt;code&gt;k8s.gcr.io/defaultbackend&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;default backend container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1.4&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;default backend container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.extraArgs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional default backend container arguments&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Http port number&lt;/td&gt;
&lt;td&gt;&lt;code&gt;8080&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node taints to tolerate (requires Kubernetes &amp;gt;=1.6)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.affinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node/pod affinities (requires Kubernetes &amp;gt;=1.6)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node labels for pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations to be added to pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.podLabels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;labels to add to the pod container metadata&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.replicaCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;desired number of default backend pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.minAvailable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;minimum number of available default backend pods for PodDisruptionBudget&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;default backend pod resource requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;default backend  priorityClassName&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for default backend service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;internal default backend cluster service IP&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.service.externalIPs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;default backend service external IP addresses&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address to assign to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.service.loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list of IP CIDRs allowed access to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;defaultBackend.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;type of default backend service to create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;imagePullSecrets&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;name of Secret resource containing private registry credentials&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;rbac.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;if &lt;code&gt;true&lt;/code&gt;, create &amp;amp; use RBAC resources&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;podSecurityPolicy.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;if &lt;code&gt;true&lt;/code&gt;, create &amp;amp; use Pod Security Policy resources&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccount.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;if &lt;code&gt;true&lt;/code&gt;, create a service account&lt;/td&gt;
&lt;td&gt;``&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccount.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The name of the service account to use. If not set and &lt;code&gt;create&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;, a name is generated using the fullname template.&lt;/td&gt;
&lt;td&gt;``&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;revisionHistoryLimit&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The number of old history to retain to allow rollback.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tcp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;TCP service key:value pairs&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;udp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;UDP service key:value pairs&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/nginx-ingress --name my-release \
    --set controller.stats.enabled=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/nginx-ingress --name my-release -f values.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A useful trick to debug issues with ingress is to increase the logLevel
as described &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/blob/master/docs/troubleshooting.md#debug&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/nginx-ingress --set controller.extraArgs.v=2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;poddisruptionbudget&#34;&gt;PodDisruptionBudget&lt;/h2&gt;

&lt;p&gt;Note that the PodDisruptionBudget resource will only be defined if the replicaCount is greater than one,
else it would make it impossible to evacuate a node. See &lt;a href=&#34;https://github.com/helm/charts/issues/7127&#34;&gt;gh issue #7127&lt;/a&gt; for more info.&lt;/p&gt;

&lt;h2 id=&#34;prometheus-metrics&#34;&gt;Prometheus Metrics&lt;/h2&gt;

&lt;p&gt;The Nginx ingress controller can export Prometheus metrics. In order for this to work, the VTS dashboard must be enabled as well.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/nginx-ingress --name my-release \
    --set controller.stats.enabled=true \
    --set controller.metrics.enabled=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can add Prometheus annotations to the metrics service using &lt;code&gt;controller.metrics.service.annotations&lt;/code&gt;. Alternatively, if you use the Prometheus Operator, you need to create a ServiceMonitor as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nginx-ingress-service-monitor
spec:
  jobLabel: nginx-ingress
  selector:
    matchLabels:
      app: nginx-ingress
      release: &amp;lt;RELEASE&amp;gt;
  namespaceSelector:
    matchNames:
      - &amp;lt;RELEASE_NAMESPACE&amp;gt;
  endpoints:
    - port: metrics
      interval: 30s
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: You can use the default &lt;a href=&#34;values.yaml&#34;&gt;values.yaml&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;externaldns-service-configuration&#34;&gt;ExternalDNS Service configuration&lt;/h2&gt;

&lt;p&gt;Add an &lt;a href=&#34;https://github.com/kubernetes-incubator/external-dns&#34;&gt;ExternalDNS&lt;/a&gt; annotation to the LoadBalancer service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;annotations:
  external-dns.alpha.kubernetes.io/hostname: kubernetes-example.com.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;aws-l7-elb-with-ssl-termination&#34;&gt;AWS L7 ELB with SSL Termination&lt;/h2&gt;

&lt;p&gt;Annotate the controller as shown in the &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/blob/master/deploy/provider/aws/service-l7.yaml&#34;&gt;nginx-ingress l7 patch&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;controller:
  service:
    targetPorts:
      http: http
      https: http
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:XX-XXXX-X:XXXXXXXXX:certificate/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXX
      service.beta.kubernetes.io/aws-load-balancer-backend-protocol: &amp;quot;http&amp;quot;
      service.beta.kubernetes.io/aws-load-balancer-ssl-ports: &amp;quot;https&amp;quot;
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: &#39;3600&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;aws-route53-mapper&#34;&gt;AWS route53-mapper&lt;/h2&gt;

&lt;p&gt;To configure the LoadBalancer service with the &lt;a href=&#34;https://github.com/kubernetes/kops/tree/master/addons/route53-mapper&#34;&gt;route53-mapper addon&lt;/a&gt;, add the &lt;code&gt;domainName&lt;/code&gt; annotation and &lt;code&gt;dns&lt;/code&gt; label:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;controller:
  service:
    labels:
      dns: &amp;quot;route53&amp;quot;
    annotations:
      domainName: &amp;quot;kubernetes-example.com&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/prometheus/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/prometheus/README/</guid>
      <description>

&lt;h1 id=&#34;prometheus&#34;&gt;Prometheus&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt;, a &lt;a href=&#34;https://cncf.io/&#34;&gt;Cloud Native Computing Foundation&lt;/a&gt; project, is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true.&lt;/p&gt;

&lt;h2 id=&#34;tl-dr&#34;&gt;TL;DR;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This chart bootstraps a &lt;a href=&#34;https://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt; deployment on a &lt;a href=&#34;http://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt; cluster using the &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; package manager.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 1.3+ with Beta APIs enabled&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install --name my-release stable/prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command deploys Prometheus on the Kubernetes cluster in the default configuration. The &lt;a href=&#34;#configuration&#34;&gt;configuration&lt;/a&gt; section lists the parameters that can be configured during installation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: List all releases using &lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the &lt;code&gt;my-release&lt;/code&gt; deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes all the Kubernetes components associated with the chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;prometheus-2-x&#34;&gt;Prometheus 2.x&lt;/h2&gt;

&lt;p&gt;Prometheus version 2.x has made changes to alertmanager, storage and recording rules. Check out the migration guide &lt;a href=&#34;https://prometheus.io/docs/prometheus/2.0/migration/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Users of this chart will need to update their alerting rules to the new format before they can upgrade.&lt;/p&gt;

&lt;h2 id=&#34;upgrading-from-previous-chart-versions&#34;&gt;Upgrading from previous chart versions.&lt;/h2&gt;

&lt;p&gt;As of version 5.0, this chart uses Prometheus 2.1. This version of prometheus introduces a new data format and is not compatible with prometheus 1.x. It is recommended to install this as a new release, as updating existing releases will not work. See the &lt;a href=&#34;https://prometheus.io/docs/prometheus/latest/migration/#storage&#34;&gt;prometheus docs&lt;/a&gt; for instructions on retaining your old data.&lt;/p&gt;

&lt;h3 id=&#34;example-migration&#34;&gt;Example migration&lt;/h3&gt;

&lt;p&gt;Assuming you have an existing release of the prometheus chart, named &lt;code&gt;prometheus-old&lt;/code&gt;. In order to update to prometheus 2.1 while keeping your old data do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Update the &lt;code&gt;prometheus-old&lt;/code&gt; release. Disable scraping on every component besides the prometheus server, similar to the configuration below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;alertmanager:
  enabled: false
alertmanagerFiles:
  alertmanager.yml: &amp;quot;&amp;quot;
kubeStateMetrics:
  enabled: false
nodeExporter:
  enabled: false
pushgateway:
  enabled: false
server:
  extraArgs:
    storage.local.retention: 720h
serverFiles:
  alerts: &amp;quot;&amp;quot;
  prometheus.yml: &amp;quot;&amp;quot;
  rules: &amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deploy a new release of the chart with version 5.0+ using prometheus 2.x. In the values.yaml set the scrape config as usual, and also add the &lt;code&gt;prometheus-old&lt;/code&gt; instance as a remote-read target.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;      prometheus.yml:
        ...
        remote_read:
        - url: http://prometheus-old/api/v1/read
        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Old data will be available when you query the new prometheus instance.&lt;/p&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The following table lists the configurable parameters of the Prometheus chart and their default values.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create alertmanager&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager container name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;alertmanager&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager container image repository&lt;/td&gt;
&lt;td&gt;&lt;code&gt;prom/alertmanager&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v0.15.2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.prefixURL&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The prefix slug at which the server can be accessed&lt;/td&gt;
&lt;td&gt;``&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.baseURL&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The external url at which the server can be accessed&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.extraArgs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional alertmanager container arguments&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.configMapOverrideName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus alertmanager ConfigMap override where full-name is &lt;code&gt;{{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}}&lt;/code&gt; and setting this value will prevent the default alertmanager ConfigMap from being generated&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.ingress.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, alertmanager Ingress will be created&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.ingress.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager Ingress annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.ingress.extraLabels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager Ingress additional labels&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.ingress.hosts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager Ingress hostnames&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.ingress.tls&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager Ingress TLS configuration (YAML)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node labels for alertmanager pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node taints to tolerate (requires Kubernetes &amp;gt;=1.6)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.affinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pod affinity&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.schedulerName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager alternate scheduler name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.persistentVolume.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, alertmanager will create a Persistent Volume Claim&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.persistentVolume.accessModes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager data Persistent Volume access modes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[ReadWriteOnce]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.persistentVolume.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Annotations for alertmanager Persistent Volume Claim&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.persistentVolume.existingClaim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager data Persistent Volume existing claim name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.persistentVolume.mountPath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager data Persistent Volume mount root path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/data&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.persistentVolume.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager data Persistent Volume size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;2Gi&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.persistentVolume.storageClass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager data Persistent Volume Storage Class&lt;/td&gt;
&lt;td&gt;&lt;code&gt;unset&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.persistentVolume.subPath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Subdirectory of alertmanager data Persistent Volume to mount&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations to be added to alertmanager pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.replicaCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;desired number of alertmanager pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager priorityClassName&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager pod resource requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.securityContext&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Custom &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/&#34;&gt;security context&lt;/a&gt; for Alert Manager containers&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for alertmanager service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;internal alertmanager cluster service IP&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.service.externalIPs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager service external IP addresses&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address to assign to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.service.loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list of IP CIDRs allowed access to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.service.servicePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;alertmanager service port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;80&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanager.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;type of alertmanager service to create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;alertmanagerFiles.alertmanager.yml&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus alertmanager configuration&lt;/td&gt;
&lt;td&gt;example configuration&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;configmapReload.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;configmap-reload container name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;configmap-reload&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;configmapReload.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;configmap-reload container image repository&lt;/td&gt;
&lt;td&gt;&lt;code&gt;jimmidyson/configmap-reload&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;configmapReload.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;configmap-reload container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v0.2.2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;configmapReload.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;configmap-reload container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;configmapReload.extraArgs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional configmap-reload container arguments&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;configmapReload.extraConfigmapMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional configmap-reload configMap mounts&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;configmapReload.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;configmap-reload pod resource requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;initChownData.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If false, don&amp;rsquo;t reset data ownership at startup&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;initChownData.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;init-chown-data container name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;init-chown-data&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;initChownData.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;init-chown-data container image repository&lt;/td&gt;
&lt;td&gt;&lt;code&gt;busybox&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;initChownData.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;init-chown-data container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;latest&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;initChownData.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;init-chown-data container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;initChownData.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;init-chown-data pod resource requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create kube-state-metrics&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;kube-state-metrics container name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;kube-state-metrics&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;kube-state-metrics container image repository&lt;/td&gt;
&lt;td&gt;&lt;code&gt;quay.io/coreos/kube-state-metrics&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;kube-state-metrics container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v1.4.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;kube-state-metrics container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.args&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;kube-state-metrics container arguments&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node labels for kube-state-metrics pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations to be added to kube-state-metrics pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.deploymentAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations to be added to kube-state-metrics deployment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node taints to tolerate (requires Kubernetes &amp;gt;=1.6)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.replicaCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;desired number of kube-state-metrics pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;kube-state-metrics priorityClassName&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;kube-state-metrics resource requests and limits (YAML)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.securityContext&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Custom &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/&#34;&gt;security context&lt;/a&gt; for kube-state-metrics containers&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for kube-state-metrics service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{prometheus.io/scrape: &amp;quot;true&amp;quot;}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;internal kube-state-metrics cluster service IP&lt;/td&gt;
&lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.service.externalIPs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;kube-state-metrics service external IP addresses&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address to assign to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.service.loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list of IP CIDRs allowed access to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.service.servicePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;kube-state-metrics service port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;80&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubeStateMetrics.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;type of kube-state-metrics service to create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create node-exporter&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node-exporter container name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;node-exporter&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node-exporter container image repository&lt;/td&gt;
&lt;td&gt;&lt;code&gt;prom/node-exporter&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node-exporter container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v0.16.0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node-exporter container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.extraArgs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional node-exporter container arguments&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.extraHostPathMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional node-exporter hostPath mounts&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.extraConfigmapMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional node-exporter configMap mounts&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node labels for node-exporter pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations to be added to node-exporter pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.pod.labels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;labels to be added to node-exporter pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node taints to tolerate (requires Kubernetes &amp;gt;=1.6)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node-exporter priorityClassName&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node-exporter resource requests and limits (YAML)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.securityContext&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;securityContext for containers in pod&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for node-exporter service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{prometheus.io/scrape: &amp;quot;true&amp;quot;}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;internal node-exporter cluster service IP&lt;/td&gt;
&lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.service.externalIPs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node-exporter service external IP addresses&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address to assign to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.service.loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list of IP CIDRs allowed access to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.service.servicePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node-exporter service port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;9100&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;nodeExporter.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;type of node-exporter service to create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create pushgateway&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway container name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;pushgateway&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway container image repository&lt;/td&gt;
&lt;td&gt;&lt;code&gt;prom/pushgateway&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v0.5.2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.extraArgs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional pushgateway container arguments&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.ingress.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, pushgateway Ingress will be created&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.ingress.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway Ingress annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.ingress.hosts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway Ingress hostnames&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.ingress.tls&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway Ingress TLS configuration (YAML)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node labels for pushgateway pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations to be added to pushgateway pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node taints to tolerate (requires Kubernetes &amp;gt;=1.6)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.replicaCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;desired number of pushgateway pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway priorityClassName&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway pod resource requests &amp;amp; limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for pushgateway service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;internal pushgateway cluster service IP&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.service.externalIPs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway service external IP addresses&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address to assign to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.service.loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list of IP CIDRs allowed access to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.service.servicePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pushgateway service port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;9091&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;pushgateway.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;type of pushgateway service to create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;rbac.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create &amp;amp; use RBAC resources&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server container name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;server&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server container image repository&lt;/td&gt;
&lt;td&gt;&lt;code&gt;prom/prometheus&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server container image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v2.3.2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server container image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.enableAdminApi&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, Prometheus administrative HTTP API will be enabled. Please note, that you should take care of administrative API access protection (ingress or some frontend Nginx with auth) before enabling it.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.global.scrape_interval&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How frequently to scrape targets by default&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1m&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.global.scrape_timeout&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How long until a scrape request times out&lt;/td&gt;
&lt;td&gt;&lt;code&gt;10s&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.global.evaluation_interval&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How frequently to evaluate rules&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1m&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.extraArgs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional Prometheus server container arguments&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.prefixURL&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The prefix slug at which the server can be accessed&lt;/td&gt;
&lt;td&gt;``&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.baseURL&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The external url at which the server can be accessed&lt;/td&gt;
&lt;td&gt;``&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.extraHostPathMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional Prometheus server hostPath mounts&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.extraConfigmapMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional Prometheus server configMap mounts&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.extraSecretMounts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional Prometheus server Secret mounts&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.configMapOverrideName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server ConfigMap override where full-name is &lt;code&gt;{{.Release.Name}}-{{.Values.server.configMapOverrideName}}&lt;/code&gt; and setting this value will prevent the default server ConfigMap from being generated&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.ingress.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, Prometheus server Ingress will be created&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.ingress.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server Ingress annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.ingress.extraLabels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server Ingress additional labels&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.ingress.hosts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server Ingress hostnames&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.ingress.tls&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server Ingress TLS configuration (YAML)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node labels for Prometheus server pod assignment&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;node taints to tolerate (requires Kubernetes &amp;gt;=1.6)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.affinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;pod affinity&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.priorityClassName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server priorityClassName&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.schedulerName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server alternate scheduler name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.persistentVolume.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, Prometheus server will create a Persistent Volume Claim&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.persistentVolume.accessModes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server data Persistent Volume access modes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[ReadWriteOnce]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.persistentVolume.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server data Persistent Volume annotations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.persistentVolume.existingClaim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server data Persistent Volume existing claim name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.persistentVolume.mountPath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server data Persistent Volume mount root path&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/data&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.persistentVolume.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server data Persistent Volume size&lt;/td&gt;
&lt;td&gt;&lt;code&gt;8Gi&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.persistentVolume.storageClass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server data Persistent Volume Storage Class&lt;/td&gt;
&lt;td&gt;&lt;code&gt;unset&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.persistentVolume.subPath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Subdirectory of Prometheus server data Persistent Volume to mount&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations to be added to Prometheus server pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.deploymentAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations to be added to Prometheus server deployment&lt;/td&gt;
&lt;td&gt;`{}&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.replicaCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;desired number of Prometheus server pods&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server resource requests and limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.securityContext&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Custom &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/&#34;&gt;security context&lt;/a&gt; for server containers&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for Prometheus server service&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.service.clusterIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;internal Prometheus server cluster service IP&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.service.externalIPs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server service external IP addresses&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;IP address to assign to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.service.loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;list of IP CIDRs allowed access to load balancer (if supported)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.service.nodePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Port to be used as the service NodePort (ignored if &lt;code&gt;server.service.type&lt;/code&gt; is not &lt;code&gt;NodePort&lt;/code&gt;)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.service.servicePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server service port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;80&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;type of Prometheus server service to create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.alertmanager.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create the alertmanager service account&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.alertmanager.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;name of the alertmanager service account to use or create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ prometheus.alertmanager.fullname }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.kubeStateMetrics.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create the kubeStateMetrics service account&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.kubeStateMetrics.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;name of the kubeStateMetrics service account to use or create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ prometheus.kubeStateMetrics.fullname }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.nodeExporter.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create the nodeExporter service account&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.nodeExporter.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;name of the nodeExporter service account to use or create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ prometheus.nodeExporter.fullname }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.pushgateway.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create the pushgateway service account&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.pushgateway.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;name of the pushgateway service account to use or create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ prometheus.pushgateway.fullname }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.server.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;If true, create the server service account&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccounts.server.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;name of the server service account to use or create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{{ prometheus.server.fullname }}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.terminationGracePeriodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server Pod termination grace period&lt;/td&gt;
&lt;td&gt;&lt;code&gt;300&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;server.retention&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;(optional) Prometheus data retention&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serverFiles.alerts&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server alerts configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serverFiles.rules&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server rules configuration&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serverFiles.prometheus.yml&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Prometheus server scrape configuration&lt;/td&gt;
&lt;td&gt;example configuration&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;networkPolicy.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enable NetworkPolicy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Specify each parameter using the &lt;code&gt;--set key=value[,key=value]&lt;/code&gt; argument to &lt;code&gt;helm install&lt;/code&gt;. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/prometheus --name my-release \
    --set server.terminationGracePeriodSeconds=360
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively, a YAML file that specifies the values for the above parameters can be provided while installing the chart. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ helm install stable/prometheus --name my-release -f values.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: You can use the default &lt;a href=&#34;values.yaml&#34;&gt;values.yaml&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;rbac-configuration&#34;&gt;RBAC Configuration&lt;/h3&gt;

&lt;p&gt;Roles and RoleBindings resources will be created automatically for &lt;code&gt;server&lt;/code&gt; and &lt;code&gt;kubeStateMetrics&lt;/code&gt; services.&lt;/p&gt;

&lt;p&gt;To manually setup RBAC you need to set the parameter &lt;code&gt;rbac.create=false&lt;/code&gt; and specify the service account to be used for each service by setting the parameters: &lt;code&gt;serviceAccounts.{{ component }}.create&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; and &lt;code&gt;serviceAccounts.{{ component }}.name&lt;/code&gt; to the name of a pre-existing service account.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: You can refer to the default &lt;code&gt;*-clusterrole.yaml&lt;/code&gt; and &lt;code&gt;*-clusterrolebinding.yaml&lt;/code&gt; files in &lt;a href=&#34;templates/&#34;&gt;templates&lt;/a&gt; to customize your own.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;configmap-files&#34;&gt;ConfigMap Files&lt;/h3&gt;

&lt;p&gt;AlertManager is configured through &lt;a href=&#34;https://prometheus.io/docs/alerting/configuration/&#34;&gt;alertmanager.yml&lt;/a&gt;. This file (and any others listed in &lt;code&gt;alertmanagerFiles&lt;/code&gt;) will be mounted into the &lt;code&gt;alertmanager&lt;/code&gt; pod.&lt;/p&gt;

&lt;p&gt;Prometheus is configured through &lt;a href=&#34;https://prometheus.io/docs/operating/configuration/&#34;&gt;prometheus.yml&lt;/a&gt;. This file (and any others listed in &lt;code&gt;serverFiles&lt;/code&gt;) will be mounted into the &lt;code&gt;server&lt;/code&gt; pod.&lt;/p&gt;

&lt;h3 id=&#34;ingress-tls&#34;&gt;Ingress TLS&lt;/h3&gt;

&lt;p&gt;If your cluster allows automatic creation/retrieval of TLS certificates (e.g. &lt;a href=&#34;https://github.com/jetstack/kube-lego&#34;&gt;kube-lego&lt;/a&gt;), please refer to the documentation for that mechanism.&lt;/p&gt;

&lt;p&gt;To manually configure TLS, first create/retrieve a key &amp;amp; certificate pair for the address(es) you wish to protect. Then create a TLS secret in the namespace:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;kubectl create secret tls prometheus-server-tls --cert=path/to/tls.cert --key=path/to/tls.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Include the secret&amp;rsquo;s name, along with the desired hostnames, in the alertmanager/server Ingress TLS section of your custom &lt;code&gt;values.yaml&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;server:
  ingress:
    ## If true, Prometheus server Ingress will be created
    ##
    enabled: true

    ## Prometheus server Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    hosts:
      - prometheus.domain.com

    ## Prometheus server Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls:
      - secretName: prometheus-server-tls
        hosts:
          - prometheus.domain.com
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;networkpolicy&#34;&gt;NetworkPolicy&lt;/h3&gt;

&lt;p&gt;Enabling Network Policy for Prometheus will secure connections to Alert Manager
and Kube State Metrics by only accepting connections from Prometheus Server.
All inbound connections to Prometheus Server are still allowed.&lt;/p&gt;

&lt;p&gt;To enable network policy for Prometheus, install a networking plugin that
implements the Kubernetes NetworkPolicy spec, and set &lt;code&gt;networkPolicy.enabled&lt;/code&gt; to true.&lt;/p&gt;

&lt;p&gt;If NetworkPolicy is enabled for Prometheus&amp;rsquo; scrape targets, you may also need
to manually create a networkpolicy which allows it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/src/k8s/helm/redis/README/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/src/k8s/helm/redis/README/</guid>
      <description>

&lt;h1 id=&#34;redis&#34;&gt;Redis&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://redis.io/&#34;&gt;Redis&lt;/a&gt; is an advanced key-value cache and store. It is often referred to as a data structure server since keys can contain strings, hashes, lists, sets, sorted sets, bitmaps and hyperloglogs.&lt;/p&gt;

&lt;h2 id=&#34;tl-dr&#34;&gt;TL;DR&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Testing configuration
$ helm install stable/redis
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Production configuration
$ helm install stable/redis --values values-production.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This chart bootstraps a &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-redis&#34;&gt;Redis&lt;/a&gt; deployment on a &lt;a href=&#34;http://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt; cluster using the &lt;a href=&#34;https://helm.sh&#34;&gt;Helm&lt;/a&gt; package manager.&lt;/p&gt;

&lt;p&gt;Bitnami charts can be used with &lt;a href=&#34;https://kubeapps.com/&#34;&gt;Kubeapps&lt;/a&gt; for deployment and management of Helm Charts in clusters.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 1.8+&lt;/li&gt;
&lt;li&gt;PV provisioner support in the underlying infrastructure&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installing-the-chart&#34;&gt;Installing the Chart&lt;/h2&gt;

&lt;p&gt;To install the chart with the release name &lt;code&gt;my-release&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release stable/redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command deploys Redis on the Kubernetes cluster in the default configuration. The &lt;a href=&#34;#configuration&#34;&gt;configuration&lt;/a&gt; section lists the parameters that can be configured during installation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: List all releases using &lt;code&gt;helm list&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;uninstalling-the-chart&#34;&gt;Uninstalling the Chart&lt;/h2&gt;

&lt;p&gt;To uninstall/delete the &lt;code&gt;my-release&lt;/code&gt; deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm delete my-release
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command removes all the Kubernetes components associated with the chart and deletes the release.&lt;/p&gt;

&lt;h2 id=&#34;upgrading-an-existing-release-to-a-new-major-version&#34;&gt;Upgrading an existing Release to a new major version&lt;/h2&gt;

&lt;p&gt;A major chart version change (like v1.2.3 -&amp;gt; v2.0.0) indicates that there is an
incompatible breaking change needing manual actions.&lt;/p&gt;

&lt;h3 id=&#34;5-0-0&#34;&gt;5.0.0&lt;/h3&gt;

&lt;p&gt;The default image in this release may be switched out for any image containing the &lt;code&gt;redis-server&lt;/code&gt;
and &lt;code&gt;redis-cli&lt;/code&gt; binaries. If &lt;code&gt;redis-server&lt;/code&gt; is not the default image ENTRYPOINT, &lt;code&gt;master.command&lt;/code&gt;
must be specified.&lt;/p&gt;

&lt;h4 id=&#34;breaking-changes&#34;&gt;Breaking changes&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;master.args&lt;/code&gt; and &lt;code&gt;slave.args&lt;/code&gt; are removed. Use &lt;code&gt;master.command&lt;/code&gt; or &lt;code&gt;slave.command&lt;/code&gt; instead in order to override the image entrypoint, or &lt;code&gt;master.extraFlags&lt;/code&gt; to pass additional flags to &lt;code&gt;redis-server&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;disableCommands&lt;/code&gt; is now interpreted as an array of strings instead of a string of comma separated values.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;master.persistence.path&lt;/code&gt; now defaults to &lt;code&gt;/data&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;4-0-0&#34;&gt;4.0.0&lt;/h3&gt;

&lt;p&gt;This version removes the &lt;code&gt;chart&lt;/code&gt; label from the &lt;code&gt;spec.selector.matchLabels&lt;/code&gt;
which is immutable since &lt;code&gt;StatefulSet apps/v1beta2&lt;/code&gt;. It has been inadvertently
added, causing any subsequent upgrade to fail. See &lt;a href=&#34;https://github.com/helm/charts/issues/7726&#34;&gt;https://github.com/helm/charts/issues/7726&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It also fixes &lt;a href=&#34;https://github.com/helm/charts/issues/7726&#34;&gt;https://github.com/helm/charts/issues/7726&lt;/a&gt; where a deployment &lt;code&gt;extensions/v1beta1&lt;/code&gt; can not be upgraded if &lt;code&gt;spec.selector&lt;/code&gt; is not explicitly set.&lt;/p&gt;

&lt;p&gt;Finally, it fixes &lt;a href=&#34;https://github.com/helm/charts/issues/7803&#34;&gt;https://github.com/helm/charts/issues/7803&lt;/a&gt; by removing mutable labels in &lt;code&gt;spec.VolumeClaimTemplate.metadata.labels&lt;/code&gt; so that it is upgradable.&lt;/p&gt;

&lt;p&gt;In order to upgrade, delete the Redis StatefulSet before upgrading:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ kubectl delete statefulsets.apps --cascade=false my-release-redis-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And edit the Redis slave (and metrics if enabled) deployment:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl patch deployments my-release-redis-slave --type=json -p=&#39;[{&amp;quot;op&amp;quot;: &amp;quot;remove&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/spec/selector/matchLabels/chart&amp;quot;}]&#39;
kubectl patch deployments my-release-redis-metrics --type=json -p=&#39;[{&amp;quot;op&amp;quot;: &amp;quot;remove&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/spec/selector/matchLabels/chart&amp;quot;}]&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;

&lt;p&gt;The following table lists the configurable parameters of the Redis chart and their default values.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;global.imageRegistry&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Global Docker image registry&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.registry&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis Image registry&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker.io&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis Image name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bitnami/redis&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis Image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{VERSION}&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Always&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;image.pullSecrets&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specify docker-registry secret names as an array&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cluster.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Use master-slave topology&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;cluster.slaveCount&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Number of slaves&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;existingSecret&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of existing secret object (for password authentication)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;usePassword&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Use password&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;password&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis password (ignored if existingSecret set)&lt;/td&gt;
&lt;td&gt;Randomly generated&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;configmap&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis configuration file to be used&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;networkPolicy.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enable NetworkPolicy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;networkPolicy.allowExternal&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Don&amp;rsquo;t require client label for connections&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccount.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specifies whether a ServiceAccount should be created&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;serviceAccount.name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;The name of the ServiceAccount to create&lt;/td&gt;
&lt;td&gt;Generated using the fullname template&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;rbac.create&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specifies whether RBAC resources should be created&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;rbac.role.rules&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Rules to create&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Start a side-car prometheus exporter&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.image.registry&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis exporter image registry&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker.io&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis exporter image name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;oliver006/redis_exporter&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis exporter image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;v0.20.2&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.image.pullSecrets&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Specify docker-registry secret names as an array&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.extraArgs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Extra arguments for the binary; possible values &lt;a href=&#34;https://github.com/oliver006/redis_exporter#flags&#34;&gt;here&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.podLabels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional labels for Metrics exporter pod&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional annotations for Metrics exporter pod&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes Service type (redis metrics)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Annotations for the services to monitor  (redis master and redis slave service)&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;loadBalancerIP if redis metrics service type is &lt;code&gt;LoadBalancer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Exporter resource requests/limit&lt;/td&gt;
&lt;td&gt;Memory: &lt;code&gt;256Mi&lt;/code&gt;, CPU: &lt;code&gt;100m&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.serviceMonitor.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;if &lt;code&gt;true&lt;/code&gt;, creates a Prometheus Operator ServiceMonitor (also requires &lt;code&gt;metrics.enabled&lt;/code&gt; to be &lt;code&gt;true&lt;/code&gt;)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.serviceMonitor.namespace&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Optional namespace which Prometheus is running in&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.serviceMonitor.interval&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How frequently to scrape metrics (use by default, falling back to Prometheus&amp;rsquo; default)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;metrics.serviceMonitor.selector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Default to kube-prometheus install (CoreOS recommended), but should be set according to Prometheus install&lt;/td&gt;
&lt;td&gt;&lt;code&gt;{ prometheus: kube-prometheus }&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;persistence.existingClaim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Provide an existing PersistentVolumeClaim&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Use a PVC to persist data (master node)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Path to mount the volume at, to use other images&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/data&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.subPath&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Subdirectory of the volume to mount at&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.storageClass&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Storage class of backing PVC&lt;/td&gt;
&lt;td&gt;&lt;code&gt;generic&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.accessModes&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Persistent Volume Access Modes&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[ReadWriteOnce]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.persistence.size&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Size of data volume&lt;/td&gt;
&lt;td&gt;&lt;code&gt;8Gi&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.statefulset.updateStrategy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Update strategy for StatefulSet&lt;/td&gt;
&lt;td&gt;onDelete&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.statefulset.rollingUpdatePartition&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Partition update strategy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.podLabels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional labels for Redis master pod&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional annotations for Redis master pod&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis master port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;6379&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.command&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis master entrypoint array. The docker image&amp;rsquo;s ENTRYPOINT is used if this is not provided.&lt;/td&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.disableCommands&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Array of Redis commands to disable (master)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[&amp;quot;FLUSHDB&amp;quot;, &amp;quot;FLUSHALL&amp;quot;]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.extraFlags&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis master additional command line flags&lt;/td&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.nodeSelector&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis master Node labels for pod assignment&lt;/td&gt;
&lt;td&gt;{&amp;ldquo;beta.kubernetes.io/arch&amp;rdquo;: &amp;ldquo;amd64&amp;rdquo;}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.tolerations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Toleration labels for Redis master pod assignment&lt;/td&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.affinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Affinity settings for Redis master pod assignment&lt;/td&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.schedulerName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of an alternate scheduler&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes Service type (redis master)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.service.port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes Service port (redis master)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;6379&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.service.nodePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes Service nodePort (redis master)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for redis master service&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;loadBalancerIP if redis master service type is &lt;code&gt;LoadBalancer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.securityContext.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enable security context (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.securityContext.fsGroup&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Group ID for the container (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1001&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.securityContext.runAsUser&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;User ID for the container (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1001&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis master CPU/Memory resource requests/limits&lt;/td&gt;
&lt;td&gt;Memory: &lt;code&gt;256Mi&lt;/code&gt;, CPU: &lt;code&gt;100m&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Turn on and off liveness probe (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delay before liveness probe is initiated (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.periodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How often to perform the probe (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;When the probe times out (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.successThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive successes for the probe to be considered successful after having failed (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.failureThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive failures for the probe to be considered failed after having succeeded.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Turn on and off readiness probe (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delay before readiness probe is initiated (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.periodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How often to perform the probe (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;When the probe times out (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.successThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive successes for the probe to be considered successful after having failed (redis master pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.failureThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive failures for the probe to be considered failed after having succeeded.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;volumePermissions.image.registry&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Init container volume-permissions image registry&lt;/td&gt;
&lt;td&gt;&lt;code&gt;docker.io&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;volumePermissions.image.repository&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Init container volume-permissions image name&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bitnami/minideb&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;volumePermissions.image.tag&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Init container volume-permissions image tag&lt;/td&gt;
&lt;td&gt;&lt;code&gt;latest&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;volumePermissions.image.pullPolicy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Init container volume-permissions image pull policy&lt;/td&gt;
&lt;td&gt;&lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.service.type&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes Service type (redis slave)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ClusterIP&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.service.nodePort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes Service nodePort (redis slave)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.service.annotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;annotations for redis slave service&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.service.loadBalancerIP&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;LoadBalancerIP if Redis slave service type is &lt;code&gt;LoadBalancer&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis slave port&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.port&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.command&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis slave entrypoint array. The docker image&amp;rsquo;s ENTRYPOINT is used if this is not provided.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.command&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.disableCommands&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Array of Redis commands to disable (slave)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.disableCommands&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.extraFlags&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis slave additional command line flags&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.extraFlags&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.livenessProbe.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Turn on and off liveness probe (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.enabled&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.livenessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delay before liveness probe is initiated (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.livenessProbe.periodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How often to perform the probe (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.periodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.livenessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;When the probe times out (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.livenessProbe.successThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive successes for the probe to be considered successful after having failed (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.successThreshold&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.livenessProbe.failureThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive failures for the probe to be considered failed after having succeeded.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.livenessProbe.failureThreshold&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.readinessProbe.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Turn on and off slave.readiness probe (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.enabled&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.readinessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Delay before slave.readiness probe is initiated (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.initialDelaySeconds&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.readinessProbe.periodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How often to perform the probe (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.periodSeconds&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.readinessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;When the probe times out (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.timeoutSeconds&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.readinessProbe.successThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive successes for the probe to be considered successful after having failed (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.successThreshold&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.readinessProbe.failureThreshold&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Minimum consecutive failures for the probe to be considered failed after having succeeded. (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.readinessProbe.failureThreshold&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.podLabels&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional labels for Redis slave pod&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.podLabels&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Additional annotations for Redis slave pod&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.podAnnotations&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.schedulerName&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Name of an alternate scheduler&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nil&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.securityContext.enabled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enable security context (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.securityContext.enabled&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.securityContext.fsGroup&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Group ID for the container (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.securityContext.fsGroup&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.securityContext.runAsUser&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;User ID for the container (redis slave pod)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.securityContext.runAsUser&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.resources&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Redis slave CPU/Memory resource requests/limits&lt;/td&gt;
&lt;td&gt;&lt;code&gt;master.resources&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;code&gt;slave.affinity&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Enable node/pod affinity for slaves&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Specify each parameter using the &lt;code&gt;--set key=value[,key=value]&lt;/code&gt; argument to &lt;code&gt;helm install&lt;/code&gt;. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release \
  --set password=secretpassword \
    stable/redis
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above command sets the Redis server password to &lt;code&gt;secretpassword&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --name my-release -f values.yaml stable/redis
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: You can use the default &lt;a href=&#34;values.yaml&#34;&gt;values.yaml&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note for minikube users&lt;/strong&gt;: Current versions of minikube (v0.24.1 at the time of writing) provision &lt;code&gt;hostPath&lt;/code&gt; persistent volumes that are only writable by root. Using chart defaults cause pod failure for the Redis pod as it attempts to write to the &lt;code&gt;/bitnami&lt;/code&gt; directory. Consider installing Redis with &lt;code&gt;--set persistence.enabled=false&lt;/code&gt;. See minikube issue &lt;a href=&#34;https://github.com/kubernetes/minikube/issues/1990&#34;&gt;1990&lt;/a&gt; for more information.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;networkpolicy&#34;&gt;NetworkPolicy&lt;/h2&gt;

&lt;p&gt;To enable network policy for Redis, install
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy#before-you-begin&#34;&gt;a networking plugin that implements the Kubernetes NetworkPolicy spec&lt;/a&gt;,
and set &lt;code&gt;networkPolicy.enabled&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For Kubernetes v1.5 &amp;amp; v1.6, you must also turn on NetworkPolicy by setting
the DefaultDeny namespace annotation. Note: this will enforce policy for &lt;em&gt;all&lt;/em&gt; pods in the namespace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl annotate namespace default &amp;quot;net.beta.kubernetes.io/network-policy={\&amp;quot;ingress\&amp;quot;:{\&amp;quot;isolation\&amp;quot;:\&amp;quot;DefaultDeny\&amp;quot;}}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With NetworkPolicy enabled, only pods with the generated client label will be
able to connect to Redis. This label will be displayed in the output
after a successful install.&lt;/p&gt;

&lt;h2 id=&#34;persistence&#34;&gt;Persistence&lt;/h2&gt;

&lt;p&gt;By default, the chart mounts a &lt;a href=&#34;http://kubernetes.io/docs/user-guide/persistent-volumes/&#34;&gt;Persistent Volume&lt;/a&gt; at the &lt;code&gt;/data&lt;/code&gt; path. The volume is created using dynamic volume provisioning. If a Persistent Volume Claim already exists, specify it during installation.&lt;/p&gt;

&lt;h3 id=&#34;existing-persistentvolumeclaim&#34;&gt;Existing PersistentVolumeClaim&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Create the PersistentVolume&lt;/li&gt;
&lt;li&gt;Create the PersistentVolumeClaim&lt;/li&gt;
&lt;li&gt;Install the chart&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ helm install --set persistence.existingClaim=PVC_NAME stable/redis
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;metrics&#34;&gt;Metrics&lt;/h2&gt;

&lt;p&gt;The chart optionally can start a metrics exporter for &lt;a href=&#34;https://prometheus.io&#34;&gt;prometheus&lt;/a&gt;. The metrics endpoint (port 9121) is exposed in the service. Metrics can be scraped from within the cluster using something similar as the described in the &lt;a href=&#34;https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml&#34;&gt;example Prometheus scrape configuration&lt;/a&gt;. If metrics are to be scraped from outside the cluster, the Kubernetes API proxy can be utilized to access the endpoint.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>