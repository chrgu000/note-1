<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>工作笔记</title>
    <link>http://dishui.oschina.io/note-hugo/categories/hadoop/index.xml</link>
    <description>Recent content on 工作笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <atom:link href="http://dishui.oschina.io/note-hugo/categories/hadoop/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>hive</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hive&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hive只在一个节点上安装即可&#34;&gt;1. Hive只在一个节点上安装即可&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive只在一个节点上安装即可&#34;&gt;1. Hive只在一个节点上安装即可&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;上传tar包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf hive-1.2.1.tar.gz -C /hadoop/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装mysql数据库（切换到root用户）（装在哪里没有限制，只有能联通hadoop集群的节点）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;mysql.yml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;version: &#39;2&#39;
services:
  mysql:
    image: dishui.io:5000/mysql:5.5.52
    container_name: mysql
    environment:
      - &#34;MYSQL_ROOT_PASSWORD=111111&#34;
    ports:
      - &#34;3306:3306&#34;
    networks:
      - hadoop
networks:
  hadoop:
    external: true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置 &lt;strong&gt;hive&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;配置 &lt;strong&gt;HIVE_HOME&lt;/strong&gt; 环境变量  &lt;strong&gt;vi conf/hive-env.sh&lt;/strong&gt; 配置其中的 &lt;strong&gt;$hadoop_home&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置元数据库信息   &lt;strong&gt;vi hive-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;jdbc:mysql://mysql:3306/hive?createDatabaseIfNotExist=true&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;JDBC connect string for a JDBC metastore&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;Driver class name for a JDBC metastore&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;root&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;username to use against metastore database&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;111111&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;password to use against metastore database&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装 &lt;strong&gt;hive&lt;/strong&gt; 和 &lt;strong&gt;mysql&lt;/strong&gt; 完成后，将 &lt;strong&gt;mysql&lt;/strong&gt; 的连接 &lt;strong&gt;jar&lt;/strong&gt; 包拷贝到 &lt;strong&gt;$HIVE_HOME/lib&lt;/strong&gt; 目录下&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果出现没有权限的问题，在 &lt;strong&gt;mysql&lt;/strong&gt; 授权(在安装 &lt;strong&gt;mysql&lt;/strong&gt; 的机器上执行)&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mysql -uroot -p&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;执行下面的语句  &lt;strong&gt;.&lt;/strong&gt;:所有库下的所有表   %：任何IP地址或主机都可以连接&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;111111&#39; WITH GRANT OPTION;
FLUSH PRIVILEGES;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Jline&lt;/strong&gt; 包版本不一致的问题，需要拷贝 &lt;strong&gt;hive&lt;/strong&gt; 的 &lt;strong&gt;lib&lt;/strong&gt; 目录中 &lt;strong&gt;jline.2.12.jar&lt;/strong&gt; 的 &lt;strong&gt;jar&lt;/strong&gt; 包替换掉 &lt;strong&gt;hadoop&lt;/strong&gt; 中的
&lt;strong&gt;/home/hadoop/app/hadoop-2.6.4/share/hadoop/yarn/lib/jline-0.9.94.jar&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动 &lt;strong&gt;hive&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hive&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;建表(默认是内部表)&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;create table trade_detail(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39;;
# 建分区表
create table td_part(id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by &#39;\t&#39;;
# 建外部表
create external table td_ext(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39; location &#39;/td_ext&#39;;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建分区表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 普通表和分区表区别：有大量数据增加的需要建分区表
create table book (id bigint, name string) partitioned by (pubdate string) row format delimited fields terminated by &#39;\t&#39;;

# 分区表加载数据
load data local inpath &#39;./book.txt&#39; overwrite into table book partition (pubdate=&#39;2010-08-22&#39;);

load data local inpath &#39;/root/data.am&#39; into table beauty partition (nation=&#34;USA&#34;);

select nation, avg(size) from beauties group by nation order by avg(size);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hadoop-shell</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop-shell/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop-shell/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hadoop-shell&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hdfs&#34;&gt;1. hdfs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hdfs&#34;&gt;1. hdfs&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;列表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -ls /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -put /test.ee /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看文件内容&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -cat /test.ee&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;下载文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -get /test.ee /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建目录&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -mkdir -p /wordcount/input&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hadoop2.4.1集群搭建</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop2/</link>
      <pubDate>Sun, 12 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop2/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hadoop&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#__strong_centos_strong_安装&#34;&gt;1. &lt;strong&gt;CentOS&lt;/strong&gt; 安装&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_准备linux环境&#34;&gt;1.1. 准备Linux环境&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_安装_strong_jdk_strong&#34;&gt;1.2. 安装 &lt;strong&gt;JDK&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_安装_strong_hadoop2_4_1_strong&#34;&gt;1.3. 安装 &lt;strong&gt;hadoop2.4.1&lt;/strong&gt;&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_将_strong_hadoop_strong_添加到环境变量&#34;&gt;1.3.1. 将 &lt;strong&gt;hadoop&lt;/strong&gt; 添加到环境变量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_格式化_strong_namenode_strong_是对_strong_namenode_strong_进行初始化&#34;&gt;1.3.2. 格式化 &lt;strong&gt;namenode&lt;/strong&gt;（是对 &lt;strong&gt;namenode&lt;/strong&gt; 进行初始化）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_启动_strong_hadoop_strong&#34;&gt;1.3.3. 启动 &lt;strong&gt;hadoop&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_验证是否启动成功&#34;&gt;1.3.4. 验证是否启动成功&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_配置ssh免登陆&#34;&gt;1.4. 配置ssh免登陆&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_docker_方式&#34;&gt;2. docker 方式&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_compose&#34;&gt;2.1. compose&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_example&#34;&gt;3. Example&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_wordcount&#34;&gt;3.1. wordcount&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hdfs_文件上传流程&#34;&gt;4. hdfs 文件上传流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_参考&#34;&gt;5. 参考&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;__strong_centos_strong_安装&#34;&gt;1. &lt;strong&gt;CentOS&lt;/strong&gt; 安装&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_准备linux环境&#34;&gt;1.1. 准备Linux环境&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;先将虚拟机的网络模式选为NAT&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改主机名&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vi /etc/sysconfig/network

NETWORKING=yes
HOSTNAME=mini1    ###&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改IP&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/sysconfig/network-scripts/ifcfg-eth0

DEVICE=&#34;eth0&#34;
BOOTPROTO=&#34;static&#34;               ###
HWADDR=&#34;00:0C:29:3C:BF:E7&#34;
IPV6INIT=&#34;yes&#34;
NM_CONTROLLED=&#34;yes&#34;
ONBOOT=&#34;yes&#34;
TYPE=&#34;Ethernet&#34;
UUID=&#34;ce22eeca-ecde-4536-8cc2-ef0dc36d4a8c&#34;
IPADDR=&#34;192.168.1.101&#34;           ###
NETMASK=&#34;255.255.255.0&#34;          ###
GATEWAY=&#34;192.168.1.1&#34;            ###&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改主机名和IP的映射关系&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/hosts

192.168.1.101   itcast&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;关闭防火墙&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#查看防火墙状态
service iptables status
#关闭防火墙
service iptables stop
#查看防火墙开机启动状态
chkconfig iptables --list
#关闭防火墙开机启动
chkconfig iptables off&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改sudo&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su root
vim /etc/sudoers
给hadoop用户添加执行的权限&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装_strong_jdk_strong&#34;&gt;1.2. 安装 &lt;strong&gt;JDK&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;上传 &lt;strong&gt;jdk-7u_65-i585.tar.gz&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压 &lt;strong&gt;jdk&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#创建文件夹
mkdir /home/hadoop/app
#解压
tar -zxvf jdk-7u55-linux-i586.tar.gz -C /home/hadoop/app&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将 &lt;strong&gt;java&lt;/strong&gt; 添加到环境变量中&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/profile
#在文件最后添加
export JAVA_HOME=/home/hadoop/app/jdk-7u_65-i585
export PATH=$PATH:$JAVA_HOME/bin

#刷新配置
source /etc/profile&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装_strong_hadoop2_4_1_strong&#34;&gt;1.3. 安装 &lt;strong&gt;hadoop2.4.1&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&#34;admonitionblock tip&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-tip&#34; title=&#34;Tip&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
先上传 hadoop 的安装包到服务器上去 /home/hadoop/&lt;br&gt;
注意：hadoop2.x 的配置文件 $HADOOP_HOME/etc/hadoop&lt;br&gt;
伪分布式需要修改5个配置文件
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;配置 hadoop&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hadoop-env.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim hadoop-env.sh
#第27行
export JAVA_HOME=/usr/java/jdk1.7.0_65&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;core-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://weekend-1206-01:9000&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;!-- 指定hadoop运行时产生文件的存储目录 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/home/hadoop/hadoop-2.4.1/tmp&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;hdfs-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定HDFS副本的数量 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.secondary.http.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;192.168.1.152:50090&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;mapred-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mv mapred-site.xml.template mapred-site.xml
vim mapred-site.xml
&amp;lt;!-- 指定mr运行在yarn上 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;yarn-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定YARN的老大（ResourceManager）的地址 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.hostname&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;weekend-1206-01&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;!-- reducer获取数据的方式 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_将_strong_hadoop_strong_添加到环境变量&#34;&gt;1.3.1. 将 &lt;strong&gt;hadoop&lt;/strong&gt; 添加到环境变量&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/proflie

export JAVA_HOME=/usr/java/jdk1.7.0_65
export HADOOP_HOME=/itcast/hadoop-2.4.1
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

source /etc/profile&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_格式化_strong_namenode_strong_是对_strong_namenode_strong_进行初始化&#34;&gt;1.3.2. 格式化 &lt;strong&gt;namenode&lt;/strong&gt;（是对 &lt;strong&gt;namenode&lt;/strong&gt; 进行初始化）&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hdfs namenode -format
或
hadoop namenode -format&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_启动_strong_hadoop_strong&#34;&gt;1.3.3. 启动 &lt;strong&gt;hadoop&lt;/strong&gt;&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 先启动HDFS
sbin/start-dfs.sh

# 再启动YARN
sbin/start-yarn.sh

# 启动 namenode
sbin/hadoop-daemon.sh start namenode

# 启动 dataNode
sbin/hadoop-daemon.sh start datanode&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_验证是否启动成功&#34;&gt;1.3.4. 验证是否启动成功&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# jps
27408 NameNode
28218 Jps
27643 SecondaryNameNode
28066 NodeManager
27803 ResourceManager
27512 DataNode&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;http://192.168.1.101:50070 （HDFS管理界面）
http://192.168.1.101:8088 （MR管理界面）&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_配置ssh免登陆&#34;&gt;1.4. 配置ssh免登陆&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#生成ssh免登陆密钥
#进入到我的home目录
cd ~/.ssh

ssh-keygen -t rsa （四个回车）
# 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）
# 将公钥拷贝到要免密登陆的目标机器上
ssh-copy-id localhost
# ssh免登陆：
# 生成key:
ssh-keygen
# 复制从A复制到B上:
ssh-copy-id B
# 验证：
ssh localhost/exit，ps -e|grep ssh
ssh A  #在B中执行&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_docker_方式&#34;&gt;2. docker 方式&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_compose&#34;&gt;2.1. compose&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hadoop.yml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;version: &#39;2&#39;
services:
  mini1:
    image: dishui.io:5000/sequenceiq/hadoop-docker:2.4.1
    container_name: mini1
    environment:
      - &#34;HOSTNAME=mini1&#34;
    ports:
      - &#34;50070:50070&#34;
      - &#34;8088:8088&#34;
    volumes:
      - ./core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml
      - ./yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop.sh:/etc/profile.d/hadoop.sh
    command: /etc/bootstrap.sh -d
    networks:
      - hadoop
  mini2:
    image: dishui.io:5000/sequenceiq/hadoop-docker:2.4.1
    container_name: mini2
    environment:
      - &#34;HOSTNAME=mini1&#34;
    volumes:
      - ./core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml
      - ./yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop.sh:/etc/profile.d/hadoop.sh
    command: /usr/sbin/sshd -d
    networks:
      - hadoop

networks:
  hadoop:
    driver: bridge&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;core-site.xml&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;hdfs://mini1:9000&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;/hadoop-2.4.1/tmp&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ps: &lt;code&gt;cd $HADOOP_PREFIX&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;hadoop.sh&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;export PATH=$HADOOP_PREFIX/bin:HADOOP_PREFIX/sbin:$PATH&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_example&#34;&gt;3. Example&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_wordcount&#34;&gt;3.1. wordcount&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;input&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;a.txt&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;a
b
abc
ef
efg
abc
ef
h
aakk
ef
h&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传到 &lt;strong&gt;HDFS&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 目录不存在,创建目录 ( `hadoop fs -mkdir -p /wordcount/input` )
hadoop fs -put /a.txt /wordcount/input&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hdfs_文件上传流程&#34;&gt;4. hdfs 文件上传流程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop-upload.svg&#34; alt=&#34;hadoop upload&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_参考&#34;&gt;5. 参考&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kiwenlau/hadoop-cluster-docker&#34;&gt;hadoop-cluster-docker&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>