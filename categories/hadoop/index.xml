<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>工作笔记</title>
    <link>http://dishui.oschina.io/note-hugo/categories/hadoop/index.xml</link>
    <description>Recent content on 工作笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <atom:link href="http://dishui.oschina.io/note-hugo/categories/hadoop/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>hive详解</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive%E8%AF%A6%E8%A7%A3/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hive详解&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hive简介&#34;&gt;1. Hive简介&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_什么是hive&#34;&gt;1.1. 什么是Hive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_为什么使用hive&#34;&gt;1.2. 为什么使用Hive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive的特点&#34;&gt;1.3. Hive的特点&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#__strong_hive_strong_架构&#34;&gt;2. &lt;strong&gt;Hive&lt;/strong&gt; 架构&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_基本组成&#34;&gt;2.1. 基本组成&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_各组件的基本功能&#34;&gt;2.2. 各组件的基本功能&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive与hadoop的关系&#34;&gt;3. Hive与Hadoop的关系&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive与传统数据库对比&#34;&gt;4. Hive与传统数据库对比&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_对比&#34;&gt;4.1. 对比&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive的数据存储&#34;&gt;5. Hive的数据存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#__strong_hive_strong_的安装部署&#34;&gt;6. &lt;strong&gt;HIVE&lt;/strong&gt; 的安装部署&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_使用方式&#34;&gt;6.1. 使用方式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive基本操作&#34;&gt;7. Hive基本操作&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_ddl操作&#34;&gt;7.1. DDL操作&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_创建表&#34;&gt;7.1.1. 创建表&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_具体实例&#34;&gt;具体实例&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_修改表&#34;&gt;7.1.2. 修改表&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_增加_删除分区&#34;&gt;增加/删除分区&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_重命名表&#34;&gt;重命名表&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_增加_更新列&#34;&gt;增加/更新列&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_显示命令&#34;&gt;显示命令&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_dml操作&#34;&gt;7.1.3. DML操作&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_load&#34;&gt;Load&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_insert&#34;&gt;Insert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_select&#34;&gt;SELECT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive_join&#34;&gt;7.1.4. Hive Join&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive简介&#34;&gt;1. Hive简介&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_什么是hive&#34;&gt;1.1. 什么是Hive&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_为什么使用hive&#34;&gt;1.2. 为什么使用Hive&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;直接使用hadoop所面临的问题 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;人员学习成本太高&lt;br&gt;
项目周期要求太短&lt;br&gt;
MapReduce实现复杂查询逻辑开发难度太大&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;为什么要使用Hive &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;避免了去写MapReduce，减少开发人员的学习成本。+
扩展功能很方便。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_hive的特点&#34;&gt;1.3. Hive的特点&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;可扩展 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Hive可以自由的扩展集群的规模，一般情况下不需要重启服务。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;延展性 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;容错 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;良好的容错性，节点出现问题SQL仍可完成执行&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;__strong_hive_strong_架构&#34;&gt;2. &lt;strong&gt;Hive&lt;/strong&gt; 架构&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/2017-03-17_155621.png&#34; alt=&#34;2017 03 17 155621&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Jobtracker &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;hadoop1.x中的组件，它的功能相当于： Resourcemanager+AppMaster&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;TaskTracker &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;相当于：  Nodemanager  +  yarnchild&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_基本组成&#34;&gt;2.1. 基本组成&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;用户接口&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;包括 CLI、JDBC/ODBC、WebGUI。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;元数据存储&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;通常是存储在关系数据库如 mysql,derby中。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;解释器、编译器、优化器、执行器&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_各组件的基本功能&#34;&gt;2.2. 各组件的基本功能&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;用户接口主要由三个&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;CLI、JDBC/ODBC和WebGUI。其中，CLI为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;元数据存储 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;解释器、编译器、优化器 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive与hadoop的关系&#34;&gt;3. Hive与Hadoop的关系&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Hive利用HDFS存储数据，利用MapReduce查询数据&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/2017-03-17_160404.png&#34; alt=&#34;2017 03 17 160404&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive与传统数据库对比&#34;&gt;4. Hive与传统数据库对比&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/2017-03-17_160454.png&#34; alt=&#34;2017 03 17 160454&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_对比&#34;&gt;4.1. 对比&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;查询语言。由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据存储位置。Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据格式。Hive 中没有定义专门的数据格式，数据格式可以由用户指定，用户定义数据格式需要指定三个属性：列分隔符（通常为空格、”\t”、”\x001″）、行分隔符（”\n”）以及读取文件数据的方法（Hive 中默认有三个文件格式 TextFile，SequenceFile 以及 RCFile）。由于在加载数据的过程中，不需要从用户数据格式到 Hive 定义的数据格式的转换，因此，Hive 在加载的过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的 HDFS 目录中。而在数据库中，不同的数据库有不同的存储引擎，定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据更新。由于 Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive 中不支持对数据的改写和添加，所有的数据都是在加载的时候中确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO &amp;#8230;&amp;#8203;  VALUES 添加数据，使用 UPDATE &amp;#8230;&amp;#8203; SET 修改数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;索引。之前已经说过，Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 Key&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive的数据存储&#34;&gt;5. Hive的数据存储&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hive&lt;/strong&gt; 中所有的数据都存储在 &lt;strong&gt;HDFS&lt;/strong&gt;  中，没有专门的数据存储格式（可支持*Text* ，&lt;strong&gt;SequenceFile&lt;/strong&gt; ，&lt;strong&gt;ParquetFile&lt;/strong&gt; ，&lt;strong&gt;RCFILE&lt;/strong&gt; 等）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;只需要在创建表的时候告诉 &lt;strong&gt;Hive&lt;/strong&gt;  数据中的列分隔符和行分隔符，&lt;strong&gt;Hive&lt;/strong&gt;  就可以解析数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hive&lt;/strong&gt;  中包含以下数据模型：&lt;strong&gt;DB&lt;/strong&gt; 、&lt;strong&gt;Table&lt;/strong&gt; ，&lt;strong&gt;External&lt;/strong&gt;  &lt;strong&gt;Table&lt;/strong&gt; ，&lt;strong&gt;Partition&lt;/strong&gt; ，&lt;strong&gt;Bucket&lt;/strong&gt; 。&lt;/p&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;db&lt;/strong&gt; ：在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现为${&lt;strong&gt;hive&lt;/strong&gt; .&lt;strong&gt;metastore&lt;/strong&gt; .&lt;strong&gt;warehouse&lt;/strong&gt; .&lt;strong&gt;dir&lt;/strong&gt; }目录下一个文件夹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;table&lt;/strong&gt; ：在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现所属*db* 目录下一个文件夹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;external table&lt;/strong&gt; ：外部表, 与*table* 类似，不过其数据存放位置可以在任意指定路径
普通表: 删除表后, &lt;strong&gt;hdfs&lt;/strong&gt; 上的文件都删了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;External&lt;/strong&gt; 外部表删除后, &lt;strong&gt;hdfs&lt;/strong&gt; 上的文件没有删除, 只是把文件删除了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;partition&lt;/strong&gt; ：在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现为 &lt;strong&gt;table&lt;/strong&gt; 目录下的子目录&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;bucket&lt;/strong&gt; ：桶, 在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现为同一个表目录下根据 &lt;strong&gt;hash&lt;/strong&gt; 散列之后的多个文件, 会根据不同的文件把数据放到不同的文件中&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;__strong_hive_strong_的安装部署&#34;&gt;6. &lt;strong&gt;HIVE&lt;/strong&gt; 的安装部署&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive&#34;&gt;hive 安装&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_使用方式&#34;&gt;6.1. 使用方式&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Hive交互shell&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hive&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hive thrift服务&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;启动方式，（假如是在hadoop01上）&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;启动为前台 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hiveserver2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;启动为后台 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;nohup bin/hiveserver2 1&amp;gt;/var/log/hiveserver.log 2&amp;gt;/var/log/hiveserver.err &amp;amp;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;启动成功后，可以在别的节点上用beeline去连接&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;方式（1）&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;code&gt;hive/bin/beeline&lt;/code&gt;  回车，进入beeline的命令界面&lt;br&gt;
输入命令连接 &lt;code&gt;hiveserver2&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;beeline&amp;gt; !connect jdbc:hive2//mini1:10000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;（hadoop01是hiveserver2所启动的那台主机名，端口默认是10000）&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;方式（2） &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;或者启动就连接：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/beeline -u jdbc:hive2://mini1:10000 -n hadoop&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;接下来就可以做正常sql查询了&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive基本操作&#34;&gt;7. Hive基本操作&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_ddl操作&#34;&gt;7.1. DDL操作&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_创建表&#34;&gt;7.1.1. 创建表&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;建表语法&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name
   [(col_name data_type [COMMENT col_comment], ...)]
   [COMMENT table_comment]
   [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
   [CLUSTERED BY (col_name, col_name, ...)
   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
   [ROW FORMAT row_format]
   [STORED AS file_format]
   [LOCATION hdfs_path]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CREATE&lt;/strong&gt; &lt;strong&gt;TABLE&lt;/strong&gt; 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 &lt;strong&gt;IF&lt;/strong&gt; &lt;strong&gt;NOT&lt;/strong&gt; &lt;strong&gt;EXISTS&lt;/strong&gt; 选项来忽略这个异常。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;EXTERNAL&lt;/strong&gt; 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（&lt;strong&gt;LOCATION&lt;/strong&gt;），&lt;strong&gt;Hive&lt;/strong&gt; 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LIKE&lt;/strong&gt; 允许用户复制现有的表结构，但是不复制数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ROW&lt;/strong&gt; &lt;strong&gt;FORMAT&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;用户在建表的时候可以自定义 &lt;strong&gt;SerDe&lt;/strong&gt; 或者使用自带的 &lt;strong&gt;SerDe&lt;/strong&gt;。如果没有指定 &lt;strong&gt;ROW&lt;/strong&gt; &lt;strong&gt;FORMAT&lt;/strong&gt; 或者 &lt;strong&gt;ROW&lt;/strong&gt; &lt;strong&gt;FORMAT&lt;/strong&gt; &lt;strong&gt;DELIMITED&lt;/strong&gt;，将会使用自带的 &lt;strong&gt;SerDe&lt;/strong&gt;。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 &lt;strong&gt;SerDe&lt;/strong&gt;，&lt;strong&gt;Hive*通过 *SerDe&lt;/strong&gt; 确定表的具体的列的数据。&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;STORED&lt;/strong&gt; &lt;strong&gt;AS&lt;/strong&gt;
&lt;strong&gt;SEQUENCEFILE&lt;/strong&gt;|&lt;strong&gt;TEXTFILE&lt;/strong&gt;|&lt;strong&gt;RCFILE&lt;/strong&gt;
如果文件数据是纯文本，可以使用 &lt;strong&gt;STORED&lt;/strong&gt; &lt;strong&gt;AS&lt;/strong&gt; &lt;strong&gt;TEXTFILE&lt;/strong&gt;。如果数据需要压缩，使用 &lt;strong&gt;STORED&lt;/strong&gt; &lt;strong&gt;AS&lt;/strong&gt; &lt;strong&gt;SEQUENCEFILE&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLUSTERED&lt;/strong&gt; &lt;strong&gt;BY&lt;/strong&gt;
对于每一个表（&lt;strong&gt;table&lt;/strong&gt;）或者分区， &lt;strong&gt;Hive&lt;/strong&gt; 可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。&lt;strong&gt;Hive&lt;/strong&gt; 也是 针对某一列进行桶的组织。&lt;strong&gt;Hive&lt;/strong&gt; 采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。
把表（或者分区）组织成桶（&lt;strong&gt;Bucket&lt;/strong&gt;）有两个理由：&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;获得更高的查询处理效率。桶为表加上了额外的结构，&lt;strong&gt;Hive&lt;/strong&gt; 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 &lt;strong&gt;Map&lt;/strong&gt; 端连接 （&lt;strong&gt;Map&lt;/strong&gt;-&lt;strong&gt;side&lt;/strong&gt; &lt;strong&gt;join&lt;/strong&gt;）高效的实现。比如 &lt;strong&gt;JOIN&lt;/strong&gt; 操作。对于 &lt;strong&gt;JOIN&lt;/strong&gt; 操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行 &lt;strong&gt;JOIN&lt;/strong&gt; 操作就可以，可以大大较少 &lt;strong&gt;JOIN&lt;/strong&gt; 的数据量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使取样（&lt;strong&gt;sampling&lt;/strong&gt;）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_具体实例&#34;&gt;具体实例&lt;/h5&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建内部表mytable&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image010.png&#34; alt=&#34;image010&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建外部表pageview&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image012.png&#34; alt=&#34;image012&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建分区表invites&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;create table student_p(Sno int,Sname string,Sex string,Sage int,Sdept string) partitioned by(part string) row format delimited fields terminated by &#39;,&#39;stored as textfile;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image014.png&#34; alt=&#34;image014&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建带桶的表student&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image016.png&#34; alt=&#34;image016&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_修改表&#34;&gt;7.1.2. 修改表&lt;/h4&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_增加_删除分区&#34;&gt;增加/删除分区&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION &#39;location1&#39; ] partition_spec [ LOCATION &#39;location2&#39; ] ...
partition_spec:
: PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)

ALTER TABLE table_name DROP partition_spec, partition_spec,...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;alter table student_p add partition(part=&#39;a&#39;) partition(part=&#39;b&#39;);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image018.png&#34; alt=&#34;image018&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image020.png&#34; alt=&#34;image020&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_重命名表&#34;&gt;重命名表&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name RENAME TO new_table_name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image022.png&#34; alt=&#34;image022&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_增加_更新列&#34;&gt;增加/更新列&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image024.png&#34; alt=&#34;image024&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_显示命令&#34;&gt;显示命令&lt;/h5&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;show tables
show databases
show partitions
show functions
desc extended t_name;
desc formatted table_name;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_dml操作&#34;&gt;7.1.3. DML操作&lt;/h4&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_load&#34;&gt;Load&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO
TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;说明&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;filepath&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;相对路径，例如：&lt;strong&gt;project/data1&lt;/strong&gt;&lt;br&gt;
绝对路径，例如：&lt;strong&gt;/user/hive/project/data1&lt;/strong&gt;&lt;br&gt;
包含模式的完整 URI，列如：+
&lt;strong&gt;hdfs://namenode:9000/user/hive/project/data1&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LOCAL关键字&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。&lt;br&gt;
如果没有指定 LOCAL 关键字，则根据inpath中的uri 查找文件&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OVERWRITE 关键字&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。&lt;br&gt;
如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;加载相对路径数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image026.png&#34; alt=&#34;image026&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加载绝对路径数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image028.png&#34; alt=&#34;image028&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加载包含模式数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image030.png&#34; alt=&#34;image030&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OVERWRITE关键字使用&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image032.png&#34; alt=&#34;image032&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_insert&#34;&gt;Insert&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement

Multiple inserts:
FROM from_statement
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1
[INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...

Dynamic partition inserts:
INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;基本模式插入&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image034.png&#34; alt=&#34;image034&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多插入模式&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image036.png&#34; alt=&#34;image036&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自动分区模式&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image038.png&#34; alt=&#34;image038&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect5&#34;&gt;
&lt;h6 id=&#34;_导出表数据&#34;&gt;导出表数据&lt;/h6&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...


multiple inserts:
FROM from_statement
INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1
[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;导出文件到本地&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image040.png&#34; alt=&#34;image040&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：
数据写入到文件系统时进行文本序列化，且每列用^A来区分，\n为换行符。用more命令查看时不容易看出分割符，可以使用:sed -e &#39;s/\x01/|/g&#39; filename 来查看。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;导出数据到HDFS&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image042.png&#34; alt=&#34;image042&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_select&#34;&gt;SELECT&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list [HAVING condition]]
[CLUSTER BY col_list
  | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]
]
[LIMIT number]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注：
. order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。
. sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&amp;gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。
. distribute by根据distribute by指定的内容将数据分到同一个reducer。
. Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;获取年龄大的3个学生&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image044.png&#34; alt=&#34;image044&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查询学生信息按年龄，降序排序&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image046.png&#34; alt=&#34;image046&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image048.png&#34; alt=&#34;image048&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image050.png&#34; alt=&#34;image050&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image052.png&#34; alt=&#34;image052&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;按学生名称汇总学生年龄&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image0.png&#34; alt=&#34;image0&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_hive_join&#34;&gt;7.1.4. Hive Join&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;join_table:
  table_reference JOIN table_factor [join_condition]
  | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition
  | table_reference LEFT SEMI JOIN table_reference join_condition&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Hive 支持等值连接（equality joins）、外连接（outer joins）和（left/right joins）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。
另外，Hive 支持多于 2 个表的连接。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;写 join 查询时，需要注意几个关键点：
1. 只支持等值join
例如：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;  SELECT a.* FROM a JOIN b ON (a.id = b.id)
  SELECT a.* FROM a JOIN b
    ON (a.id = b.id AND a.department = b.department)
是正确的，然而:
  SELECT a.* FROM a JOIN b ON (a.id&amp;gt;b.id)
是错误的。&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;可以 join 多于 2 个表。
例如
  SELECT a.val, b.val, c.val FROM a JOIN b
    ON (a.key = b.key1) JOIN c ON (c.key = b.key2)
如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，例如：
  SELECT a.val, b.val, c.val FROM a JOIN b
    ON (a.key = b.key1) JOIN c
    ON (c.key = b.key1)
被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key。
SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1)
  JOIN c ON (c.key = b.key2)
而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 join。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;3．join 时，每次 map/reduce 任务的逻辑：
    reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如：
SELECT a.val, b.val, c.val FROM a
    JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)
所有表都使用同一个 join key（使用 1 次 map/reduce 任务计算）。Reduce 端会缓存 a 表和 b 表的记录，然后每次取得一个 c 表的记录就计算一次 join 结果，类似的还有：
  SELECT a.val, b.val, c.val FROM a
    JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)
这里用了 2 次 map/reduce 任务。第一次缓存 a 表，用 b 表序列化；第二次缓存第一次 map/reduce 任务的结果，然后用 c 表序列化。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;4．LEFT，RIGHT 和 FULL OUTER 关键字用于处理 join 中空记录的情况
例如：
  SELECT a.val, b.val FROM
a LEFT OUTER JOIN b ON (a.key=b.key)
对应所有 a 表中的记录都有一条记录输出。输出的结果应该是 a.val, b.val，当 a.key=b.key 时，而当 b.key 中找不到等值的 a.key 记录时也会输出:
a.val, NULL
所以 a 表中的所有记录都被保留了；
“a RIGHT OUTER JOIN b”会保留所有 b 表的记录。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Join 发生在 WHERE 子句之前。如果你想限制 join 的输出，应该在 WHERE 子句中写过滤条件——或是在 join 子句中写。这里面一个容易混淆的问题是表分区的情况：
  SELECT a.val, b.val FROM a
  LEFT OUTER JOIN b ON (a.key=b.key)
  WHERE a.ds=&#39;2009-07-07&#39; AND b.ds=&#39;2009-07-07&#39;
会 join a 表到 b 表（OUTER JOIN），列出 a.val 和 b.val 的记录。WHERE 从句中可以使用其他列作为过滤条件。但是，如前所述，如果 b 表中找不到对应 a 表的记录，b 表的所有列都会列出 NULL，包括 ds 列。也就是说，join 会过滤 b 表中不能找到匹配 a 表 join key 的所有记录。这样的话，LEFT OUTER 就使得查询结果与 WHERE 子句无关了。解决的办法是在 OUTER JOIN 时使用以下语法：
  SELECT a.val, b.val FROM a LEFT OUTER JOIN b
  ON (a.key=b.key AND
      b.ds=&#39;2009-07-07&#39; AND
      a.ds=&#39;2009-07-07&#39;)
这一查询的结果是预先在 join 阶段过滤过的，所以不会存在上述问题。这一逻辑也可以应用于 RIGHT 和 FULL 类型的 join 中。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hive</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hive&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hive只在一个节点上安装即可&#34;&gt;1. Hive只在一个节点上安装即可&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive只在一个节点上安装即可&#34;&gt;1. Hive只在一个节点上安装即可&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;上传tar包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf hive-1.2.1.tar.gz -C /hadoop/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装mysql数据库（切换到root用户）（装在哪里没有限制，只有能联通hadoop集群的节点）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;mysql.yml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;version: &#39;2&#39;
services:
  mysql:
    image: dishui.io:5000/mysql:5.5.52
    container_name: mysql
    environment:
      - &#34;MYSQL_ROOT_PASSWORD=111111&#34;
    ports:
      - &#34;3306:3306&#34;
    networks:
      - hadoop
networks:
  hadoop:
    external: true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置 &lt;strong&gt;hive&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;配置 &lt;strong&gt;HIVE_HOME&lt;/strong&gt; 环境变量  &lt;strong&gt;vi conf/hive-env.sh&lt;/strong&gt; 配置其中的 &lt;strong&gt;$hadoop_home&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置元数据库信息   &lt;strong&gt;vi hive-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;jdbc:mysql://mysql:3306/hive?createDatabaseIfNotExist=true&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;JDBC connect string for a JDBC metastore&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;Driver class name for a JDBC metastore&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;root&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;username to use against metastore database&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;111111&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;password to use against metastore database&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装 &lt;strong&gt;hive&lt;/strong&gt; 和 &lt;strong&gt;mysql&lt;/strong&gt; 完成后，将 &lt;strong&gt;mysql&lt;/strong&gt; 的连接 &lt;strong&gt;jar&lt;/strong&gt; 包拷贝到 &lt;strong&gt;$HIVE_HOME/lib&lt;/strong&gt; 目录下&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果出现没有权限的问题，在 &lt;strong&gt;mysql&lt;/strong&gt; 授权(在安装 &lt;strong&gt;mysql&lt;/strong&gt; 的机器上执行)&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mysql -uroot -p&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;执行下面的语句  &lt;strong&gt;.&lt;/strong&gt;:所有库下的所有表   %：任何IP地址或主机都可以连接&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;111111&#39; WITH GRANT OPTION;
FLUSH PRIVILEGES;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Jline&lt;/strong&gt; 包版本不一致的问题，需要拷贝 &lt;strong&gt;hive&lt;/strong&gt; 的 &lt;strong&gt;lib&lt;/strong&gt; 目录中 &lt;strong&gt;jline.2.12.jar&lt;/strong&gt; 的 &lt;strong&gt;jar&lt;/strong&gt; 包替换掉 &lt;strong&gt;hadoop&lt;/strong&gt; 中的
&lt;strong&gt;/home/hadoop/app/hadoop-2.6.4/share/hadoop/yarn/lib/jline-0.9.94.jar&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动 &lt;strong&gt;hive&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hive&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;建表(默认是内部表)&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;create table trade_detail(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39;;
# 建分区表
create table td_part(id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by &#39;\t&#39;;
# 建外部表
create external table td_ext(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39; location &#39;/td_ext&#39;;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建分区表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 普通表和分区表区别：有大量数据增加的需要建分区表
create table book (id bigint, name string) partitioned by (pubdate string) row format delimited fields terminated by &#39;\t&#39;;

# 分区表加载数据
load data local inpath &#39;./book.txt&#39; overwrite into table book partition (pubdate=&#39;2010-08-22&#39;);

load data local inpath &#39;/root/data.am&#39; into table beauty partition (nation=&#34;USA&#34;);

select nation, avg(size) from beauties group by nation order by avg(size);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hadoop-shell</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop-shell/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop-shell/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hadoop-shell&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hdfs&#34;&gt;1. hdfs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hdfs&#34;&gt;1. hdfs&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;列表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -ls /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -put /test.ee /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看文件内容&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -cat /test.ee&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;下载文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -get /test.ee /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建目录&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -mkdir -p /wordcount/input&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hadoop2.4.1集群搭建</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop2/</link>
      <pubDate>Sun, 12 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop2/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hadoop&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#__strong_centos_strong_安装&#34;&gt;1. &lt;strong&gt;CentOS&lt;/strong&gt; 安装&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_准备linux环境&#34;&gt;1.1. 准备Linux环境&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_安装_strong_jdk_strong&#34;&gt;1.2. 安装 &lt;strong&gt;JDK&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_安装_strong_hadoop2_4_1_strong&#34;&gt;1.3. 安装 &lt;strong&gt;hadoop2.4.1&lt;/strong&gt;&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_将_strong_hadoop_strong_添加到环境变量&#34;&gt;1.3.1. 将 &lt;strong&gt;hadoop&lt;/strong&gt; 添加到环境变量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_格式化_strong_namenode_strong_是对_strong_namenode_strong_进行初始化&#34;&gt;1.3.2. 格式化 &lt;strong&gt;namenode&lt;/strong&gt;（是对 &lt;strong&gt;namenode&lt;/strong&gt; 进行初始化）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_启动_strong_hadoop_strong&#34;&gt;1.3.3. 启动 &lt;strong&gt;hadoop&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_验证是否启动成功&#34;&gt;1.3.4. 验证是否启动成功&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_配置ssh免登陆&#34;&gt;1.4. 配置ssh免登陆&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_docker_方式&#34;&gt;2. docker 方式&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_compose&#34;&gt;2.1. compose&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_example&#34;&gt;3. Example&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_wordcount&#34;&gt;3.1. wordcount&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hdfs_文件上传流程&#34;&gt;4. hdfs 文件上传流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_参考&#34;&gt;5. 参考&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;__strong_centos_strong_安装&#34;&gt;1. &lt;strong&gt;CentOS&lt;/strong&gt; 安装&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_准备linux环境&#34;&gt;1.1. 准备Linux环境&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;先将虚拟机的网络模式选为NAT&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改主机名&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vi /etc/sysconfig/network

NETWORKING=yes
HOSTNAME=mini1    ###&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改IP&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/sysconfig/network-scripts/ifcfg-eth0

DEVICE=&#34;eth0&#34;
BOOTPROTO=&#34;static&#34;               ###
HWADDR=&#34;00:0C:29:3C:BF:E7&#34;
IPV6INIT=&#34;yes&#34;
NM_CONTROLLED=&#34;yes&#34;
ONBOOT=&#34;yes&#34;
TYPE=&#34;Ethernet&#34;
UUID=&#34;ce22eeca-ecde-4536-8cc2-ef0dc36d4a8c&#34;
IPADDR=&#34;192.168.1.101&#34;           ###
NETMASK=&#34;255.255.255.0&#34;          ###
GATEWAY=&#34;192.168.1.1&#34;            ###&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改主机名和IP的映射关系&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/hosts

192.168.1.101   itcast&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;关闭防火墙&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#查看防火墙状态
service iptables status
#关闭防火墙
service iptables stop
#查看防火墙开机启动状态
chkconfig iptables --list
#关闭防火墙开机启动
chkconfig iptables off&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改sudo&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su root
vim /etc/sudoers
给hadoop用户添加执行的权限&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装_strong_jdk_strong&#34;&gt;1.2. 安装 &lt;strong&gt;JDK&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;上传 &lt;strong&gt;jdk-7u_65-i585.tar.gz&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压 &lt;strong&gt;jdk&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#创建文件夹
mkdir /home/hadoop/app
#解压
tar -zxvf jdk-7u55-linux-i586.tar.gz -C /home/hadoop/app&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将 &lt;strong&gt;java&lt;/strong&gt; 添加到环境变量中&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/profile
#在文件最后添加
export JAVA_HOME=/home/hadoop/app/jdk-7u_65-i585
export PATH=$PATH:$JAVA_HOME/bin

#刷新配置
source /etc/profile&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装_strong_hadoop2_4_1_strong&#34;&gt;1.3. 安装 &lt;strong&gt;hadoop2.4.1&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&#34;admonitionblock tip&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-tip&#34; title=&#34;Tip&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
先上传 hadoop 的安装包到服务器上去 /home/hadoop/&lt;br&gt;
注意：hadoop2.x 的配置文件 $HADOOP_HOME/etc/hadoop&lt;br&gt;
伪分布式需要修改5个配置文件
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;配置 hadoop&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hadoop-env.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim hadoop-env.sh
#第27行
export JAVA_HOME=/usr/java/jdk1.7.0_65&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;core-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://weekend-1206-01:9000&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;!-- 指定hadoop运行时产生文件的存储目录 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/home/hadoop/hadoop-2.4.1/tmp&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;hdfs-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定HDFS副本的数量 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.secondary.http.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;192.168.1.152:50090&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;mapred-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mv mapred-site.xml.template mapred-site.xml
vim mapred-site.xml
&amp;lt;!-- 指定mr运行在yarn上 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;yarn-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定YARN的老大（ResourceManager）的地址 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.hostname&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;weekend-1206-01&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;!-- reducer获取数据的方式 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_将_strong_hadoop_strong_添加到环境变量&#34;&gt;1.3.1. 将 &lt;strong&gt;hadoop&lt;/strong&gt; 添加到环境变量&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/proflie

export JAVA_HOME=/usr/java/jdk1.7.0_65
export HADOOP_HOME=/itcast/hadoop-2.4.1
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

source /etc/profile&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_格式化_strong_namenode_strong_是对_strong_namenode_strong_进行初始化&#34;&gt;1.3.2. 格式化 &lt;strong&gt;namenode&lt;/strong&gt;（是对 &lt;strong&gt;namenode&lt;/strong&gt; 进行初始化）&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hdfs namenode -format
或
hadoop namenode -format&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_启动_strong_hadoop_strong&#34;&gt;1.3.3. 启动 &lt;strong&gt;hadoop&lt;/strong&gt;&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 先启动HDFS
sbin/start-dfs.sh

# 再启动YARN
sbin/start-yarn.sh

# 启动 namenode
sbin/hadoop-daemon.sh start namenode

# 启动 dataNode
sbin/hadoop-daemon.sh start datanode&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_验证是否启动成功&#34;&gt;1.3.4. 验证是否启动成功&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# jps
27408 NameNode
28218 Jps
27643 SecondaryNameNode
28066 NodeManager
27803 ResourceManager
27512 DataNode&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;http://192.168.1.101:50070 （HDFS管理界面）
http://192.168.1.101:8088 （MR管理界面）&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_配置ssh免登陆&#34;&gt;1.4. 配置ssh免登陆&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#生成ssh免登陆密钥
#进入到我的home目录
cd ~/.ssh

ssh-keygen -t rsa （四个回车）
# 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）
# 将公钥拷贝到要免密登陆的目标机器上
ssh-copy-id localhost
# ssh免登陆：
# 生成key:
ssh-keygen
# 复制从A复制到B上:
ssh-copy-id B
# 验证：
ssh localhost/exit，ps -e|grep ssh
ssh A  #在B中执行&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_docker_方式&#34;&gt;2. docker 方式&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_compose&#34;&gt;2.1. compose&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hadoop.yml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;version: &#39;2&#39;
services:
  mini1:
    image: dishui.io:5000/sequenceiq/hadoop-docker:2.4.1
    container_name: mini1
    environment:
      - &#34;HOSTNAME=mini1&#34;
    ports:
      - &#34;50070:50070&#34;
      - &#34;8088:8088&#34;
    volumes:
      - ./core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml
      - ./yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop.sh:/etc/profile.d/hadoop.sh
    command: /etc/bootstrap.sh -d
    networks:
      - hadoop
  mini2:
    image: dishui.io:5000/sequenceiq/hadoop-docker:2.4.1
    container_name: mini2
    environment:
      - &#34;HOSTNAME=mini1&#34;
    volumes:
      - ./core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml
      - ./yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop.sh:/etc/profile.d/hadoop.sh
    command: /usr/sbin/sshd -d
    networks:
      - hadoop

networks:
  hadoop:
    driver: bridge&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;core-site.xml&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;hdfs://mini1:9000&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;/hadoop-2.4.1/tmp&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ps: &lt;code&gt;cd $HADOOP_PREFIX&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;hadoop.sh&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;export PATH=$HADOOP_PREFIX/bin:HADOOP_PREFIX/sbin:$PATH&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_example&#34;&gt;3. Example&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_wordcount&#34;&gt;3.1. wordcount&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;input&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;a.txt&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;a
b
abc
ef
efg
abc
ef
h
aakk
ef
h&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传到 &lt;strong&gt;HDFS&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 目录不存在,创建目录 ( `hadoop fs -mkdir -p /wordcount/input` )
hadoop fs -put /a.txt /wordcount/input&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hdfs_文件上传流程&#34;&gt;4. hdfs 文件上传流程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop-upload.svg&#34; alt=&#34;hadoop upload&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_参考&#34;&gt;5. 参考&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kiwenlau/hadoop-cluster-docker&#34;&gt;hadoop-cluster-docker&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>