<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>零零散散</title>
    <link>/index.xml</link>
    <description>Recent content on 零零散散</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 05 May 2017 15:39:55 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>tmp</title>
      <link>/tmp/tmp2017-01-08/</link>
      <pubDate>Fri, 05 May 2017 15:39:55 +0000</pubDate>
      
      <guid>/tmp/tmp2017-01-08/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Contents&lt;/div&gt;
&lt;ul class=&#34;sectlevel0&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_settings_to_quiet_third_party_logs_that_are_too_verbose&#34;&gt;Settings to quiet third party logs that are too verbose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_spark_9183_settings_to_avoid_annoying_messages_when_looking_up_nonexistent_udfs_in_sparksql_with_hive_support&#34;&gt;SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;上传日志文件到hdfs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;日志预处理,日志点击流
.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;command.1=sh log_click.sh ${HADOOP_PRE} ${FILE_NAME}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;hadoop fs -rm -r /data/weblog/preprocess/valid_output/2017-05-12&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;hadoop fs -ls /data/weblog/preprocess/valid_output/2017-05-12&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cat &amp;gt; /etc/apt/sources.list &amp;lt;&amp;lt;_EOF_
deb &lt;a href=&#34;http://mirrors.aliyun.com/ubuntu/&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/ubuntu/&lt;/a&gt; trusty main restricted universe multiverse
deb &lt;a href=&#34;http://mirrors.aliyun.com/ubuntu/&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/ubuntu/&lt;/a&gt; trusty-security main restricted universe multiverse
deb &lt;a href=&#34;http://mirrors.aliyun.com/ubuntu/&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/ubuntu/&lt;/a&gt; trusty-updates main restricted universe multiverse
deb &lt;a href=&#34;http://mirrors.aliyun.com/ubuntu/&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/ubuntu/&lt;/a&gt; trusty-proposed main restricted universe multiverse
deb &lt;a href=&#34;http://mirrors.aliyun.com/ubuntu/&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/ubuntu/&lt;/a&gt; trusty-backports main restricted universe multiverse
deb-src &lt;a href=&#34;http://mirrors.aliyun.com/ubuntu/&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/ubuntu/&lt;/a&gt; trusty main restricted universe multiverse
deb-src &lt;a href=&#34;http://mirrors.aliyun.com/ubuntu/&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/ubuntu/&lt;/a&gt; trusty-security main restricted universe multiverse
deb-src &lt;a href=&#34;http://mirrors.aliyun.com/ubuntu/&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/ubuntu/&lt;/a&gt; trusty-updates main restricted universe multiverse
deb-src &lt;a href=&#34;http://mirrors.aliyun.com/ubuntu/&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/ubuntu/&lt;/a&gt; trusty-proposed main restricted universe multiverse
deb-src &lt;a href=&#34;http://mirrors.aliyun.com/ubuntu/&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/ubuntu/&lt;/a&gt; trusty-backports main restricted universe multiverse
&lt;em&gt;EOF&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cat &amp;gt; /etc/apt/sources.list &amp;lt;&amp;lt;_EOF_
deb &lt;a href=&#34;http://mirrors.aliyun.com/debian&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/debian&lt;/a&gt; wheezy main contrib non-free
deb-src &lt;a href=&#34;http://mirrors.aliyun.com/debian&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/debian&lt;/a&gt; wheezy main contrib non-free
deb &lt;a href=&#34;http://mirrors.aliyun.com/debian&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/debian&lt;/a&gt; wheezy-updates main contrib non-free
deb-src &lt;a href=&#34;http://mirrors.aliyun.com/debian&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/debian&lt;/a&gt; wheezy-updates main contrib non-free
deb &lt;a href=&#34;http://mirrors.aliyun.com/debian-security&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/debian-security&lt;/a&gt; wheezy/updates main contrib non-free
deb-src &lt;a href=&#34;http://mirrors.aliyun.com/debian-security&#34; class=&#34;bare&#34;&gt;http://mirrors.aliyun.com/debian-security&lt;/a&gt; wheezy/updates main contrib non-free
&lt;em&gt;EOF&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker exec hadoop-master /bin/sh -c &#39;echo 1&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;/usr/bin/docker exec hadoop-master /usr/local/hadoop/bin/hadoop jar hadoop-demo-1.0-SNAPSHOT.jar wordcount /wordcount/input /wordcount/output&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.7.2-sources.jar org.apache.hadoop.examples.WordCount /wordcount/input /wordcount/output&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;相当于一个yarn集群的客户端
需要在此封装我们的mr程序的相关运行参数
指定jar包,最后提交给yarn&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;将job中配置的相关参数,以及job所用的java类所在的jar包,提交给yarn去运行&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;type&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;运营场景
决策场景&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;调度&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;数据仓库&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;数据建模&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;业务场景接口&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ImmutableListMultimap&amp;lt;String, ?&amp;gt; cktmp = Multimaps.index((List&amp;lt;ZhAsphaltRepertoryModel&amp;gt;)ckList, multiTypeFunction);
ImmutableListMultimap&amp;lt;String, ?&amp;gt; sctmp = Multimaps.index((List&amp;lt;ZhAsphaltRepertoryModel&amp;gt;)scList, multiTypeFunction);
ImmutableListMultimap&amp;lt;String, ?&amp;gt; dgtmp = Multimaps.index((List&amp;lt;ZhAsphaltRepertoryModel&amp;gt;)dgList, multiTypeFunction);
ImmutableListMultimap&amp;lt;String, ?&amp;gt; pdtmp = Multimaps.index((List&amp;lt;ZhAsphaltRepertoryModel&amp;gt;)pdList, multiTypeFunction);
ImmutableListMultimap&amp;lt;String, ?&amp;gt; kctmp = Multimaps.index((List&amp;lt;ZhAsphaltRepertoryModel&amp;gt;)kcList, multiTypeFunction);&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;builder.put(&#34;rk&#34;, rktmp.asMap());
builder.put(&#34;ck&#34;, cktmp.asMap());
builder.put(&#34;sc&#34;, sctmp.asMap());
builder.put(&#34;dg&#34;, dgtmp.asMap());
builder.put(&#34;pd&#34;, pdtmp.asMap());
builder.put(&#34;kc&#34;, kctmp.asMap());&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;*id {lable:&#34;varchar(32) NOT NULL&#34;}
reservoir_code {lable:&#34;varchar(32) DEFAULT NULL&#34;}
abbreviation {lable:&#34;varchar(64) DEFAULT NULL&#34;}
reservoir_name {lable:&#34;varchar(256) DEFAULT NULL&#34;}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;id {lable:&#34;varchar(32) NOT NULL&#34;}
storage_code {lable:&#34;varchar(32) DEFAULT NULL&#34;}
storage_name {lable:&#34;varchar(128) DEFAULT NULL&#34;}
reservoir_code {lable:&#34;varchar(32) DEFAULT NULL&#34;}
storage_capacity {lable:&#34;double(8,0) DEFAULT NULL&#34;}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;SELECT   *,   (SELECT     zra.abbreviation   FROM     zh_reservoir_area zra   WHERE zra.reservoir_code = zar.reservoir_code) AS abbreviation,   (SELECT     zs.storage_capacity   FROM     zh_storage zs   WHERE zs.reservoir_code = zar.reservoir_code     AND zs.storage_code = zar.storage_code GROUP BY zar.reservoir_code) AS storageCapacity FROM   zh_asphalt_repertory zar GROUP BY zar.reservoir_code,   zar.storage_code HAVING 1=1 AND zar.type = &#39;00&#39; ORDER BY abbreviation&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cabal sandbox hc-pkg&amp;#8201;&amp;#8212;&amp;#8201;unregister base&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT
  tranCat (LEFT(cat_code, LENGTH(cat_code) - 1)) AS category,
  (SELECT
    brand_name
  FROM
    mlq_brand
  WHERE id = brand_id) brand_name,
  (SELECT
    brand_name
  FROM
    mlq_brand
  WHERE id = brand_id) brand_name_noa,
  (SELECT
    msi.shop_name
  FROM
    mlq_shop_info msi
  WHERE msi.id = mg.shop_id) new_seller_nick,
  (SELECT
    msi.shop_name
  FROM
    mlq_shop_info msi
  WHERE msi.id = mg.shop_id) new_seller_nick_noa,
  mg.*
FROM
  mlq_goods mg
WHERE mg.gmt_modify &amp;gt; &#39;2017-05-08 10:53:51&#39;
OR mg.gmt_create &amp;gt; &#39;2017-05-08 10:53:51&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;mklink D:\env\bin\dot.exe D:\env\graphviz-2.38\release\bin\dot.exe&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;%graphvizdot%\bin;%SBT_HOME%\bin;D:\env\code\bin;C:\msys64\mingw64\bin;d:\env\Ruby22\bin;D:\env\cwRsync\bin;%GOROOT%\bin;C:\ProgramData\Oracle\Java\javapath;%MAVEN_HOME%\bin;%JAVA_HOME%\bin;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;D:\HashiCorp\Vagrant\bin;C:\Program Files (x86)\MySQL\MySQL Server 5.5\bin&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;sssssssss&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;WebRoot/web/storagemanagement/repertory/asphalt_count_jxc.jsp&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;gem install asciidoctor-diagram&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.graphviz.org/Download_windows.php&#34; class=&#34;bare&#34;&gt;http://www.graphviz.org/Download_windows.php&lt;/a&gt;
images_dir = parent.attr(&#39;plantimgdir&#39;)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;#{parent.attr(&#39;plantimgdir&#39;)}/&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;#title {label: &#34;大物流仓库库存表关系&#34;, size: &#34;20&#34;}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/plantuml/erd-1.png&#34; alt=&#34;erd 1&#34; width=&#34;1768&#34; height=&#34;723&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/plantuml/diagram-classes4.png&#34; alt=&#34;diagram classes4&#34; width=&#34;317&#34; height=&#34;949&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/plantuml/diagram-classes25.svg&#34; alt=&#34;diagram classes25&#34; width=&#34;314&#34; height=&#34;325&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/plantuml/ditaa-diagram.png&#34; alt=&#34;ditaa diagram&#34; width=&#34;540&#34; height=&#34;280&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT zspl.*
FROM zh_sub_plan_launch zspl
WHERE zspl.plan_code IN
(SELECT plan_code
FROM zh_plan_launch zpl
WHERE zpl.state = &#39;01&#39;
AND zpl.table_date &amp;gt;= ?
AND zpl.table_date &amp;#8656; ?)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT *
FROM zh_plan zp阿三地方
WHERE plan_code = ?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT * FROM zh_plan zp WHERE plan_code = &#39;20170116101801016&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;$(this).find(&#39;&lt;strong&gt;[name=&#34;zhPlanCode&#34;]&#39;)
$(this).find(&#39;&lt;/strong&gt;[name=&#34;projectName&#34;]&#39;)
$(this).find(&#39;&lt;strong&gt;[name=&#34;reservoiCode&#34;]&#39;)
$(this).find(&#39;select[name=&#34;goodsCode&#34;]&#39;)
$(this).find(&#39;&lt;/strong&gt;[name=&#34;pickMode&#34;]&#39;)
$(this).find(&#39;&lt;strong&gt;[name=&#34;shipmentAddress&#34;]&#39;)
$(this).find(&#39;&lt;/strong&gt;[name=&#34;shipmentsCount&#34;]&#39;)
$(this).find(&#39;&lt;strong&gt;[name=&#34;receivingAddress&#34;]&#39;)
$(this).find(&#39;&lt;/strong&gt;[name=&#34;receivingUnit&#34;]&#39;)
$(this).find(&#39;&lt;strong&gt;[name=&#34;consignee&#34;]&#39;)
$(this).find(&#39;&lt;/strong&gt;[name=&#34;receivingPhone&#34;]&#39;)
$(this).find(&#39;&lt;strong&gt;[name=&#34;memo&#34;]&#39;)
$(this).find(&#39;&lt;/strong&gt;[name=&#34;contractCode&#34;]&#39;)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT plan.plan_name,sub.*,obo.number,sub.shipments_count) - (obo.number AS w_count,gi.goods_name,gi.brand,gi.model,ra.abbreviation FROM zh_plan plan,zh_sub_plan sub LEFT JOIN zh_out_bound_order obo ON sub.zh_plan_code = obo.zh_plan_code LEFT JOIN zh_goods_info gi ON sub.goods_code = gi.goods_code LEFT JOIN zh_reservoir_area ra ON sub.shipment_address = ra.reservoir_code WHERE plan.plan_code = sub.plan_code AND plan.plan_code = &#39;20170116154403004&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker run -it --rm --volume-driver=cifs -v 192.168.137.2/note-hugo:/hugo ansible/centos7-ansible:1.7 /bin/bash&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;服务器升级:
1. Rancher(docker容器管理平台)升级
	1.1 rancher/server:v1.2 升级到 v1.3.1
	1.2 rancher/agent:v1.1.1 升级到 v1.1.3
2. CI/CD 持续集成和持续交付
	1.1 Ansible 批量部署操作
	1.2 Jenkins与Ansible结合实现一键部署重启
Bug修复:
发货申请审批:
	1. 审批中的记录-审批状态显示错误-请改为审批中
	2. 查询条件-执行状态下拉值请和审核状态一致
	3. 直销合同新增审批-先保存之后修改时点提交则审批流名称则变为招标合同
	4. 直销合同发货审批流-赊销大于5000万时少了一步营销中心总经理审批
	5. 招标合同货物审批通过后-审批状态依然显示为待审核-没有变为已审核&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;优点：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1、光佳团队策划流程很到位&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;①活动人员安排：  尽管时间很紧，人员变动比较频繁，整体掌控很到位&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;②活动提前预热：  通过微信、圈子传播能够在短期内报名数60，很到位&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;③活动效果预估：  直播活动预估3万+，最终5万+&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;④活动成本预估：  赚到钱是硬道理&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;⑤活动互动效果：  笪总、美女主持人穿插广告、互动营销、红包形式的互动，提高了互动性&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;⑥活动效果公布：  通过微信、纸质形式通知效果很好&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;2、笪总团队线下配合很到位&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;①线下接待、让嘉宾没有陌生感、得到尊重感，有共同话题才是硬道理&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;②笪总的主持游刃有余，沥青届的老炮&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;改善：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1、直播体验需要提高：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1）直播网络稳定性、系统稳定性要提高，导致直播卡顿&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;反省：① 时间维度不够充裕：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;中午13点到下午17点，共4个小时，时间比较紧凑，配置的设备比较多，容易影响直播效果的因素太多&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;② 技术人员切入不够早：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;设备问题包括硬件、软件两方面，需要在启动项目时，尽早切入，以备不时之需，工作要交给擅长的人                       去做&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;2）嘉宾直播气氛需要提高&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;反省：① 房间不够大：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;需要针对不同级别的嘉宾或者企业要有不同的配置，特别针对500强的企业要做的更加符合企业的气质&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;② 观众需要一定量级：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;对于传统嘉宾来讲更有气氛，需要有人捧场，需要有人鼓掌，才有分享的激情&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;2、活动数据收集差&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1）5万9的观众，如何收集起来？目前没有方法，即时有，也不够准确；&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;建议：app作为统一报名入口，但需要开发，需要周期&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;3、直播还能做什么？&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1）广告营销：直播2小时，可以穿插广告（在直播屏幕上，不同时间段穿插广告）&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;2）延长营销：直播仅仅2个小时，如何能够让2个小时的营销效果时间延长，个人认为可以将直播经典内容沉淀下来&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;可以分段营销，在不同的地点、平台均可以&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;3）直播专场：此次是学术界的直播，后期可以做专场，譬如采购、销售、大客户比如中石化、中海油、京博等等&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;建议：提前准备听众&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;优点:
	运营部在人员少,资源少,时间紧的情况下办好一场很精彩的直播.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;改善:
	直播环境准备:
		1. 电脑
			笔记本和台式机都要准备,做好一比一测试,如果其中一台电脑有问题,可以用另一台电脑做备用
		2. 直播过程中可以插播公司广告,公司宣传片,充分利用直播平台
		3. 需要准备足够多的观众,烘托现场气氛&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT
  mp_tmp.product_name AS product_name_noa,
  mp_tmp.&lt;strong&gt;,
  mpa_tmp.&lt;/strong&gt;
FROM
  (SELECT
    *
  FROM
    mlq_product
  WHERE gmt_modify &amp;gt; &#39;2016-07-22&#39;
  ) mp_tmp,
  (SELECT
    mpa.product_id,
    MAX(
      CASE
        mpa.attr_name
        WHEN &#39;销售地区&#39;
        THEN IFNULL(
          tranAddr (mpa.attr_value, &#39;-&#39;),
          CAST(mpa.attr_value AS CHAR(50))
        )
        ELSE NULL
      END
    ) sale_region,
    MAX(
      CASE
        mpa.attr_name
        WHEN &#39;出库地区&#39;
        THEN IFNULL(
          tranAddr (mpa.attr_value, &#39;-&#39;),
          CAST(mpa.attr_value AS CHAR(50))
        )
        ELSE NULL
      END
    ) repository_region,
    MAX(
      CASE
        mpa.attr_name
        WHEN &#39;出库地区&#39;
        THEN IFNULL(
          tranAddr (mpa.attr_value, &#39;-&#39;),
          CAST(mpa.attr_value AS CHAR(50))
        )
        ELSE NULL
      END
    ) repository_region_noa,
    MAX(
      CASE
        mpa.attr_name
        WHEN &#39;最小起订量&#39;
        THEN mpa.attr_value
        ELSE 0
      END
    ) min_quantity,
    MAX(
      CASE
        mpa.attr_name
        WHEN &#39;库存量&#39;
        THEN mpa.attr_value
        ELSE 0
      END
    ) stock_quantity,
    MAX(
      CASE
        mpa.attr_name
        WHEN &#39;单位&#39;
        THEN mpa.attr_value
        ELSE &#39;吨&#39;
      END
    ) unit,
    MAX(
      CASE
        mpa.attr_name
        WHEN &#39;交货天数&#39;
        THEN mpa.attr_value
        ELSE 7
      END
    ) delivery_day,
    MAX(
      CASE
        mpa.attr_name
        WHEN &#39;包装类型&#39;
        THEN mpa.attr_value
        ELSE &#39;散装&#39;
      END
    ) package_type
  FROM
    mlq_product_attr mpa
  WHERE mpa.product_id IN
    (SELECT
      id
    FROM
      mlq_product
    WHERE gmt_modify &amp;gt; &#39;2016-07-22&#39;)
  GROUP BY mpa.product_id) mpa_tmp
WHERE mp_tmp.id = mpa_tmp.product_id&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;大物流服务器重新规划部署:
1. CoreOS 系统安装
2. Docker 环境配置
3. Rancher Client 环境配置
4. nginx 反向代理 搭建
5. 大物流数据迁移
  5.1 Web 数据迁移
  5.2 数据库迁移&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Mai沥青 PC 端 Bug 修复
大物流 PC 端 Bug 修复&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker create --volumes-from rancher-server \
 --name rancher-data rancher/server:1.2&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker run -d --volumes-from rancher-data \
  -p 9090:8080 rancher/server:stable&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;jw.name(&#34;bool&#34;);
jw.beginObject();
jw.name(&#34;must&#34;);
jw.beginArray();
jw.beginObject();
jw.name(&#34;match&#34;);
jw.beginObject();
jw.name(&#34;cat_code&#34;);
jw.value(&#34;20150914q0asfEw&#34;);
jw.endObject();
jw.endObject();
jw.beginObject();
jw.name(&#34;match&#34;);
jw.beginObject();
jw.name(&#34;goods_status&#34;);
jw.value(&#34;c&#34;);
jw.endObject();
jw.endObject();
jw.endArray();
jw.endObject();&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker run -d --name=httpd -p 8088:80 --volume-driver=cifs -v 192.168.137.2/note-hugo/public:/usr/local/apache2/htdocs/ httpd:2.2.32-alpine&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;供应商：&amp;lt;input name=&#34;new_seller_nick&#34; type=&#34;text&#34; value=&#34;${param.new_seller_nick }&#34;/&amp;gt;&amp;lt;/span&amp;gt;
商品名称：&amp;lt;input name=&#34;title&#34; type=&#34;text&#34; value=&#34;${param.title }&#34;/&amp;gt;&amp;lt;/span&amp;gt;
商品编号：&amp;lt;input name=&#34;id&#34; type=&#34;text&#34;/&amp;gt;&amp;lt;/span&amp;gt;
品牌：&amp;lt;select name=&#34;brand_id&#34; id=&#34;brand_id&#34;&amp;gt;&amp;lt;/select&amp;gt;&amp;lt;/span&amp;gt;
管理类目：&amp;lt;select name=&#34;cat_code&#34; id=&#34;first_category&#34;&amp;gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;server {
    listen       9090;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;    error_log /var/log/nginx/rancher-error.log
    access_log /var/log/nginx/rancher-access.log
    location / {
        proxy_redirect          off;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header REMOTE-HOST $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://192.168.1.129:9090;
    }
}&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker run -d --volumes-from rancher-data -p 9090:8080 --name rancher-server rancher/server:stable&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker run -d -p 192.168.1.129:8080:8080 --name rancher-server rancher/server:stable&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;dishui.io:5000/nginx:stable&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;syslog-address               tcp://192.168.137.20:514&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cat &amp;gt;&amp;gt; /opt/logio/docker-entrypoint.sh &amp;lt;&amp;lt;EOF
if [ &#34;$2&#34; = &#39;rsyslogd&#39; ]; then
  rsyslogd -n
fi
EOF&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cat &amp;gt; /etc/nginx/conf.d/default.conf &amp;lt;&amp;lt;EOF
server {
    listen       80;
    access_log /var/log/nginx/test.log;
    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
}
EOF&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://192.168.1.129:8080/v2-beta/schemas&#34; class=&#34;bare&#34;&gt;http://192.168.1.129:8080/v2-beta/schemas&lt;/a&gt;
9FFE6681FF339A1A747D
m2E45BeYUxzr7FSAeNvhiR4R18hQL1n7nCdNQnqi&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SYSLOG_SERVER=192.168.137.20
SYSLOG_PORT=514
SYSLOG_PROTO=tcp&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;-e &#39;SYSLOG_SERVER=192.168.137.20&#39;-e &#39;SYSLOG_PORT=514&#39;-e &#39;SYSLOG_PROTO=tcp&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;$ModLoad imfile
$InputFilePollInterval 10
$WorkDirectory /var/spool/rsyslog
$PrivDropToGroup adm&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;$template BiglogFormatTomcat,&#34;%msg%\n&#34;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;$InputFileName /apache-tomcat-7.0.62/logs/localhost_access_log.2017-02-21.txt
$InputFileTag access-log
$InputFileStateFile stat-access-log
$InputFileSeverity info
$InputFilePersistStateInterval 25000
$InputRunFileMonitor&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;if $programname == &#39;access-log&#39; then @192.168.137.20:514;BiglogFormatTomcat
if $programname == &#39;access-log&#39; then ~&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;FROM wuliu/tomcat:1.0&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ENV SYSLOG_SERVER 192.168.137.20
ENV SYSLOG_PORT 514
ENV SYSLOG_PROTO tcp&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;COPY ./tomcat.conf /etc/supervisord.conf.d/tomcat.conf
RUN yum -y install epel-release \
       &amp;amp;&amp;amp; yum -y install python-pip \
       &amp;amp;&amp;amp; pip install supervisor-logging \
       &amp;amp;&amp;amp; yum remove epel-release python-pip -y \
       &amp;amp;&amp;amp; yum clean all&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;echo &#34;30 22 * * * . /usr/sbin/logrotate /etc/logrotate.conf&#34; &amp;gt;&amp;gt; /var/spool/cron/root&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT
  zra.reservoir_code,
  zra.abbreviation,
  zs.storage_code,
  zs.storage_capacity,
  CAST(rk.model70_rk AS CHAR) AS model70_rk,
  CAST(rk.model90_rk AS CHAR) AS model90_rk,
  CAST(ck.model70_ck AS CHAR) AS model70_ck,
  CAST(ck.model90_ck AS CHAR) AS model90_ck,
  CAST(ck.modelIC_ck AS CHAR) AS modelIC_ck,
  CAST(ck.modelID_ck AS CHAR) AS modelID_ck,
  CAST(ck.modelgt_ck AS CHAR) AS modelgt_ck,
  CAST(ck.modelgz_ck AS CHAR) AS modelgz_ck,
  CAST(sc.modelIC_sc AS CHAR) AS modelIC_sc,
  CAST(sc.modelID_sc AS CHAR) AS modelID_sc,
  CAST(sc.modelgt_sc AS CHAR) AS modelgt_sc,
  CAST(sc.modelgz_sc AS CHAR) AS modelgz_sc,
  CAST(kc.model70_kc AS CHAR) AS model70_kc,
  CAST(kc.model90_kc AS CHAR) AS model90_kc,
  CAST(kc.modelIC_kc AS CHAR) AS modelIC_kc,
  CAST(kc.modelID_kc AS CHAR) AS modelID_kc,
  CAST(kc.modelgt_kc AS CHAR) AS modelgt_kc,
  CAST(kc.modelgz_kc AS CHAR) AS modelgz_kc,
  CAST(pd.model70_pd AS CHAR) AS model70_pd,
  CAST(pd.model90_pd AS CHAR) AS model90_pd,
  CAST(pd.modelIC_pd AS CHAR) AS modelIC_pd,
  CAST(pd.modelID_pd AS CHAR) AS modelID_pd,
  CAST(pd.modelgt_pd AS CHAR) AS modelgt_pd,
  pd.modelgz_pd,
  pd.model_yk,
  (SELECT
    ait.temp
  FROM
    zh_asphalt_inventory_table ait,
    zh_goods_info gi
  WHERE ait.tank = zs.storage_code
    AND ait.storage_area = zs.reservoir_code
    AND ait.goods_code = gi.goods_code) AS temp
FROM
  zh_storage zs
  LEFT JOIN zh_reservoir_area zra
    ON zs.reservoir_code = zra.reservoir_code
  LEFT JOIN
    (SELECT
      zar.storage_code,
      zar.reservoir_code,
      zar.goods_model,
      (
        CASE
          WHEN zar.goods_model = &#39;70#&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS model70_rk,
      (
        CASE
          WHEN zar.goods_model = &#39;90#&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS model90_rk
    FROM
      zh_asphalt_repertory zar,
      zh_goods_info gi
    WHERE zar.goods_code = gi.goods_code
      AND zar.order_type IN (&#39;00&#39;)
      AND zar.type = &#39;00&#39;
    GROUP BY zar.storage_code,
      zar.reservoir_code,
      zar.goods_model) rk
    ON zs.reservoir_code = rk.reservoir_code
    AND zs.storage_code = rk.storage_code
  LEFT JOIN
    (SELECT
      zar.storage_code,
      zar.reservoir_code,
      zar.goods_model,
      (
        CASE
          WHEN zar.goods_model = &#39;70#&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS model70_ck,
      (
        CASE
          WHEN zar.goods_model = &#39;90#&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS model90_ck,
      (
        CASE
          WHEN zar.goods_model = &#39;I-C&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelIC_ck,
      (
        CASE
          WHEN zar.goods_model = &#39;I-D&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelID_ck,
      (
        CASE
          WHEN zar.goods_model = &#39;高弹&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelgt_ck,
      (
        CASE
          WHEN zar.goods_model = &#39;高粘&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelgz_ck
    FROM
      zh_asphalt_repertory zar,
      zh_goods_info gi
    WHERE zar.goods_code = gi.goods_code
      AND zar.order_type IN (&#39;01&#39;, &#39;02&#39;)
      AND zar.type = &#39;00&#39;
    GROUP BY zar.storage_code,
      zar.reservoir_code,
      zar.goods_model) ck
    ON zs.reservoir_code = ck.reservoir_code
    AND zs.storage_code = ck.storage_code
  LEFT JOIN
    (SELECT
      zar.storage_code,
      zar.reservoir_code,
      zar.goods_model,
      (
        CASE
          WHEN zar.goods_model = &#39;I-C&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelIC_sc,
      (
        CASE
          WHEN zar.goods_model = &#39;I-D&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelID_sc,
      (
        CASE
          WHEN zar.goods_model = &#39;高弹&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelgt_sc,
      (
        CASE
          WHEN zar.goods_model = &#39;高粘&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelgz_sc
    FROM
      zh_asphalt_repertory zar,
      zh_goods_info gi
    WHERE zar.goods_code = gi.goods_code
      AND zar.order_type IN (&#39;02&#39;)
      AND zar.type = &#39;00&#39;
    GROUP BY zar.storage_code,
      zar.reservoir_code,
      zar.goods_model) sc
    ON zs.reservoir_code = sc.reservoir_code
    AND zs.storage_code = sc.storage_code
  LEFT JOIN
    (SELECT
      zar.storage_code,
      zar.reservoir_code,
      zar.goods_model,
      (
        CASE
          WHEN zar.goods_model = &#39;70#&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS model70_kc,
      (
        CASE
          WHEN zar.goods_model = &#39;90#&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS model90_kc,
      (
        CASE
          WHEN zar.goods_model = &#39;I-C&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelIC_kc,
      (
        CASE
          WHEN zar.goods_model = &#39;I-D&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelID_kc,
      (
        CASE
          WHEN zar.goods_model = &#39;高弹&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelgt_kc,
      (
        CASE
          WHEN zar.goods_model = &#39;高粘&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelgz_kc
    FROM
      zh_asphalt_repertory zar,
      zh_goods_info gi
    WHERE zar.goods_code = gi.goods_code
      AND zar.order_type IN (&#39;00&#39;, &#39;01&#39;, &#39;02&#39;)
      AND zar.type = &#39;00&#39;
    GROUP BY zar.storage_code,
      zar.reservoir_code,
      zar.goods_model) kc
    ON zs.reservoir_code = kc.reservoir_code
    AND zs.storage_code = kc.storage_code
  LEFT JOIN
    (SELECT
      zar.storage_code,
      zar.reservoir_code,
      zar.goods_model,
      SUM(zar.goods_count) AS model_yk,
      (
        CASE
          WHEN zar.goods_model = &#39;70#&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS model70_pd,
      (
        CASE
          WHEN zar.goods_model = &#39;90#&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS model90_pd,
      (
        CASE
          WHEN zar.goods_model = &#39;I-C&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelIC_pd,
      (
        CASE
          WHEN zar.goods_model = &#39;I-D&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelID_pd,
      (
        CASE
          WHEN zar.goods_model = &#39;高弹&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelgt_pd,
      (
        CASE
          WHEN zar.goods_model = &#39;高粘&#39;
          THEN SUM(zar.goods_count)
          ELSE &#39;&#39;
        END
      ) AS modelgz_pd
    FROM
      zh_asphalt_repertory zar,
      zh_goods_info gi
    WHERE zar.goods_code = gi.goods_code
      AND zar.order_type IN (&#39;05&#39;)
      AND zar.type = &#39;00&#39;
    GROUP BY zar.storage_code,
      zar.reservoir_code,
      zar.goods_model) pd
    ON zs.reservoir_code = pd.reservoir_code
    AND zs.storage_code = pd.storage_code
WHERE 1 = 1
GROUP BY zs.storage_code,
  zs.reservoir_code
ORDER BY zra.abbreviation DESC&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;WHERE zs.storage_code = &#39;TKH-03&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;00入库单
01出库单
02生产单
03结余调整单
04倒灌单
05盘点单号&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;库区  名称  入库的型号(70 90等)
  储罐 罐号 库容&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT&#34;&lt;br&gt;
&#34;  zar.reservoir_code,&#34;&lt;br&gt;
&#34;  zar.storage_code,&#34;&lt;br&gt;
&#34;  zar.goods_model,&#34;&lt;br&gt;
&#34;  zar.order_type&#34;&lt;br&gt;
&#34; FROM&#34;&lt;br&gt;
&#34;  zh_asphalt_repertory zar&#34;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT  zar.reservoir_code,  zar.storage_code,  zar.goods_model,  zar.order_typeFROM  zh_asphalt_repertory zar&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;{title:&#39;入库数量（吨）&#39;,colspan:2},
{title:&#39;出库数量（吨）&#39;,colspan:6},
{title:&#39;生产数量（吨）&#39;,colspan:4},
{title:&#39;库存数量（吨）&#39;,colspan:6},
{title:&#39;盘点数量（吨）&#39;,colspan:6},&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT &#34;&lt;br&gt;
&#34;   *&#34;&lt;br&gt;
&#34; FROM&#34;&lt;br&gt;
&#34;   zh_asphalt_repertory zar&#34;&lt;br&gt;
&#34; GROUP BY zar.reservoir_code,zar.storage_code&#34;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT *&#34;+
&#34; FROM zh_asphalt_repertory zar&#34;+
&#34; WHERE zar.reservoir_code = &#39;001&#39;&#34;+
&#34; AND zar.storage_code = &#39;T1003&#39;&#34;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;{id=18b0cef136694001a0849fcb1cee3a7f, reservoirCode=002, orderNo=20170222-A, orderType=00, occurDate=Wed Feb 22 00:00:00 CST 2017, storageCode=TKH-07, goodsCount=500.0000, goodsCode=077, type=00, createDate=Wed Feb 22 16:03:12 CST 2017, goodsModel=70, goodsBatch=, model70rk=500.0000}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT
  *,
  (SELECT
    zra.abbreviation
  FROM
    zh_reservoir_area zra
  WHERE zra.reservoir_code = zar.reservoir_code) AS abbreviation,
  (SELECT
    zs.storage_capacity
  FROM
    zh_storage zs
  WHERE zs.reservoir_code = zar.reservoir_code
    AND zs.storage_code = zar.storage_code) AS storage_capacity
FROM
  zh_asphalt_repertory zar
GROUP BY zar.reservoir_code,
  zar.storage_code&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;HAVING 1=1
AND zar.reservoir_code = &#39;001&#39;
AND zar.storage_code = &#39;T1003&#39;
AND zar.occur_date = &#39;2017-01-17&#39;
ORDER BY abbreviation&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;LIMIT 0, 10&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;周凯(男) 29岁 洛阳师范-生物技术 本科生 现在北京金兆路华电子商务担任Java开发工程师&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;工作经验:
2013-2014 地点:郑州  OA,ERP系统开发
2015-~    地点:北京   电商系统开发&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;喜欢全栈&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;电商系统开发:
  二次开发系统,搜索模块由Lucene迁移到ElasticSearch
  同步Mysql数据到ElasticSearch
  ElasticSearch权重设计&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;服务器部署:
  CoreOS+Docker
  Rancher
  Jenkins+Ansible+Docker
  实现自动化部署&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;喜欢研究技术
希望通过Leader课程,系统的学习&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT zas.category&#34;+
&#34; FROM zh_asphalt_storage zas&#34;+
&#34; GROUP BY zas.category&#34;+
&#34; HAVING zas.category IS NOT NULL&#34;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT *&#34;+
&#34; FROM zh_asphalt_storage zas&#34;+
&#34; WHERE zas.category = &#39;01&#39;&#34;+
&#34; GROUP BY zas.model&#34;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;String categorySql =
        &#34; SELECT *&#34;+
        &#34; FROM zh_asphalt_storage zas&#34;+
        &#34; WHERE zas.category = &#39;01&#39;&#34;+
        &#34; GROUP BY zas.model&#34;;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;AsphaltStorageServiceImpl&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT &#34;+
&#34;   *,&#34;+
&#34;   zas.instruct_type,&#34;+
&#34;   zas.Net_profit_loss,&#34;+
&#34;   zas.Storage_area,&#34;+
&#34;   DATE_FORMAT(zas.in_date, &#39;%Y-%m-%d&#39;) AS dat,&#34;+
&#34;   (SELECT &#34;+
&#34;     SUM(zasc.goods_count) &#34;+
&#34;   FROM&#34;+
&#34;     zh_asphalt_storage_count zasc &#34;+
&#34;   WHERE zasc.asphalt_number = zas.asphalt_number) AS goods_count,&#34;+
&#34;   (SELECT &#34;+
&#34;     zra.abbreviation &#34;+
&#34;   FROM&#34;+
&#34;     zh_reservoir_area zra &#34;+
&#34;   WHERE zra.reservoir_code = zas.Storage_area) AS abbreviation &#34;+
&#34; FROM&#34;+
&#34;   zh_asphalt_storage zas &#34;+
&#34; ORDER BY zas.in_date DESC&#34;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT
  Storage_area
FROM
  zh_sr_productin
WHERE 1=1&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT&#34;&lt;br&gt;
&#34;  *, DATE_FORMAT(storage_date, &#39;%Y-%m-%d&#39;) AS dat2&#34;&lt;br&gt;
&#34; FROM&#34;&lt;br&gt;
&#34;   zh_sr_productin&#34;&lt;br&gt;
&#34; WHERE Storage_area = ?&#34;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT
 *, DATE_FORMAT(out_date, &#39;%Y-%m-%d&#39;) AS dat2
FROM
  zh_material_outbound
WHERE Storage_area = &#39;001&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT&#34;&lt;br&gt;
                &#34;  *, DATE_FORMAT(storage_date, &#39;%Y-%m-%d&#39;) AS dat2&#34;&lt;br&gt;
                &#34; FROM&#34;&lt;br&gt;
                &#34;   zh_material_outbound&#34;&lt;br&gt;
                &#34; WHERE Storage_area = ?&#34;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT&#34;+
&#34;   zmo.model,&#34;+
&#34;   zmo.number,&#34;+
&#34;   DATE_FORMAT(zmo.out_date, &#39;%Y-%m-%d&#39;) AS dat2,&#34;+
&#34;   (SELECT&#34;+
&#34;     zra.abbreviation&#34;+
&#34;   FROM&#34;+
&#34;     zh_reservoir_area zra&#34;+
&#34;   WHERE zra.reservoir_code = zmo.Storage_area) Storage_area&#34;+
&#34; FROM&#34;+
&#34;   zh_material_outbound zmo&#34;+
&#34; WHERE zmo.Storage_area = ?&#34;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT
 zar.reservoir_code,
 zar.storage_code,
 zar.goods_model,
 zar.order_type
FROM
 zh_asphalt_repertory zar
WHERE zar.order_type =
GROUP BY zar.goods_model&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;jquery.bigautocomplete.js&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;echo &#34;public&#34; &amp;gt;&amp;gt; .gitignore &amp;amp;&amp;amp; \
echo &#34;themes/mainroad/static/js/app.js&#34; &amp;gt;&amp;gt; .gitignore &amp;amp;&amp;amp; \
echo &#34;themes/mainroad/static/js/jquery.bigautocomplete.js&#34; &amp;gt;&amp;gt; .gitignore &amp;amp;&amp;amp; \
echo &#34;config.toml&#34; &amp;gt;&amp;gt; .gitignore &amp;amp;&amp;amp; \
echo &#34;content/post/base.adoc&#34; &amp;gt;&amp;gt; .gitignore&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;baseurl = &#34;http://dishui.oschina.io/note-hugo/&#34;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;version: &#39;2&#39;
services:
  zoo1:
    image: zookeeper
    restart: always
    ports:
      - 2181:2181
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zoo1:2888:3888
07.阿里开源分布式框架dubbo&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;9df8e09ee259
cadc21522a4a
e3eb53c850a3
0ec18fcb5f14
dcfb02a0f0c0
1ae259b410a4
9a0db3e1e5fa
a299de9d8b43
2354e58c407f&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT&#34;&lt;br&gt;
&#34;   dat.*,&#34;&lt;br&gt;
&#34;   SUM(dat.rksl) AS count_rk,&#34;&lt;br&gt;
&#34;   SUM(dat.cksl) AS count_ck,&#34;&lt;br&gt;
&#34;   (SELECT&#34;&lt;br&gt;
&#34;     SUM(zar.goods_count)&#34;&lt;br&gt;
&#34;   FROM&#34;&lt;br&gt;
&#34;     zh_asphalt_repertory zar&#34;&lt;br&gt;
&#34;   WHERE dat.goods_code = zar.goods_code&#34;&lt;br&gt;
&#34;     AND TYPE = &#39;01&#39;) AS count_kc,&#34;&lt;br&gt;
&#34;   SUM(dat.scsl) AS count_sc,&#34;&lt;br&gt;
&#34;   SUM(dat.pdsl) AS count_pdf&#34;&lt;br&gt;
&#34; FROM&#34;&lt;br&gt;
&#34;   (SELECT&#34;&lt;br&gt;
&#34;     DATE_FORMAT(sp.storage_date, &#39;%Y-%m-%d&#39;) AS storage_date,&#34;&lt;br&gt;
&#34;     sp.Storage_area,&#34;&lt;br&gt;
&#34;     sp.reservoir_code,&#34;&lt;br&gt;
&#34;     sp.goods_code,&#34;&lt;br&gt;
&#34;     gi.goods_name,&#34;&lt;br&gt;
&#34;     sp.brand,&#34;&lt;br&gt;
&#34;     sp.model,&#34;&lt;br&gt;
&#34;     sp.specifications,&#34;&lt;br&gt;
&#34;     sp.Inventory_quantity AS rksl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS cksl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS scsl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS pdsl&#34;&lt;br&gt;
&#34;   FROM&#34;&lt;br&gt;
&#34;     zh_sr_productin sp,&#34;&lt;br&gt;
&#34;     zh_goods_info gi&#34;&lt;br&gt;
&#34;   WHERE sp.goods_code = gi.goods_code&#34;&lt;br&gt;
&#34;   UNION&#34;&lt;br&gt;
&#34;   ALL&#34;&lt;br&gt;
&#34;   SELECT&#34;&lt;br&gt;
&#34;     mo.start_date,&#34;&lt;br&gt;
&#34;     mo.Storage_area,&#34;&lt;br&gt;
&#34;     ra.abbreviation,&#34;&lt;br&gt;
&#34;     mo.goods_code,&#34;&lt;br&gt;
&#34;     gi.goods_name,&#34;&lt;br&gt;
&#34;     mo.brand,&#34;&lt;br&gt;
&#34;     mo.model,&#34;&lt;br&gt;
&#34;     mo.specifications,&#34;&lt;br&gt;
&#34;     &#39;&#39;,&#34;&lt;br&gt;
&#34;     mo.number,&#34;&lt;br&gt;
&#34;     &#39;&#39;,&#34;&lt;br&gt;
&#34;     &#39;&#39;&#34;&lt;br&gt;
&#34;   FROM&#34;&lt;br&gt;
&#34;     zh_material_outbound mo,&#34;&lt;br&gt;
&#34;     zh_goods_info gi,&#34;&lt;br&gt;
&#34;     zh_reservoir_area ra&#34;&lt;br&gt;
&#34;   WHERE mo.Storage_area = ra.reservoir_code&#34;&lt;br&gt;
&#34;     AND mo.goods_code = gi.goods_code&#34;&lt;br&gt;
&#34;   UNION&#34;&lt;br&gt;
&#34;   ALL&#34;&lt;br&gt;
&#34;   SELECT&#34;&lt;br&gt;
&#34;     DATE_FORMAT(zps.start_time, &#39;%Y-%m-%d&#39;),&#34;&lt;br&gt;
&#34;     pp.Storage_area,&#34;&lt;br&gt;
&#34;     pp.reservoir_code,&#34;&lt;br&gt;
&#34;     zps.Name_commodity,&#34;&lt;br&gt;
&#34;     gi.goods_name,&#34;&lt;br&gt;
&#34;     zps.brand,&#34;&lt;br&gt;
&#34;     zps.model,&#34;&lt;br&gt;
&#34;     zps.specifications,&#34;&lt;br&gt;
&#34;     &#39;&#39;,&#34;&lt;br&gt;
&#34;     &#39;&#39;,&#34;&lt;br&gt;
&#34;     zps.Production_quantity,&#34;&lt;br&gt;
&#34;     &#39;&#39;&#34;&lt;br&gt;
&#34;   FROM&#34;&lt;br&gt;
&#34;     zh_production_subsidiary zps,&#34;&lt;br&gt;
&#34;     zh_goods_info gi,&#34;&lt;br&gt;
&#34;     zh_production_processing pp&#34;&lt;br&gt;
&#34;   WHERE pp.Production_order = zps.production_order&#34;&lt;br&gt;
&#34;     AND zps.goods_code = gi.goods_code&#34;&lt;br&gt;
&#34;   UNION&#34;&lt;br&gt;
&#34;   ALL&#34;&lt;br&gt;
&#34;   SELECT&#34;&lt;br&gt;
&#34;     DATE_FORMAT(ait.&lt;code&gt;data&lt;/code&gt;, &#39;%Y-%m-%d&#39;),&#34;&lt;br&gt;
&#34;     ra.abbreviation,&#34;&lt;br&gt;
&#34;     ait.storage_area,&#34;&lt;br&gt;
&#34;     ait.accessories_name,&#34;&lt;br&gt;
&#34;     gi.goods_name,&#34;&lt;br&gt;
&#34;     ait.brand,&#34;&lt;br&gt;
&#34;     ait.model,&#34;&lt;br&gt;
&#34;     ait.specifications,&#34;&lt;br&gt;
&#34;     &#39;&#39;,&#34;&lt;br&gt;
&#34;     &#39;&#39;,&#34;&lt;br&gt;
&#34;     &#39;&#39;,&#34;&lt;br&gt;
&#34;     ait.inventory_number&#34;&lt;br&gt;
&#34;   FROM&#34;&lt;br&gt;
&#34;     zh_additive_inventory_table ait,&#34;&lt;br&gt;
&#34;     zh_goods_info gi,&#34;&lt;br&gt;
&#34;     zh_reservoir_area ra&#34;&lt;br&gt;
&#34;   WHERE ait.storage_area = ra.reservoir_code&#34;&lt;br&gt;
&#34;     AND gi.goods_code = ait.accessories_name) dat&#34;&lt;br&gt;
&#34; WHERE 1 = 1&#34;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;向 namenode 请求上传文件 /a/sss.txt&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;响应,可以上传&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;rpc 请求上传第一个 block(0-128M), 请求返回 datanode&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;返回 (datanode1,datanode2,datanode3)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;请求建立 block 传输通道 channel
5.1. 请求建立通道
5.2. 请求建立通道
6.1. 应答成功
6.2. 应答成功&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;请求建立通道&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;考虑因素: 空间/距离&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;上传数据时, datanode 的选择策略
1. 第一个副本先考虑跟 client 离最近的 (同机架)
2. 第二个副本再考虑跨机架挑选一个 datanode, 增加副本的可靠性
3. 第三个副本就在第一个副本同机架另外挑选一台 datanode 存放&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT
  dat.*,
  SUM(dat.rksl) AS count_rk,
  SUM(dat.cksl) AS count_ck,
  (SELECT
    SUM(zar.goods_count)
  FROM
    zh_asphalt_repertory zar
  WHERE dat.goods_code = zar.goods_code
    AND TYPE = &#39;01&#39;) AS count_kc,
  SUM(dat.scsl) AS count_sc,
  SUM(dat.pdsl) AS count_pdf
FROM
  (SELECT
    DATE_FORMAT(sp.storage_date, &#39;%Y-%m-%d&#39;) AS storage_date,
    sp.Storage_area,
    sp.reservoir_code,
    sp.goods_code,
    gi.goods_name,
    sp.brand,
    sp.model,
    sp.specifications,
    sp.Inventory_quantity AS rksl,
    &#39;&#39; AS cksl,
    &#39;&#39; AS scsl,
    &#39;&#39; AS pdsl
  FROM
    zh_sr_productin sp,
    zh_goods_info gi
  WHERE sp.goods_code = gi.goods_code
  UNION
  ALL
  SELECT
    mo.start_date,
    mo.Storage_area,
    ra.abbreviation,
    mo.goods_code,
    gi.goods_name,
    mo.brand,
    mo.model,
    mo.specifications,
    &#39;&#39; AS rksl,
    mo.number AS cksl,
    &#39;&#39; AS scsl,
    &#39;&#39; AS pdsl
  FROM
    zh_material_outbound mo,
    zh_goods_info gi,
    zh_reservoir_area ra
  WHERE mo.Storage_area = ra.reservoir_code
    AND mo.goods_code = gi.goods_code
  UNION
  ALL
  SELECT
    DATE_FORMAT(zps.start_time, &#39;%Y-%m-%d&#39;),
    pp.Storage_area,
    pp.reservoir_code,
    zps.Name_commodity,
    gi.goods_name,
    zps.brand,
    zps.model,
    zps.specifications,
    &#39;&#39; AS rksl,
    &#39;&#39; AS cksl,
    zps.Production_quantity AS scsl,
    &#39;&#39; AS pdsl
  FROM
    zh_production_subsidiary zps,
    zh_goods_info gi,
    zh_production_processing pp
  WHERE pp.Production_order = zps.production_order
    AND zps.goods_code = gi.goods_code
  UNION
  ALL
  SELECT
    DATE_FORMAT(ait.&lt;code&gt;data&lt;/code&gt;, &#39;%Y-%m-%d&#39;),
    ra.abbreviation,
    ait.storage_area,
    ait.accessories_name,
    gi.goods_name,
    ait.brand,
    ait.model,
    ait.specifications,
    &#39;&#39; AS rksl,
    &#39;&#39; AS cksl,
    &#39;&#39; AS scsl,
    ait.inventory_number AS pdsl
  FROM
    zh_additive_inventory_table ait,
    zh_goods_info gi,
    zh_reservoir_area ra
  WHERE ait.storage_area = ra.reservoir_code
    AND gi.goods_code = ait.accessories_name) dat
WHERE 1 = 1
GROUP BY dat.goods_code
ORDER BY dat.storage_date DESC&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT&#34;&lt;br&gt;
&#34;   dat.*,&#34;&lt;br&gt;
&#34;   SUM(dat.rksl) AS count_rk,&#34;&lt;br&gt;
&#34;   SUM(dat.cksl) AS count_ck,&#34;&lt;br&gt;
&#34;   (SELECT&#34;&lt;br&gt;
&#34;     SUM(zar.goods_count)&#34;&lt;br&gt;
&#34;   FROM&#34;&lt;br&gt;
&#34;     zh_asphalt_repertory zar&#34;&lt;br&gt;
&#34;   WHERE dat.goods_code = zar.goods_code&#34;&lt;br&gt;
&#34;     AND TYPE = &#39;01&#39;) AS count_kc,&#34;&lt;br&gt;
&#34;   SUM(dat.scsl) AS count_sc,&#34;&lt;br&gt;
&#34;   SUM(dat.pdsl) AS count_pdf&#34;&lt;br&gt;
&#34; FROM&#34;&lt;br&gt;
&#34;   (SELECT&#34;&lt;br&gt;
&#34;     DATE_FORMAT(sp.storage_date, &#39;%Y-%m-%d&#39;) AS storage_date,&#34;&lt;br&gt;
&#34;     sp.Storage_area,&#34;&lt;br&gt;
&#34;     sp.reservoir_code,&#34;&lt;br&gt;
&#34;     sp.goods_code,&#34;&lt;br&gt;
&#34;     gi.goods_name,&#34;&lt;br&gt;
&#34;     sp.brand,&#34;&lt;br&gt;
&#34;     sp.model,&#34;&lt;br&gt;
&#34;     sp.specifications,&#34;&lt;br&gt;
&#34;     sp.Inventory_quantity AS rksl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS cksl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS scsl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS pdsl&#34;&lt;br&gt;
&#34;   FROM&#34;&lt;br&gt;
&#34;     zh_sr_productin sp,&#34;&lt;br&gt;
&#34;     zh_goods_info gi&#34;&lt;br&gt;
&#34;   WHERE sp.goods_code = gi.goods_code&#34;&lt;br&gt;
&#34;   UNION&#34;&lt;br&gt;
&#34;   ALL&#34;&lt;br&gt;
&#34;   SELECT&#34;&lt;br&gt;
&#34;     mo.start_date,&#34;&lt;br&gt;
&#34;     ra.abbreviation AS Storage_area,&#34;&lt;br&gt;
&#34;     mo.Storage_area AS reservoir_code,&#34;&lt;br&gt;
&#34;     mo.goods_code,&#34;&lt;br&gt;
&#34;     gi.goods_name,&#34;&lt;br&gt;
&#34;     mo.brand,&#34;&lt;br&gt;
&#34;     mo.model,&#34;&lt;br&gt;
&#34;     mo.specifications,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS rksl,&#34;&lt;br&gt;
&#34;     mo.number AS cksl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS scsl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS pdsl&#34;&lt;br&gt;
&#34;   FROM&#34;&lt;br&gt;
&#34;     zh_material_outbound mo,&#34;&lt;br&gt;
&#34;     zh_goods_info gi,&#34;&lt;br&gt;
&#34;     zh_reservoir_area ra&#34;&lt;br&gt;
&#34;   WHERE mo.Storage_area = ra.reservoir_code&#34;&lt;br&gt;
&#34;     AND mo.goods_code = gi.goods_code&#34;&lt;br&gt;
&#34;   UNION&#34;&lt;br&gt;
&#34;   ALL&#34;&lt;br&gt;
&#34;   SELECT&#34;&lt;br&gt;
&#34;     DATE_FORMAT(zps.start_time, &#39;%Y-%m-%d&#39;),&#34;&lt;br&gt;
&#34;     pp.Storage_area,&#34;&lt;br&gt;
&#34;     pp.reservoir_code,&#34;&lt;br&gt;
&#34;     zps.goods_code,&#34;&lt;br&gt;
&#34;     zps.Name_commodity AS goods_name,&#34;&lt;br&gt;
&#34;     zps.brand,&#34;&lt;br&gt;
&#34;     zps.model,&#34;&lt;br&gt;
&#34;     zps.specifications,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS rksl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS cksl,&#34;&lt;br&gt;
&#34;     zps.Production_quantity AS scsl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS pdsl&#34;&lt;br&gt;
&#34;   FROM&#34;&lt;br&gt;
&#34;     zh_production_subsidiary zps,&#34;&lt;br&gt;
&#34;     zh_goods_info gi,&#34;&lt;br&gt;
&#34;     zh_production_processing pp&#34;&lt;br&gt;
&#34;   WHERE pp.Production_order = zps.production_order&#34;&lt;br&gt;
&#34;     AND zps.goods_code = gi.goods_code&#34;&lt;br&gt;
&#34;   UNION&#34;&lt;br&gt;
&#34;   ALL&#34;&lt;br&gt;
&#34;   SELECT&#34;&lt;br&gt;
&#34;     DATE_FORMAT(ait.&lt;code&gt;data&lt;/code&gt;, &#39;%Y-%m-%d&#39;),&#34;&lt;br&gt;
&#34;     ra.abbreviation,&#34;&lt;br&gt;
&#34;     ait.storage_area,&#34;&lt;br&gt;
&#34;     ait.accessories_name AS goods_code,&#34;&lt;br&gt;
&#34;     gi.goods_name,&#34;&lt;br&gt;
&#34;     ait.brand,&#34;&lt;br&gt;
&#34;     ait.model,&#34;&lt;br&gt;
&#34;     ait.specifications,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS rksl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS cksl,&#34;&lt;br&gt;
&#34;     &#39;&#39; AS scsl,&#34;&lt;br&gt;
&#34;     ait.inventory_number AS pdsl&#34;&lt;br&gt;
&#34;   FROM&#34;&lt;br&gt;
&#34;     zh_additive_inventory_table ait,&#34;&lt;br&gt;
&#34;     zh_goods_info gi,&#34;&lt;br&gt;
&#34;     zh_reservoir_area ra&#34;&lt;br&gt;
&#34;   WHERE ait.storage_area = ra.reservoir_code&#34;&lt;br&gt;
&#34;     AND gi.goods_code = ait.accessories_name) dat&#34;&lt;br&gt;
&#34; GROUP BY dat.reservoir_code,&#34;&lt;br&gt;
&#34;   dat.goods_code&#34;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;GRANT ALL PRIVILEGES ON &lt;strong&gt;.&lt;/strong&gt; TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;111111&#39; WITH GRANT OPTION;
mysql&amp;gt; flush privileges;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker run -it --rm \
  --net=hadoop \
  --volume-driver=cifs \
  -v 192.168.137.2/hadoop:/hadoop \
  --hostname hadoop-master \
  kiwenlau/hadoop:1.0&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;jdbc:mysql://mysql:3306/hive?createDatabaseIfNotExist=true&amp;lt;/value&amp;gt;
    &amp;lt;description&amp;gt;JDBC connect string for a JDBC metastore&amp;lt;/description&amp;gt;
  &amp;lt;/property&amp;gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
  &amp;lt;description&amp;gt;Driver class name for a JDBC metastore&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;root&amp;lt;/value&amp;gt;
  &amp;lt;description&amp;gt;username to use against metastore database&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;111111&amp;lt;/value&amp;gt;
    &amp;lt;description&amp;gt;password to use against metastore database&amp;lt;/description&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;create database shizhan03;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;use shizhan03;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;create table t_sz01(id int,name string);&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;drop table t_sz01;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;create table t_sz01(id int,name string) row format delimited fields terminated by &#39;,&#39;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;vi /sz.dat&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1,zhangsan
2,lisi
3,fengjie
4,chunge
5,shizi
6,koukjj&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;hadoop fs -put /sz.dat /user/hive/warehouse/shizhan03.db/t_sz01&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;select * from t_sz01;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;select count(1) from t_sz01;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;select id,name from t_sz01 where id&amp;gt;3;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;show tables;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;create exte&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;sed -i &#39;s/dishui.oschina.io\/note-hugo/localhost:1313/g&#39; themes/mainroad/static/js/app.js content/post/base.adoc config.toml&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;git checkout&amp;#8201;&amp;#8212;&amp;#8201;themes/mainroad/static/js/app.js content/post/base.adoc config.toml&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cat &amp;lt;&amp;lt;&amp;lt; &#39;FLUME_CLASSPATH=&#34;/opt/flume/lib&#34;&#39; &amp;gt; flume-env.sh&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;curl &lt;a href=&#34;https://raw.githubusercontent.com/burnettk/delete-docker-registry-image/master/delete_docker_registry_image.py&#34; class=&#34;bare&#34;&gt;https://raw.githubusercontent.com/burnettk/delete-docker-registry-image/master/delete_docker_registry_image.py&lt;/a&gt; | sudo tee /opt/bin/delete_docker_registry_image &amp;gt;/dev/null&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;sudo chmod a+x /opt/bin/delete_docker_registry_image&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;export REGISTRY_DATA_DIR=/home/core/registry/docker/registry/v2&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;{
    &#34;server&#34;:&#34;0.0.0.0&#34;,
    &#34;server_port&#34;:8888,
    &#34;local_address&#34;: &#34;127.0.0.1&#34;,
    &#34;local_port&#34;:1080,
    &#34;password&#34;:&#34;111111&#34;,
    &#34;timeout&#34;:300,
    &#34;method&#34;:&#34;aes-256-cfb&#34;,
    &#34;fast_open&#34;: false,
    &#34;workers&#34;: 1
}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT zspl.*
FROM zh_sub_plan_launch zspl,
zh_send_goods_check zsgc
WHERE zspl.plan_code IN
(SELECT plan_code
FROM zh_plan_launch zpl
WHERE zpl.state = &#39;01&#39;
AND zpl.table_date &amp;gt;= &#39;2017-03-24 14:44:19&#39;
AND zpl.table_date &amp;#8656; &#39;2017-03-24 23:44:25&#39;)
AND zspl.project_name = zsgc.project_name
AND zsgc.make_time = &#39;2017-03-24&#39;
AND zsgc.status  = &#39;4&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;storm jar examples/storm-starter/storm-starter-topologies-0.9.6.jar org.apache.storm.starter.WordCountTopology wordcount&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;2017-03-28_163004
2017-03-28_163014
2017-03-28_163023
2017-03-28_163035
2017-03-28_163042
2017-03-28_163050
2017-03-28_163058
2017-03-28_163734
2017-03-28_163743
2017-03-28_163751
2017-03-28_163757
2017-03-28_163804
2017-03-28_163811&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/.png&#34; alt=&#34;.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker run -d -p 8888:1984 oddrationale/docker-shadowsocks -s 0.0.0.0 -p 1984 -k 111111 -m aes-256-cfb&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;/v2/tutum/influxdb/manifests/0.9&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT zspl.* FROM zh_sub_plan_launch zspl, zh_send_goods_check zsgc WHERE zspl.plan_code IN (SELECT plan_code FROM zh_plan_launch zpl WHERE zpl.state = &#39;01&#39; AND zpl.table_date &amp;gt;= &#39;2017-03-29 00:35:07&#39; AND zpl.table_date &amp;#8656; &#39;2017-03-29 23:35:14&#39;) AND zspl.project_name = zsgc.project_name AND zsgc.make_time = &#39;2017-03-29&#39; AND zspl.goods_code = zsgc.goods_name AND zsgc.status  = &#39;4&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;hi 陈经理:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;大物流服务器配置(高-中-低),在附件的三个sheet里,请下载查看&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;谢谢&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;version: &#39;2&#39;
services:
  registry2:
    image: dishui.io:5000/registry:2.5
    container_name: registry2
    ports:
      - &#34;5000:5000&#34;
    environment:
      REGISTRY_HTTP_TLS_CERTIFICATE: /certs/registry.crt
      REGISTRY_HTTP_TLS_KEY: /certs/registry.key
    volumes:
      - /home/core/registrydata:/var/lib/registry
      - /home/core/registry/certs:/certs&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;2017-04-01_104025
2017-04-01_104135
2017-04-01_104143
2017-04-01_104152
2017-04-01_104159
2017-04-01_104213&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;val lines = List(&#34;hello tom hello jerry&#34;,&#34;hello tom kitty hello hello&#34;)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;lines.flatMap(&lt;em&gt;.split(&#34; &#34;)).map&lt;/em&gt;,1.groupBy(&lt;em&gt;._1).map(t &amp;#8658; (t._1,t._2.size)).toList.sortBy(&lt;/em&gt;._2)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;io.dishui.test.construct&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;2017-04-07_145546.png
2017-04-07_145555.png
2017-04-07_145603.png
2017-04-07_145612.png
2017-04-07_145620.png
2017-04-07_145627.png&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;2017-04-07_151513.png
2017-04-07_151523.png&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://master:7077 \
--executor-memory 512M \
--total-executor-cores 1 \
lib/spark-examples-1.5.2-hadoop2.2.0.jar \
10&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;/bin/spark-submit \&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;--class org.apache.spark.examples.SparkPi \
--master spark://master:7077 \
--executor-memory 512M \
--total-executor-cores 2 \
/usr/local/spark-1.5.2/lib/spark-examples-1.5.2-hadoop2.2.0.jar \
10&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cd $SPARK_HOME&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;bin/spark-shell --master spark://master:7077&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cat &amp;gt; log4j.properties &amp;lt;&amp;lt;_EOF_
# Set everything to be logged to the console
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n&lt;/p&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_settings_to_quiet_third_party_logs_that_are_too_verbose&#34; class=&#34;sect0&#34;&gt;Settings to quiet third party logs that are too verbose&lt;/h1&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;log4j.logger.org.spark-project.jetty=WARN
log4j.logger.org.spark-project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR&lt;/p&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_spark_9183_settings_to_avoid_annoying_messages_when_looking_up_nonexistent_udfs_in_sparksql_with_hive_support&#34; class=&#34;sect0&#34;&gt;SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support&lt;/h1&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
&lt;em&gt;EOF&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#39;2017-04-11 00:26:06&#39;
&#39;2017-04-11 23:26:06&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT zpl.* FROM zh_plan_launch zpl WHERE zpl.state = &#39;01&#39; AND zpl.table_date &amp;gt;= &#39;2017-04-10 00:26:06&#39; AND zpl.table_date &amp;#8656; &#39;2017-04-10 23:26:06&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT zspl.* FROM zh_sub_plan_launch zspl, zh_send_goods_check zsgc WHERE zspl.plan_code IN (SELECT plan_code FROM zh_plan_launch zpl WHERE zpl.state = &#39;01&#39; AND zpl.table_date &amp;gt;= &#39;2017-04-10 00:26:06&#39; AND zpl.table_date &amp;#8656; &#39;2017-04-10 23:26:06&#39;) AND zspl.project_name = zsgc.project_name AND zsgc.make_time = &#39;2017-04-10&#39; AND zspl.goods_code = zsgc.goods_name AND zsgc.status  = &#39;4&#39; GROUP BY zspl.reservoir_code&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT zspl.* FROM zh_sub_plan_launch zspl, zh_send_goods_check zsgc WHERE zspl.plan_code IN (SELECT plan_code FROM zh_plan_launch zpl WHERE zpl.state = &#39;01&#39; AND zpl.table_date &amp;gt;= &#39;2017-04-10 00:26:06&#39; AND zpl.table_date &amp;#8656; &#39;2017-04-10 23:26:06&#39;) AND zspl.project_name = zsgc.project_name AND zsgc.make_time = &#39;2017-04-10 AND zspl.goods_code = zsgc.goods_name AND zsgc.status  = &#39;4&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT zspl.* FROM zh_sub_plan_launch zspl, zh_send_goods_check zsgc WHERE zspl.plan_code IN (SELECT plan_code FROM zh_plan_launch zpl WHERE zpl.state = &#39;01&#39; AND zpl.table_date &amp;gt;= &#39;2017-04-10 00:26:06&#39; AND zpl.table_date &amp;#8656; &#39;2017-04-10 23:26:06&#39;) AND zspl.project_name = zsgc.project_name AND zsgc.make_time = &#39;2017-04-10&#39; AND zspl.goods_code = zsgc.goods_name AND zsgc.status  = &#39;4&amp;#8217;GROUP BY id&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34;SELECT &#34;&lt;br&gt;
&#34;  zpl.* &#34;&lt;br&gt;
&#34;FROM &#34;&lt;br&gt;
&#34;  zh_plan_launch zpl &#34;&lt;br&gt;
&#34;WHERE zpl.state = &#39;01&#39; &#34;&lt;br&gt;
&#34;  AND zpl.plan_code IN &#34;&lt;br&gt;
&#34;  (SELECT &#34;&lt;br&gt;
&#34;    zspl.plan_code &#34;&lt;br&gt;
&#34;  FROM &#34;&lt;br&gt;
&#34;    zh_sub_plan_launch zspl, &#34;&lt;br&gt;
&#34;    zh_send_goods_check zsgc &#34;&lt;br&gt;
&#34;  WHERE zspl.plan_code IN &#34;&lt;br&gt;
&#34;    (SELECT &#34;&lt;br&gt;
&#34;      plan_code &#34;&lt;br&gt;
&#34;    FROM &#34;&lt;br&gt;
&#34;      zh_plan_launch zpl &#34;&lt;br&gt;
&#34;    WHERE zpl.state = &#39;01&#39; &#34;&lt;br&gt;
&#34;      AND zpl.table_date &amp;gt;= ? &#34;&lt;br&gt;
&#34;      AND zpl.table_date &amp;#8656; ?) &#34;&lt;br&gt;
&#34;    AND zspl.project_name = zsgc.project_name &#34;&lt;br&gt;
&#34;    AND zsgc.make_time = ? &#34;&lt;br&gt;
&#34;    AND zspl.goods_code = zsgc.goods_name &#34;&lt;br&gt;
&#34;    AND zsgc.status = &#39;4&#39; &#34;&lt;br&gt;
&#34;    AND zspl.plan_code NOT IN &#34;&lt;br&gt;
&#34;    (SELECT DISTINCT &#34;&lt;br&gt;
&#34;      (zspl.plan_code) &#34;&lt;br&gt;
&#34;    FROM &#34;&lt;br&gt;
&#34;      zh_sub_plan_launch zspl &#34;&lt;br&gt;
&#34;    WHERE zspl.plan_code IN &#34;&lt;br&gt;
&#34;      (SELECT &#34;&lt;br&gt;
&#34;        plan_code &#34;&lt;br&gt;
&#34;      FROM &#34;&lt;br&gt;
&#34;        zh_plan_launch zpl &#34;&lt;br&gt;
&#34;      WHERE zpl.state = &#39;01&#39; &#34;&lt;br&gt;
&#34;        AND zpl.table_date &amp;gt;= ? &#34;&lt;br&gt;
&#34;        AND zpl.table_date &amp;#8656; ?)) &#34;&lt;br&gt;
&#34;  GROUP BY zspl.plan_code) &#34;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;String subPlanLanchSql = &#34;SELECT zspl.* &#34;&lt;br&gt;
        &#34;FROM zh_sub_plan_launch zspl, &#34;&lt;br&gt;
        &#34;zh_send_goods_check zsgc &#34;&lt;br&gt;
        &#34;WHERE zspl.plan_code IN &#34;&lt;br&gt;
        &#34;(SELECT plan_code &#34;&lt;br&gt;
        &#34;FROM zh_plan_launch zpl &#34;&lt;br&gt;
        &#34;WHERE zpl.state = &#39;01&#39; &#34;&lt;br&gt;
        &lt;span class=&#34;menuseq&#34;&gt;&lt;span class=&#34;menu&#34;&gt;AND zpl.table_date&lt;/span&gt;&amp;#160;&amp;#9656; &lt;span class=&#34;menuitem&#34;&gt;= ?&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
        &#34;AND zpl.table_date &amp;#8656; ?) &#34;&lt;br&gt;
        &#34;AND zspl.project_name = zsgc.project_name &#34;&lt;br&gt;
        &#34;AND zsgc.make_time = ? &#34;+
        &#34;AND zspl.goods_code = zsgc.goods_name &#34;+
        &#34;AND zsgc.status  = &#39;4&#39;&#34;&lt;br&gt;
        &#34;GROUP BY id&#34;;
    String difPlanLanchSql = &#34;SELECT &#34;&lt;br&gt;
        &#34;  zpl.* &#34;&lt;br&gt;
        &#34;FROM &#34;&lt;br&gt;
        &#34;  zh_plan_launch zpl &#34;&lt;br&gt;
        &#34;WHERE zpl.state = &#39;01&#39; &#34;&lt;br&gt;
        &#34;  AND zpl.plan_code IN &#34;&lt;br&gt;
        &#34;  (SELECT &#34;&lt;br&gt;
        &#34;    zspl.plan_code &#34;&lt;br&gt;
        &#34;  FROM &#34;&lt;br&gt;
        &#34;    zh_sub_plan_launch zspl, &#34;&lt;br&gt;
        &#34;    zh_send_goods_check zsgc &#34;&lt;br&gt;
        &#34;  WHERE zspl.plan_code IN &#34;&lt;br&gt;
        &#34;    (SELECT &#34;&lt;br&gt;
        &#34;      plan_code &#34;&lt;br&gt;
        &#34;    FROM &#34;&lt;br&gt;
        &#34;      zh_plan_launch zpl &#34;&lt;br&gt;
        &#34;    WHERE zpl.state = &#39;01&#39; &#34;&lt;br&gt;
        &#34;      AND zpl.table_date &amp;gt;= ? &#34;&lt;br&gt;
        &#34;      AND zpl.table_date &amp;#8656; ?) &#34;&lt;br&gt;
        &#34;    AND zspl.project_name = zsgc.project_name &#34;&lt;br&gt;
        &#34;    AND zsgc.make_time = ? &#34;&lt;br&gt;
        &#34;    AND zspl.goods_code = zsgc.goods_name &#34;&lt;br&gt;
        &#34;    AND zsgc.status = &#39;4&#39; &#34;&lt;br&gt;
        &#34;    AND zspl.plan_code IN &#34;&lt;br&gt;
        &#34;    (SELECT DISTINCT &#34;&lt;br&gt;
        &#34;      (zspl.plan_code) &#34;&lt;br&gt;
        &#34;    FROM &#34;&lt;br&gt;
        &#34;      zh_sub_plan_launch zspl &#34;&lt;br&gt;
        &#34;    WHERE zspl.plan_code IN &#34;&lt;br&gt;
        &#34;      (SELECT &#34;&lt;br&gt;
        &#34;        plan_code &#34;&lt;br&gt;
        &#34;      FROM &#34;&lt;br&gt;
        &#34;        zh_plan_launch zpl &#34;&lt;br&gt;
        &#34;      WHERE zpl.state = &#39;01&#39; &#34;&lt;br&gt;
        &#34;        AND zpl.table_date &amp;gt;= ? &#34;&lt;br&gt;
        &#34;        AND zpl.table_date &amp;#8656; ?)) &#34;&lt;br&gt;
        &#34;  GROUP BY zspl.plan_code) &#34;;
    String planLanchSql = &#34;SELECT zpl.* &#34;&lt;br&gt;
        &#34;FROM zh_plan_launch zpl &#34;&lt;br&gt;
        &#34;WHERE zpl.state = &#39;01&#39; &#34;&lt;br&gt;
        &lt;span class=&#34;menuseq&#34;&gt;&lt;span class=&#34;menu&#34;&gt;AND zpl.table_date&lt;/span&gt;&amp;#160;&amp;#9656; &lt;span class=&#34;menuitem&#34;&gt;= ?&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
        &#34;AND zpl.table_date &amp;#8656; ? &#34;;
    String reservoirCodeSql = subPlanLanchSql + &#34; ,zspl.reservoir_code&#34;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;    Object[] args1 = new Object[]{DateUtil2.getCurDayStart(),DateUtil2.getCurDayEnd(),DateUtil2.getDate()};
    Object[] args2 = new Object[]{DateUtil2.getCurDayStart(),DateUtil2.getCurDayEnd()};
    Object[] args3 = new Object[]{DateUtil2.getCurDayStart(),DateUtil2.getCurDayEnd(),DateUtil2.getDate(),DateUtil2.getCurDayStart(),DateUtil2.getCurDayEnd()};
    List&amp;lt;?&amp;gt; subPlanLanchs = deliveryPlanService.integratePlan(subPlanLanchSql, args1, new SubPlanLaunchModel());
    List&amp;lt;?&amp;gt; diffPlanLanchs = deliveryPlanService.integratePlan(difPlanLanchSql, args3, new PlanLaunchModel());
//    List&amp;lt;?&amp;gt; planLanchs =  deliveryPlanService.integratePlan(planLanchSql, args2, new PlanLaunchModel());
    List&amp;lt;?&amp;gt; reservoirs = deliveryPlanService.integratePlan(reservoirCodeSql, args1,new SubPlanLaunchModel());&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cat &amp;gt; /person.txt &amp;lt;&amp;lt;_EOF_
1,a,23
2,hello,34
3,xxxx,12
&lt;em&gt;EOF&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;val rdd1 = sc.textFile(&#34;hdfs://master:9000/word/&#34;).flatMap(&lt;em&gt;.split(&#34; &#34;)).map&lt;/em&gt;, 1&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;scala&amp;gt; sc.textFile(&#34;hdfs://master:8020/word/&#34;)
res2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at textFile at &amp;lt;console&amp;gt;:22&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;scala&amp;gt; sc.textFile(&#34;hdfs://master:8020/word/&#34;).collect
res3: Array[String] = Array(hello tom, hello xixi, hello xxxxooo, hello tom, hello xixi, hello xxxxooo, hello tom, hello xixi, hello xxxxooo)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;scala&amp;gt; val rdd1 = sc.textFile(&#34;hdfs://master:8020/word/&#34;).flatMap(_.split(&#34; &#34;)).collect
rdd1: Array[String] = Array(hello, tom, hello, xixi, hello, xxxxooo, hello, tom, hello, xixi, hello, xxxxooo, hello, tom, hello, xixi, hello, xxxxooo)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT &#34;&lt;br&gt;
&#34;   * &#34;&lt;br&gt;
&#34; FROM &#34;&lt;br&gt;
&#34;   (SELECT &#34;&lt;br&gt;
&#34;     *, &#34;&lt;br&gt;
&#34;     DATE_FORMAT(zas.out_date, &#39;%Y-%m-%d&#39;) AS dat, &#34;&lt;br&gt;
&#34;     (SELECT &#34;&lt;br&gt;
&#34;       SUM(zasc.goods_count) &#34;&lt;br&gt;
&#34;     FROM &#34;&lt;br&gt;
&#34;       zh_asphalt_storage_count zasc &#34;&lt;br&gt;
&#34;     WHERE zasc.asphalt_number = zas.out_order_no) AS goods_count, &#34;&lt;br&gt;
&#34;     (SELECT &#34;&lt;br&gt;
&#34;       zra.abbreviation &#34;&lt;br&gt;
&#34;     FROM &#34;&lt;br&gt;
&#34;       zh_reservoir_area zra &#34;&lt;br&gt;
&#34;     WHERE zra.reservoir_code = zas.Storage_area) AS abbreviation &#34;&lt;br&gt;
&#34;   FROM &#34;&lt;br&gt;
&#34;     zh_out_bound_order zas &#34;&lt;br&gt;
&#34;   ORDER BY zas.out_date DESC) AS tmp1 &#34;&lt;br&gt;
&#34; GROUP BY tmp1.Storage_area,tmp1.dat &#34;&lt;br&gt;
&#34; HAVING 1 = 1 &#34;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT &#34;&lt;br&gt;
&#34;   *, &#34;&lt;br&gt;
&#34;   (SELECT &#34;&lt;br&gt;
&#34;     SUM(zasc.goods_count) &#34;&lt;br&gt;
&#34;   FROM &#34;&lt;br&gt;
&#34;     zh_asphalt_storage_count zasc &#34;&lt;br&gt;
&#34;   WHERE zasc.asphalt_number = zas.out_order_no) AS goods_count &#34;&lt;br&gt;
&#34; FROM &#34;&lt;br&gt;
&#34;   zh_out_bound_order AS zas &#34;&lt;br&gt;
&#34; WHERE DATE_FORMAT(zas.out_date, &#39;%Y-%m-%d&#39;) = &#39;2017-04-13&#39; &#34;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;bin/spark-submit \
--class StreamingWordCount \
--master spark://master:7077 \
/streaming-1.0-SNAPSHOT.jar \
worker \
10010 \
hdfs://master:8020/nc&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;netsh interface portproxy add v4tov4 listenport=3333 connectaddress=192.168.137.15 connectport=6066&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;101.40.208.232:22&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker run --name shadow -d -p 80:1984 oddrationale/docker-shadowsocks -s 0.0.0.0 -p 1984 -k 111111 -m aes-256-cfb&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;101.40.208.232
80
111111&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;exampleblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;环境:&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&#34;tableblock frame-all grid-all spread&#34;&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 50%;&#34;&gt;
&lt;col style=&#34;width: 50%;&#34;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&#34;tableblock halign-left valign-top&#34;&gt;版本&lt;/th&gt;
&lt;th class=&#34;tableblock halign-left valign-top&#34;&gt;地址&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-left valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;hadoop2.6&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-left valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;&lt;a href=&#34;http://pan.baidu.com/s/1nvJkKeH&#34;&gt;密码：roxm&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf -g &#39;daemon off;&#39;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;(function () {
    var params = {};
    //Document对象数据
    if(document) {
        params.domain = document.domain || &#39;&#39;;
        params.url = document.URL || &#39;&#39;;
        params.title = document.title || &#39;&#39;;
        params.referrer = document.referrer || &#39;&#39;;
    }
    //Window对象数据
    if(window &amp;amp;&amp;amp; window.screen) {
        params.sh = window.screen.height || 0;
        params.sw = window.screen.width || 0;
        params.cd = window.screen.colorDepth || 0;
    }
    //navigator对象数据
    if(navigator) {
        params.lang = navigator.language || &#39;&#39;;
    }
    //解析_maq配置
    if(_maq) {
        for(var i in _maq) {
            switch(_maq[i][0]) {
                case &#39;_setAccount&#39;:
                    params.account = _maq[i][1];
                    break;
                default:
                    break;
            }
        }
    }
    //拼接参数串
    var args = &#39;&#39;;
    for(var i in params) {
        if(args != &#39;&#39;) {
            args += &#39;&amp;amp;&#39;;
        }
        args += i + &#39;=&#39; + encodeURIComponent(params[i]);
    }&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;    //通过Image对象请求后端脚本
    var img = new Image(1, 1);
    img.src = &#39;192.168.137.15/log.gif?&#39; + args;
})();&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;docker pull wurstmeister/kafka&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;if [[ -z &#34;$a&#34; ]]; then
  echo &#34;a&#34;
else
  echo &#34;b&#34;
fi&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;input {
  file {
    path &amp;#8658; &#34;/var/nginx_logs/*.log&#34;
    discover_interval &amp;#8658; 5
    start_position &amp;#8658; &#34;beginning&#34;
  }
}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;output {
  kafka {
    topic_id &amp;#8658; &#34;tracklog&#34;
    codec &amp;#8658; plain {
      format &amp;#8658; &#34;%{message}&#34;
      charset &amp;#8658; &#34;UTF-8&#34;
    }
    bootstrap_servers &amp;#8658; &#34;kafka:9092,kafka2:9092&#34;
  }
}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;INSERT INTO zh_asphalt_repertory (id,reservoir_code,storage_code,goods_code,goods_count,goods_batch,order_no,order_type,type,occur_date,goods_model)VALUES(&#39;a54a38c44cb4417aa975c3581879e41a&#39;,&#39;002&#39;,&#39;TKH-ZHIQU2&#39;,&#39;003&#39;,&#39;-4351.72&#39;,&#39;null&#39;,&#39;20170422389&#39;,&#39;03&#39;,&#39;00&#39;,&#39;2017-04-22 00:00:00&#39;,&#39;70#&#39;)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;UPDATE &lt;code&gt;wuliu&lt;/code&gt;.&lt;code&gt;zh_contract_info&lt;/code&gt; SET &lt;code&gt;execute_state&lt;/code&gt; = &#39;01&#39; WHERE &lt;code&gt;contract_code&lt;/code&gt; IN(&#39;35900000-17-MY0635-0006&amp;#8212;&amp;#8203;03&#39;,&#39;35900000-17-MY0635-0006&amp;#8212;&amp;#8203;05&#39;,&#39;35900000-17-MY0635-0006&amp;#8212;&amp;#8203;04&#39;);&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;UPDATE
  &lt;code&gt;wuliu&lt;/code&gt;.&lt;code&gt;zh_contract_info&lt;/code&gt;
SET
  &lt;code&gt;execute_state&lt;/code&gt; = &#39;01&#39;
WHERE &lt;code&gt;contract_code&lt;/code&gt; IN (
    &#39;35900000-17-MY0635-0006&amp;#8212;&amp;#8203;03&#39;,
    &#39;35900000-17-MY0635-0006&amp;#8212;&amp;#8203;05&#39;,
    &#39;35900000-17-MY0635-0006&amp;#8212;&amp;#8203;04&#39;
  ) ;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;SELECT
  *,
  zas.Storage_area,
  zas.project_name,
  DATE_FORMAT(zas.out_date, &#39;%Y-%m-%d&#39;) AS dat,
  (SELECT
    SUM(zasc.goods_count)
  FROM
    zh_asphalt_storage_count zasc
  WHERE zasc.asphalt_number = zas.out_order_no) AS goods_count,
  (SELECT
    zra.abbreviation
  FROM
    zh_reservoir_area zra
  WHERE zra.reservoir_code = zas.Storage_area) AS abbreviation
FROM
  zh_out_bound_order zas
WHERE 1 = 1
GROUP BY dat,zas.Storage_area
ORDER BY zas.out_date DESC&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&#34; SELECT &#34;+
&#34;   zas.Storage_area, &#34;+
&#34;   zas.goods_code, &#34;+
&#34;   zas.model, &#34;+
&#34;   zas.category, &#34;+
&#34;   DATE_FORMAT(zas.out_date, &#39;%Y-%m-%d&#39;) AS dat, &#34;+
&#34;   SUMSELECT &#34;+ &#34;     SUM(zasc.goods_count) &#34;+ &#34;   FROM &#34;+ &#34;     zh_asphalt_storage_count zasc &#34;+ &#34;   WHERE zasc.asphalt_number = zas.out_order_no AS goods_count &#34;+
&#34; FROM &#34;+
&#34;   zh_out_bound_order zas &#34;+
&#34; WHERE 1 = 1 &#34;+
&#34; AND DATE_FORMAT(zas.out_date, &#39;%Y-%m-%d&#39;) = &#39;2017-03-02&#39; &#34;+
&#34; AND zas.category = &#39;00&#39; &#34;+
&#34; AND zas.Storage_area = &#39;002&#39; &#34;+
&#34; GROUP BY zas.goods_code &#34;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;hi 陈经理:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;服务器系统
CentOS-7-x86_64-Minimal-1611.iso&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;大物流正式服务器,配置双网卡(内,外网)
大物流测试服务器,配置双网卡(内,外网)
其余的服务器,配置内网网卡
服务器之间走内网通信&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;谢谢&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;echo &#34;4|2016年2月1日,星期一,10:04:14|10.117.45.20|DongKe小雨|法师|女|7|0|588/800000000&#34; &amp;gt; game_logs/1.log&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;echo &#34;4|2016年2月1日,星期一,10:04:16|10.168.8.103|潮流哥|法师|男|9|0|517/800000000&#34; &amp;gt;&amp;gt; game_logs/1.log
echo &#34;4|2016年2月1日,星期一,10:04:16|10.168.8.103|潮流锅|武士|男|9|0|779/800000000&#34; &amp;gt;&amp;gt; game_logs/1.log&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A&amp;#8592;-] : RequestCreated
deactivate A
[&amp;#8592; A: Done
deactivate A&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Schedules tasks for multiple types of clusters by acting through a SchedulerBackend.
It can also work with a local setup by using a LocalBackend and setting isLocal to true.
It handles common logic, like determining a scheduling order across jobs, waking up to launch speculative tasks, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Clients should first call initialize() and start(), then submit task sets through the
runTasks method.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;THREADING: SchedulerBackends and task-submitting clients can call this class from multiple
threads, so it needs locks in public API methods to maintain its state. In addition, some
SchedulerBackends synchronize on themselves when they want to send events here, and then
acquire a lock on us, so we need to make sure that we don&amp;#8217;t try to lock the backend while
we are holding a lock on ourselves.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.mailiqing.com/program/admin/mlqcategory/admin_goods/goodsList.jsp?menu_id=20150914j018W0M&#34; class=&#34;bare&#34;&gt;http://www.mailiqing.com/program/admin/mlqcategory/admin_goods/goodsList.jsp?menu_id=20150914j018W0M&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;#local
public: &lt;a href=&#34;http://maven.aliyun.com/nexus/content/groups/public/#这个maven&#34; class=&#34;bare&#34;&gt;http://maven.aliyun.com/nexus/content/groups/public/#这个maven&lt;/a&gt;
typesafe:http://dl.bintray.com/typesafe/ivy-releases/ , [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly#这个ivy
ivy-sbt-plugin:http://dl.bintray.com/sbt/sbt-plugin-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]#这个ivy
sonatype-oss-releases&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;sonatype-oss-snapshots&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;进销存报表
采购执行报表
销售执行报表&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>spark-源码</title>
      <link>/post/bigdata/spark/spark-%E6%BA%90%E7%A0%81/</link>
      <pubDate>Wed, 26 Apr 2017 15:43:39 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-%E6%BA%90%E7%A0%81/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;spark-基础&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_sparkcontext&#34;&gt;1. SparkContext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_job的提交和运行&#34;&gt;2. job的提交和运行&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_sparkcontext&#34;&gt;1. SparkContext&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/post/bigdata/spark/uml/sparkContext.svg&#34; alt=&#34;sparkContext&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_job的提交和运行&#34;&gt;2. job的提交和运行&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/plantuml/spark-job-1.svg&#34; alt=&#34;spark job 1&#34; width=&#34;1112&#34; height=&#34;947&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/plantuml/spark-job-2.svg&#34; alt=&#34;spark job 2&#34; width=&#34;1575&#34; height=&#34;714&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>wuliu项目</title>
      <link>/tmp/wuliuxiugai/</link>
      <pubDate>Wed, 26 Apr 2017 13:25:36 +0000</pubDate>
      
      <guid>/tmp/wuliuxiugai/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Contents&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_表关系&#34;&gt;1. 表关系&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_表关系&#34;&gt;1. 表关系&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/plantuml/wuliu-1.png&#34; alt=&#34;wuliu 1&#34; width=&#34;1099&#34; height=&#34;647&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;zh_asphalt_storage 沥青的入库通知单
zh_asphalt_storage_count 沥青的入库数量
zh_storage_in_notice 沥青入库通知单&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ALTER TABLE &lt;code&gt;zh_sub_plan_launch&lt;/code&gt; ADD COLUMN &lt;code&gt;reservoir_code&lt;/code&gt; VARCHAR(255) NULL COMMENT &#39;所属库区&#39; AFTER &lt;code&gt;create_date&lt;/code&gt;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ALTER TABLE &lt;code&gt;zh_sub_plan&lt;/code&gt; ADD COLUMN &lt;code&gt;reservoir_code&lt;/code&gt; VARCHAR(255) NULL COMMENT &#39;所属库区&#39; AFTER &lt;code&gt;create_date&lt;/code&gt;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ALTER TABLE &lt;code&gt;zh_plan_launch&lt;/code&gt; ADD COLUMN &lt;code&gt;user_id&lt;/code&gt; VARCHAR(50) NULL COMMENT &#39;登录人&#39; AFTER &lt;code&gt;plan_memo&lt;/code&gt;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ALTER TABLE &lt;code&gt;zh_plan&lt;/code&gt; CHANGE &lt;code&gt;table_date&lt;/code&gt; &lt;code&gt;table_date&lt;/code&gt; DATETIME NULL, CHANGE &lt;code&gt;release_time&lt;/code&gt; &lt;code&gt;release_time&lt;/code&gt; DATETIME NULL, CHANGE &lt;code&gt;check_time&lt;/code&gt; &lt;code&gt;check_time&lt;/code&gt; DATETIME NULL, CHANGE &lt;code&gt;change_date&lt;/code&gt; &lt;code&gt;change_date&lt;/code&gt; DATETIME NULL COMMENT &#39;变更时间&#39;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ALTER TABLE &lt;code&gt;zh_sub_plan&lt;/code&gt; CHANGE &lt;code&gt;plan_date&lt;/code&gt; &lt;code&gt;plan_date&lt;/code&gt; DATE NULL;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ALTER TABLE &lt;code&gt;zh_plan&lt;/code&gt; ADD COLUMN &lt;code&gt;user_id&lt;/code&gt; VARCHAR(20) NULL COMMENT &#39;登陆人&#39; AFTER &lt;code&gt;plan_memo&lt;/code&gt;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ALTER TABLE &lt;code&gt;wuliu2&lt;/code&gt;.&lt;code&gt;zh_asphalt_storage&lt;/code&gt; ADD COLUMN &lt;code&gt;category&lt;/code&gt; VARCHAR(50) NULL COMMENT &#39;类目&#39; AFTER &lt;code&gt;Name_commodity&lt;/code&gt;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ALTER TABLE &lt;code&gt;wuliu&lt;/code&gt;.&lt;code&gt;zh_out_bound_order&lt;/code&gt; ADD COLUMN &lt;code&gt;category&lt;/code&gt; VARCHAR(20) NULL COMMENT &#39;类目&#39; AFTER &lt;code&gt;out_date&lt;/code&gt;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ALTER TABLE &lt;code&gt;wuliu&lt;/code&gt;.&lt;code&gt;zh_material_outbound&lt;/code&gt; ADD COLUMN &lt;code&gt;goods_name&lt;/code&gt; VARCHAR(30) NULL COMMENT &#39;商品名称&#39; AFTER &lt;code&gt;out_date&lt;/code&gt;;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;计划调度管理&lt;/p&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;计划调度指令&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;WebRoot/web/plandispatch/planinstructions/plan.jsp&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;添加&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;WebRoot/web/plandispatch/planinstructions/plan_add.jsp
/plandispatch/planInstructions!addStorageNotice.action&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;合同管理(zh_contract_info)&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;采购合同管理&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;WebRoot/web/contractmanagement/purchasercontract/purchase_contractinfo.jsp&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;提货通知单(zh_storage_out_notice)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;计划调度管理&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;发货计划整合&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;WebRoot/web/plandispatch/deliveryplan/delivery_plan.jsp&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;发货申请审批&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;WebRoot/web/sendoutgoodscheck/sendgoods_check_manage.jsp
/sendgoodscheckaction/sendgoodscheck!findSendGoodsCheckData.action
zh_send_goods_check&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;仓储管理&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;库存&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;库存结余调整信息(zh_balance_table)&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;WebRoot/web/storagemanagement/repertory/repertorysurplus/repertory_surplus.jsp
/zhbalancetable/zhBalanceTableAction!queryAll.action&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;合同管理(zh_contract_info)&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;contractInfo/contract!queryContractInfoForPage.action&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;POST /task?id=1 HTTP/1.1
Host: example.org
Content-Type: application/json; charset=utf-8
Content-Length: 137

{
  &#34;status&#34;: &#34;ok&#34;,
  &#34;extended&#34;: true,
  &#34;results&#34;: [
    {&#34;value&#34;: 0, &#34;type&#34;: &#34;int64&#34;},
    {&#34;value&#34;: 1.0e+3, &#34;type&#34;: &#34;decimal&#34;}
  ]
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>采集日志服务 nginx log</title>
      <link>/post/bigdata/spark/nginx-log/</link>
      <pubDate>Thu, 20 Apr 2017 16:38:02 +0000</pubDate>
      
      <guid>/post/bigdata/spark/nginx-log/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;log&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_docker_方式&#34;&gt;1. Docker 方式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_安装&#34;&gt;2. 安装&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_安装依赖&#34;&gt;2.1. 安装依赖&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_上传luajit_2_0_4_tar_gz并安装luajit&#34;&gt;2.2. 上传LuaJIT-2.0.4.tar.gz并安装LuaJIT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_设置环境变量&#34;&gt;2.3. 设置环境变量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_创建modules保存nginx的模块&#34;&gt;2.4. 创建modules保存nginx的模块&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_上传openresty_1_9_7_3_tar_gz和依赖的模块lua_nginx_module_0_10_0_tar_ngx_devel_kit_0_2_19_tar_ngx_devel_kit_0_2_19_tar_echo_nginx_module_0_58_tar_gz&#34;&gt;2.5. 上传openresty-1.9.7.3.tar.gz和依赖的模块lua-nginx-module-0.10.0.tar、ngx_devel_kit-0.2.19.tar、ngx_devel_kit-0.2.19.tar、echo-nginx-module-0.58.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_将依赖的模块直接解压到_usr_local_nginx_modules目录即可_不需要编译安装&#34;&gt;2.6. 将依赖的模块直接解压到/usr/local/nginx/modules目录即可，不需要编译安装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_编译安装openresty&#34;&gt;2.7. 编译安装openresty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_上传nginx&#34;&gt;2.8. 上传nginx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_编译nginx并支持其他模块&#34;&gt;2.9. 编译nginx并支持其他模块&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_修改nginx配置文件&#34;&gt;2.10. 修改nginx配置文件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_在nginx所在的服务器上添加一个ma_js&#34;&gt;2.11. 在nginx所在的服务器上添加一个ma.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_在要统计的页面添加js&#34;&gt;2.12. 在要统计的页面添加js&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_docker_方式&#34;&gt;1. Docker 方式&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;docker pull registry.cn-hangzhou.aliyuncs.com/dishui/nginx-log:1.0
docker run -d -p 80:80 registry.cn-hangzhou.aliyuncs.com/dishui/nginx-log:1.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;访问 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;a href=&#34;/tmp/a/&#34; class=&#34;bare&#34;&gt;/tmp/a/&lt;/a&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_安装&#34;&gt;2. 安装&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装依赖&#34;&gt;2.1. 安装依赖&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;yum -y install gcc perl pcre-devel openssl openssl-devel&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_上传luajit_2_0_4_tar_gz并安装luajit&#34;&gt;2.2. 上传LuaJIT-2.0.4.tar.gz并安装LuaJIT&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf LuaJIT-2.0.4.tar.gz -C /usr/local/src/
cd /usr/local/src/LuaJIT-2.0.4/
make &amp;amp;&amp;amp; make install PREFIX=/usr/local/luajit&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_设置环境变量&#34;&gt;2.3. 设置环境变量&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;export LUAJIT_LIB=/usr/local/luajit/lib
export LUAJIT_INC=/usr/local/luajit/include/luajit-2.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_创建modules保存nginx的模块&#34;&gt;2.4. 创建modules保存nginx的模块&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mkdir -p /usr/local/nginx/modules&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_上传openresty_1_9_7_3_tar_gz和依赖的模块lua_nginx_module_0_10_0_tar_ngx_devel_kit_0_2_19_tar_ngx_devel_kit_0_2_19_tar_echo_nginx_module_0_58_tar_gz&#34;&gt;2.5. 上传openresty-1.9.7.3.tar.gz和依赖的模块lua-nginx-module-0.10.0.tar、ngx_devel_kit-0.2.19.tar、ngx_devel_kit-0.2.19.tar、echo-nginx-module-0.58.tar.gz&lt;/h3&gt;

&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_将依赖的模块直接解压到_usr_local_nginx_modules目录即可_不需要编译安装&#34;&gt;2.6. 将依赖的模块直接解压到/usr/local/nginx/modules目录即可，不需要编译安装&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf lua-nginx-module-0.10.0.tar.gz -C /usr/local/nginx/modules/
tar -zxvf set-misc-nginx-module-0.29.tar.gz -C /usr/local/nginx/modules/
tar -zxvf ngx_devel_kit-0.2.19.tar.gz -C /usr/local/nginx/modules/
tar -zxvf echo-nginx-module-0.58.tar.gz -C /usr/local/nginx/modules/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　
=== 解压openresty-1.9.7.3.tar.gz&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf openresty-1.9.7.3.tar.gz -C /usr/local/src/
cd /usr/local/src/openresty-1.9.7.3/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_编译安装openresty&#34;&gt;2.7. 编译安装openresty&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;./configure --prefix=/usr/local/openresty --with-luajit &amp;amp;&amp;amp; make &amp;amp;&amp;amp; make install&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_上传nginx&#34;&gt;2.8. 上传nginx&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf nginx-1.8.1.tar.gz -C /usr/local/src/
cd /usr/local/src/nginx-1.8.1/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_编译nginx并支持其他模块&#34;&gt;2.9. 编译nginx并支持其他模块&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;　　./configure --prefix=/usr/local/nginx \
　　  --with-ld-opt=&#34;-Wl,-rpath,/usr/local/luajit/lib&#34; \
　　    --add-module=/usr/local/nginx/modules/ngx_devel_kit-0.2.19 \
　　    --add-module=/usr/local/nginx/modules/lua-nginx-module-0.10.0 \
　　    --add-module=/usr/local/nginx/modules/set-misc-nginx-module-0.29 \
　　    --add-module=/usr/local/nginx/modules/echo-nginx-module-0.58
　　make -j2
　　make install&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_修改nginx配置文件&#34;&gt;2.10. 修改nginx配置文件&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;worker_processes  2;

events {
    worker_connections  1024;
}

http {
    include       mime.types;
    default_type  application/octet-stream;

log_format tick &#34;$msec^A$remote_addr^A$u_domain^A$u_url^A$u_title^A$u_referrer^A$u_sh^A$u_sw^A$u_cd^A$u_lang^A$http_user_agent^A$u_utrace^A$u_account&#34;;

    access_log  logs/access.log  tick;

    sendfile        on;

    keepalive_timeout  65;

    server {
        listen       80;
        server_name  localhost;
        location /1.gif {
            #伪装成gif文件
            default_type image/gif;
            #本身关闭access_log，通过subrequest记录log
            access_log off;

            access_by_lua &#34;
                -- 用户跟踪cookie名为__utrace
                local uid = ngx.var.cookie___utrace
                if not uid then
                    -- 如果没有则生成一个跟踪cookie，算法为md5(时间戳+IP+客户端信息)
                    uid = ngx.md5(ngx.now() .. ngx.var.remote_addr .. ngx.var.http_user_agent)
                end
                ngx.header[&#39;Set-Cookie&#39;] = {&#39;__utrace=&#39; .. uid .. &#39;; path=/&#39;}
                if ngx.var.arg_domain then
                -- 通过subrequest到/i-log记录日志，将参数和用户跟踪cookie带过去
                    ngx.location.capture(&#39;/i-log?&#39; .. ngx.var.args .. &#39;&amp;amp;utrace=&#39; .. uid)
                end
            &#34;;

            #此请求不缓存
            add_header Expires &#34;Fri, 01 Jan 1980 00:00:00 GMT&#34;;
            add_header Pragma &#34;no-cache&#34;;
            add_header Cache-Control &#34;no-cache, max-age=0, must-revalidate&#34;;

            #返回一个1×1的空gif图片
            empty_gif;
        }

        location /i-log {
            #内部location，不允许外部直接访问
            internal;

            #设置变量，注意需要unescape
            set_unescape_uri $u_domain $arg_domain;
            set_unescape_uri $u_url $arg_url;
            set_unescape_uri $u_title $arg_title;
            set_unescape_uri $u_referrer $arg_referrer;
            set_unescape_uri $u_sh $arg_sh;
            set_unescape_uri $u_sw $arg_sw;
            set_unescape_uri $u_cd $arg_cd;
            set_unescape_uri $u_lang $arg_lang;
            set_unescape_uri $u_utrace $arg_utrace;
            set_unescape_uri $u_account $arg_account;

            #打开日志
            log_subrequest on;
            #记录日志到ma.log，实际应用中最好加buffer，格式为tick
            access_log /var/nginx_logs/ma.log tick;

            #输出空字符串
            echo &#39;&#39;;
        }
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_在nginx所在的服务器上添加一个ma_js&#34;&gt;2.11. 在nginx所在的服务器上添加一个ma.js&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;(function () {
    var params = {};
    //Document对象数据
    if(document) {
        params.domain = document.domain || &#39;&#39;;
        params.url = document.URL || &#39;&#39;;
        params.title = document.title || &#39;&#39;;
        params.referrer = document.referrer || &#39;&#39;;
    }
    //Window对象数据
    if(window &amp;amp;&amp;amp; window.screen) {
        params.sh = window.screen.height || 0;
        params.sw = window.screen.width || 0;
        params.cd = window.screen.colorDepth || 0;
    }
    //navigator对象数据
    if(navigator) {
        params.lang = navigator.language || &#39;&#39;;
    }
    //解析_maq配置
    // if(_maq) {
    //     for(var i in _maq) {
    //         switch(_maq[i][0]) {
    //             case &#39;_setAccount&#39;:
    //                 params.account = _maq[i][1];
    //                 break;
    //             default:
    //                 break;
    //         }
    //     }
    // }
    //拼接参数串
    var args = &#39;&#39;;
    for(var i in params) {
        if(args != &#39;&#39;) {
            args += &#39;&amp;amp;&#39;;
        }
        args += i + &#39;=&#39; + encodeURIComponent(params[i]);
    }

    //通过Image对象请求后端脚本
    var img = new Image(1, 1);
    img.src = &#39;http://flow.itcast.zx/log.gif?&#39; + args;
})();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_在要统计的页面添加js&#34;&gt;2.12. 在要统计的页面添加js&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;script type=&#34;text/javascript&#34;&amp;gt;
    var _maq = _maq || [];
    _maq.push([&#39;_setAccount&#39;, &#39;zx5352&#39;]);

    (function() {
        var ma = document.createElement(&#39;script&#39;);
        ma.type = &#39;text/javascript&#39;;
        ma.async = true;
        ma.src = &#39;http://flow.itcast.zx/ma.js&#39;;
        var s = document.getElementsByTagName(&#39;script&#39;)[0];
        s.parentNode.insertBefore(ma, s);
    })();
&amp;lt;/script&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SparkStreaming</title>
      <link>/post/bigdata/spark/spark-streaming/</link>
      <pubDate>Tue, 18 Apr 2017 13:02:08 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-streaming/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;stream&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_tmp&#34;&gt;1. tmp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sprakstreaming_demo&#34;&gt;2. SprakStreaming Demo&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_streamingwordcount&#34;&gt;2.1. StreamingWordCount&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_tmp&#34;&gt;1. tmp&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;//监听 6066
nc -lk 6066&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_sprakstreaming_demo&#34;&gt;2. SprakStreaming Demo&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_streamingwordcount&#34;&gt;2.1. StreamingWordCount&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;环境:&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&#34;tableblock frame-all grid-all spread&#34;&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 50%;&#34;&gt;
&lt;col style=&#34;width: 50%;&#34;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&#34;tableblock halign-left valign-top&#34;&gt;名称&lt;/th&gt;
&lt;th class=&#34;tableblock halign-left valign-top&#34;&gt;下载地址&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-left valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;hadoop2.6&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-left valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;&lt;a href=&#34;http://pan.baidu.com/s/1nvJkKeH&#34;&gt;密码：roxm&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-left valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;StreamingWordCount&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-left valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;&lt;a href=&#34;https://git.oschina.net/dishui/bigdata/tree/heartbeat02&#34;&gt;bigdata&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SparkSql</title>
      <link>/post/bigdata/spark/spark-sql/</link>
      <pubDate>Mon, 17 Apr 2017 16:33:02 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-sql/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;SparkSql&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_spark_sql&#34;&gt;1. Spark SQL&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_为什么要学习spark_sql&#34;&gt;1.1. 为什么要学习Spark SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_dataframes&#34;&gt;1.2. DataFrames&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_什么是dataframes&#34;&gt;1.2.1. 什么是DataFrames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_创建dataframes&#34;&gt;1.2.2. 创建DataFrames&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_dataframe常用操作&#34;&gt;1.3. DataFrame常用操作&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_dsl风格语法&#34;&gt;1.3.1. DSL风格语法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sql风格语法&#34;&gt;1.3.2. SQL风格语法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_以编程方式执行spark_sql查询&#34;&gt;2. 以编程方式执行Spark SQL查询&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_编写spark_sql查询程序&#34;&gt;2.1. 编写Spark SQL查询程序&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_通过反射推断schema&#34;&gt;2.1.1. 通过反射推断Schema&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_通过structtype直接指定schema&#34;&gt;2.1.2. 通过StructType直接指定Schema&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_数据源&#34;&gt;3. 数据源&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_jdbc&#34;&gt;3.1. JDBC&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_将数据写入到mysql中_打jar包方式&#34;&gt;3.1.1. 将数据写入到MySQL中（打jar包方式）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_spark_sql&#34;&gt;1. Spark SQL&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_为什么要学习spark_sql&#34;&gt;1.1. 为什么要学习Spark SQL&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;易整合&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164810.png&#34; alt=&#34;2017 04 17 164810&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;统一的数据访问方式&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164822.png&#34; alt=&#34;2017 04 17 164822&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;兼容Hive&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164830.png&#34; alt=&#34;2017 04 17 164830&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;标准的数据连接&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164839.png&#34; alt=&#34;2017 04 17 164839&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_dataframes&#34;&gt;1.2. DataFrames&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_什么是dataframes&#34;&gt;1.2.1. 什么是DataFrames&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上 看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164848.png&#34; alt=&#34;2017 04 17 164848&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_创建dataframes&#34;&gt;1.2.2. 创建DataFrames&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在SparkSQL中SQLContext是创建DataFrames和执行SQL的入口，在spark-1.5.2中已经内置了一个sqlContext&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164857.png&#34; alt=&#34;2017 04 17 164857&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;在本地创建一个文件，有三列，分别是id、name、age，用空格分隔，然后上传到hdfs上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cat &amp;gt; /person.txt &amp;lt;&amp;lt;_EOF_
1,a,23
2,hello,34
3,xxxx,12
_EOF_

hdfs dfs -put /person.txt /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在spark shell执行下面命令，读取数据，将每一行的数据使用列分隔符分割&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd $SPARK_HOME &amp;amp;&amp;amp; bin/spark-shell --master spark://master:7077

val lineRDD = sc.textFile(&#34;hdfs://master:8020/person.txt&#34;).map(_.split(&#34;,&#34;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;定义case class（相当于表的schema）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;case class Person(id:Int, name:String, age:Int)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将RDD和case class关联&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val personRDD = lineRDD.map(x =&amp;gt; Person(x(0).toInt, x(1), x(2).toInt))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将RDD转换成DataFrame&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val personDF = personRDD.toDF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对DataFrame进行处理&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;personDF.show&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164909.png&#34; alt=&#34;2017 04 17 164909&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_dataframe常用操作&#34;&gt;1.3. DataFrame常用操作&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_dsl风格语法&#34;&gt;1.3.1. DSL风格语法&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;//查看DataFrame中的内容
personDF.show

//查看DataFrame部分列中的内容
personDF.select(personDF.col(&#34;name&#34;)).show
personDF.select(col(&#34;name&#34;), col(&#34;age&#34;)).show
personDF.select(&#34;name&#34;).show

//打印DataFrame的Schema信息
personDF.printSchema

//查询所有的name和age，并将age+1
personDF.select(col(&#34;id&#34;), col(&#34;name&#34;), col(&#34;age&#34;) + 1).show
personDF.select(personDF(&#34;id&#34;), personDF(&#34;name&#34;), personDF(&#34;age&#34;) + 1).show

image::{img}/img/spark/2017-04-17_164916.png[]
---

//过滤age大于等于18的
personDF.filter(col(&#34;age&#34;) &amp;gt;= 18).show

image::{img}/img/spark/2017-04-17_164924.png[]
---

//按年龄进行分组并统计相同年龄的人数
personDF.groupBy(&#34;age&#34;).count().show()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164930.png&#34; alt=&#34;2017 04 17 164930&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_sql风格语法&#34;&gt;1.3.2. SQL风格语法&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果想使用SQL风格的语法，需要将DataFrame注册成表&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;personDF.registerTempTable(&#34;t_person&#34;)
//查询年龄最大的前两名
sqlContext.sql(&#34;select * from t_person order by age desc limit 2&#34;).show&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164936.png&#34; alt=&#34;2017 04 17 164936&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;//显示表的Schema信息
sqlContext.sql(&#34;desc t_person&#34;).show&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164943.png&#34; alt=&#34;2017 04 17 164943&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_以编程方式执行spark_sql查询&#34;&gt;2. 以编程方式执行Spark SQL查询&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_编写spark_sql查询程序&#34;&gt;2.1. 编写Spark SQL查询程序&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;前面我们学习了如何在Spark Shell中使用SQL完成查询，现在我们来实现在自定义的程序中编写Spark SQL查询程序。首先在maven项目的pom.xml中添加Spark SQL的依赖&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
&amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
&amp;lt;artifactId&amp;gt;spark-sql_2.10&amp;lt;/artifactId&amp;gt;
&amp;lt;version&amp;gt;1.5.2&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_通过反射推断schema&#34;&gt;2.1.1. 通过反射推断Schema&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SQLContext

object InferringSchema {
def main(args: Array[String]) {

//创建SparkConf()并设置App名称
val conf = new SparkConf().setAppName(&#34;SQL-1&#34;)
//SQLContext要依赖SparkContext
val sc = new SparkContext(conf)
//创建SQLContext
val sqlContext = new SQLContext(sc)

//从指定的地址创建RDD
val lineRDD = sc.textFile(args(0)).map(_.split(&#34;&#34;))

//创建case class
    //将RDD和case class关联
val personRDD = lineRDD.map(x =&amp;gt;Person(x(0).toInt, x(1), x(2).toInt))
//导入隐式转换，如果不导入无法将RDD转换成DataFrame
    //将RDD转换成DataFrame
import sqlContext.implicits._
val personDF = personRDD.toDF
//注册表
personDF.registerTempTable(&#34;t_person&#34;)
//传入SQL
val df = sqlContext.sql(&#34;select * from t_person order by age desc limit 2&#34;)
//将结果以JSON的方式存储到指定位置
df.write.json(args(1))
//停止Spark Context
sc.stop()
  }
}
//case class一定要放到外面
case class Person(id: Int, name: String, age: Int)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;将程序打成jar包，上传到spark集群，提交Spark任务&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \
--class cn.itcast.spark.sql.InferringSchema \
--master spark://node1.itcast.cn:7077 \
/root/spark-mvn-1.0-SNAPSHOT.jar \
hdfs://node1.itcast.cn:9000/person.txt \
hdfs://node1.itcast.cn:9000/out&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;查看运行结果&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hdfs dfs -cat  hdfs://node1.itcast.cn:9000/out/part-r-*&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_164952.png&#34; alt=&#34;2017 04 17 164952&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_通过structtype直接指定schema&#34;&gt;2.1.2. 通过StructType直接指定Schema&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.sql.types._
import org.apache.spark.{SparkContext, SparkConf}

object SpecifyingSchema {
def main(args: Array[String]) {
//创建SparkConf()并设置App名称
val conf = new SparkConf().setAppName(&#34;SQL-2&#34;)
//SQLContext要依赖SparkContext
val sc = new SparkContext(conf)
//创建SQLContext
val sqlContext = new SQLContext(sc)
//从指定的地址创建RDD
val personRDD = sc.textFile(args(0)).map(_.split(&#34;&#34;))
//通过StructType直接指定每个字段的schema
val schema = StructType(
List(
StructField(&#34;id&#34;, IntegerType, true),
StructField(&#34;name&#34;, StringType, true),
StructField(&#34;age&#34;, IntegerType, true)
      )
    )
//将RDD映射到rowRDD
val rowRDD = personRDD.map(p =&amp;gt;Row(p(0).toInt, p(1).trim, p(2).toInt))
//将schema信息应用到rowRDD上
val personDataFrame = sqlContext.createDataFrame(rowRDD, schema)
//注册表
personDataFrame.registerTempTable(&#34;t_person&#34;)
//执行SQL
val df = sqlContext.sql(&#34;select * from t_person order by age desc limit 4&#34;)
//将结果以JSON的方式存储到指定位置
df.write.json(args(1))
//停止Spark Context
sc.stop()
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;将程序打成jar包，上传到spark集群，提交Spark任务&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \
--class cn.itcast.spark.sql.InferringSchema \
--master spark://node1.itcast.cn:7077 \
/root/spark-mvn-1.0-SNAPSHOT.jar \
hdfs://node1.itcast.cn:9000/person.txt \
hdfs://node1.itcast.cn:9000/out1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;查看结果&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hdfs dfs -cat  hdfs://node1.itcast.cn:9000/out1/part-r-*&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_165001.png&#34; alt=&#34;2017 04 17 165001&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_数据源&#34;&gt;3. 数据源&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_jdbc&#34;&gt;3.1. JDBC&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。
==== 从MySQL中加载数据（Spark Shell方式）
. 启动Spark Shell，必须指定mysql连接驱动jar包&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-shell \
--master spark://node1.itcast.cn:7077 \
--jars /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar \
--driver-class-path /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;从mysql中加载数据&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val jdbcDF = sqlContext.read.format(&#34;jdbc&#34;).options(Map(&#34;url&#34; -&amp;gt;&#34;jdbc:mysql://192.168.10.1:3306/bigdata&#34;, &#34;driver&#34; -&amp;gt;&#34;com.mysql.jdbc.Driver&#34;, &#34;dbtable&#34; -&amp;gt;&#34;person&#34;, &#34;user&#34; -&amp;gt;&#34;root&#34;, &#34;password&#34; -&amp;gt;&#34;123456&#34;)).load()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;执行查询&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;jdbcDF.show()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-04-17_165008.png&#34; alt=&#34;2017 04 17 165008&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_将数据写入到mysql中_打jar包方式&#34;&gt;3.1.1. 将数据写入到MySQL中（打jar包方式）&lt;/h4&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;编写Spark SQL程序&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import java.util.Properties
import org.apache.spark.sql.{SQLContext, Row}
import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}
import org.apache.spark.{SparkConf, SparkContext}

object JdbcRDD {
def main(args: Array[String]) {
val conf = new SparkConf().setAppName(&#34;MySQL-Demo&#34;)
val sc = new SparkContext(conf)
val sqlContext = new SQLContext(sc)
//通过并行化创建RDD
val personRDD = sc.parallelize(Array(&#34;1 tom 5&#34;, &#34;2 jerry 3&#34;, &#34;3 kitty 6&#34;)).map(_.split(&#34;&#34;))
//通过StructType直接指定每个字段的schema
val schema = StructType(
List(
StructField(&#34;id&#34;, IntegerType, true),
StructField(&#34;name&#34;, StringType, true),
StructField(&#34;age&#34;, IntegerType, true)
      )
    )
//将RDD映射到rowRDD
val rowRDD = personRDD.map(p =&amp;gt;Row(p(0).toInt, p(1).trim, p(2).toInt))
//将schema信息应用到rowRDD上
val personDataFrame = sqlContext.createDataFrame(rowRDD, schema)
//创建Properties存储数据库相关属性
val prop = new Properties()
    prop.put(&#34;user&#34;, &#34;root&#34;)
    prop.put(&#34;password&#34;, &#34;123456&#34;)
//将数据追加到数据库
personDataFrame.write.mode(&#34;append&#34;).jdbc(&#34;jdbc:mysql://192.168.10.1:3306/bigdata&#34;, &#34;bigdata.person&#34;, prop)
//停止SparkContext
sc.stop()
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用maven将程序打包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将Jar包提交到spark集群&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \
--class cn.itcast.spark.sql.JdbcRDD \
--master spark://node1.itcast.cn:7077 \
--jars /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar \
--driver-class-path /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar \
/root/spark-mvn-1.0-SNAPSHOT.jar&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Zhen He spark rdd api</title>
      <link>/post/bigdata/spark/spark-rdd-api/</link>
      <pubDate>Thu, 13 Apr 2017 14:04:07 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-rdd-api/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;spark&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_part1&#34;&gt;1. part1&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_aggregate&#34;&gt;1.1. aggregate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_aggregatebykey_pair&#34;&gt;1.2. aggregateByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_cartesian&#34;&gt;1.3. cartesian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_checkpoint&#34;&gt;1.4. checkpoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_coalesce_repartition&#34;&gt;1.5. coalesce, repartition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_cogroup_pair_groupwith_pair&#34;&gt;1.6. cogroup [Pair], groupWith [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_collect_toarray&#34;&gt;1.7. collect, toArray&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_collectasmap_pair&#34;&gt;1.8. collectAsMap [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_combinebykey_pair&#34;&gt;1.9. combineByKey[Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_compute&#34;&gt;1.10. compute&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_context_sparkcontext&#34;&gt;1.11. context, sparkContext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_count&#34;&gt;1.12. count&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countapprox&#34;&gt;1.13. countApprox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countapproxdistinct&#34;&gt;1.14. countApproxDistinct&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countapproxdistinctbykey_pair&#34;&gt;1.15. countApproxDistinctByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_part2&#34;&gt;2. part2&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_countbykey_pair&#34;&gt;2.1. countByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countbykeyapprox_pair&#34;&gt;2.2. countByKeyApprox [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countbyvalue&#34;&gt;2.3. countByValue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countbyvalueapprox&#34;&gt;2.4. countByValueApprox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_dependencies&#34;&gt;2.5. dependencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_distinct&#34;&gt;2.6. distinct&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_first&#34;&gt;2.7. first&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_filter&#34;&gt;2.8. filter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_filterbyrange_ordered&#34;&gt;2.9. filterByRange [Ordered]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_filterwith_deprecated&#34;&gt;2.10. filterWith  (deprecated)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_flatmap&#34;&gt;2.11. flatMap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_flatmapvalues&#34;&gt;2.12. flatMapValues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_flatmapwith_deprecated&#34;&gt;2.13. flatMapWith (deprecated)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_fold&#34;&gt;2.14. fold&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foldbykey_pair&#34;&gt;2.15. foldByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foreach&#34;&gt;2.16. foreach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foreachpartition&#34;&gt;2.17. foreachPartition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foreachwith_deprecated&#34;&gt;2.18. foreachWith (Deprecated)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_fullouterjoin_pair&#34;&gt;2.19. fullOuterJoin [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_part3&#34;&gt;3. part3&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_generator_setgenerator&#34;&gt;3.1. generator, setGenerator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_getcheckpointfile&#34;&gt;3.2. getCheckpointFile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_preferredlocations&#34;&gt;3.3. preferredLocations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_getstoragelevel&#34;&gt;3.4. getStorageLevel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_glom&#34;&gt;3.5. glom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_groupby&#34;&gt;3.6. groupBy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_groupbykey_pair&#34;&gt;3.7. groupByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_histogram_double&#34;&gt;3.8. histogram [Double]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_id&#34;&gt;3.9. id&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_intersection&#34;&gt;3.10. intersection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_ischeckpointed&#34;&gt;3.11. isCheckpointed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_iterator&#34;&gt;3.12. iterator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_join_pair&#34;&gt;3.13. join [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_keyby&#34;&gt;3.14. keyBy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_keys_pair&#34;&gt;3.15. keys [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_leftouterjoin_pair&#34;&gt;3.16. leftOuterJoin [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_lookup&#34;&gt;3.17. lookup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_map&#34;&gt;3.18. map&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mappartitions&#34;&gt;3.19. mapPartitions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mappartitionswithcontext_deprecated_and_developer_api&#34;&gt;3.20. mapPartitionsWithContext   (deprecated and developer API)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mappartitionswithindex&#34;&gt;3.21. mapPartitionsWithIndex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mappartitionswithsplit&#34;&gt;3.22. mapPartitionsWithSplit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mapvalues_pair&#34;&gt;3.23. mapValues [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mapwith_deprecated&#34;&gt;3.24. mapWith  (deprecated)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_part4&#34;&gt;4. part4&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_max&#34;&gt;4.1. max&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mean_double_meanapprox_double&#34;&gt;4.2. mean [Double], meanApprox [Double]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_min&#34;&gt;4.3. min&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_name_setname&#34;&gt;4.4. name, setName&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_partitionby_pair&#34;&gt;4.5. partitionBy [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_partitioner&#34;&gt;4.6. partitioner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_partitions&#34;&gt;4.7. partitions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_persist_cache&#34;&gt;4.8. persist, cache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_pipe&#34;&gt;4.9. pipe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_randomsplit&#34;&gt;4.10. randomSplit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_reduce&#34;&gt;4.11. reduce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_reducebykey_pair_reducebykeylocally_pair_reducebykeytodriver_pair&#34;&gt;4.12. reduceByKey [Pair],  reduceByKeyLocally [Pair], reduceByKeyToDriver [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_repartition&#34;&gt;4.13. repartition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_repartitionandsortwithinpartitions_ordered&#34;&gt;4.14. repartitionAndSortWithinPartitions [Ordered]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_rightouterjoin_pair&#34;&gt;4.15. rightOuterJoin [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sample&#34;&gt;4.16. sample&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_samplebykey_pair&#34;&gt;4.17. sampleByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_samplebykeyexact_pair_experimental&#34;&gt;4.18. sampleByKeyExact [Pair, experimental]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_saveashadoopfile_pair_saveashadoopdataset_pair_saveasnewapihadoopfile_pair&#34;&gt;4.19. saveAsHadoopFile [Pair], saveAsHadoopDataset [Pair], saveAsNewAPIHadoopFile [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_saveasobjectfile&#34;&gt;4.20. saveAsObjectFile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_saveassequencefile_seqfile&#34;&gt;4.21. saveAsSequenceFile [SeqFile]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_saveastextfile&#34;&gt;4.22. saveAsTextFile&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_part5&#34;&gt;5. part5&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_stats_double&#34;&gt;5.1. stats [Double]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sortby&#34;&gt;5.2. sortBy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sortbykey_ordered&#34;&gt;5.3. sortByKey [Ordered]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_subtract&#34;&gt;5.4. subtract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_subtractbykey_pair&#34;&gt;5.5. subtractByKey [Pair]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sum_double_sumapprox_double&#34;&gt;5.6. sum [Double], sumApprox [Double]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_take&#34;&gt;5.7. take&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_takeordered&#34;&gt;5.8. takeOrdered&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_takesample&#34;&gt;5.9. takeSample&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_todebugstring&#34;&gt;5.10. toDebugString&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_tolocaliterator&#34;&gt;5.11. toLocalIterator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_top&#34;&gt;5.12. top&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_tostring&#34;&gt;5.13. toString&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_treeaggregate&#34;&gt;5.14. treeAggregate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_treereduce&#34;&gt;5.15. treeReduce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_union&#34;&gt;5.16. union, ++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_unpersist&#34;&gt;5.17. unpersist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_values&#34;&gt;5.18. values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_variance_double_samplevariance_double&#34;&gt;5.19. variance [Double], sampleVariance [Double]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_zip&#34;&gt;5.20. zip&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_zipwithindex&#34;&gt;5.21. zipWithIndex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_zipwithuniqueid&#34;&gt;5.22. zipWithUniqueId&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_part1&#34;&gt;1. part1&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_aggregate&#34;&gt;1.1. aggregate&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;The aggregate function allows the user to apply two different reduce functions to the RDD. The first reduce function is applied within each partition to reduce the data within each partition into a single result. The second reduce function is used to combine the different reduced results of all partitions together to arrive at one final result. The ability to have two separate reduce functions for intra partition versus across partition reducing adds a lot of flexibility. For example the first reduce function can be the max function and the second one can be the sum function. The user also specifies an initial value. Here are some important facts.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;The initial value is applied at both levels of reduce. So both at the intra partition reduction and across partition reduction.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Both reduce functions have to be commutative and associative.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do not assume any execution order for either partition computations or combining partitions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why would one want to use two input data types? Let us assume we do an archaeological site survey using a metal detector. While walking through the site we take GPS coordinates of important findings based on the output of the metal detector. Later, we intend to draw an image of a map that highlights these locations using the aggregate function. In this case the zeroValue could be an area map with no highlights. The possibly huge set of input data is stored as GPS coordinates across many partitions. seqOp (first reducer) could convert the GPS coordinates to map coordinates and put a marker on the map at the respective position. combOp (second reducer) will receive these highlights as partial maps and combine them into a single final output map.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;&lt;strong&gt;Listing Variants&lt;/strong&gt; &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&amp;gt; U, combOp: (U, U) =&amp;gt; U): U&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Examples 1 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(1,2,3,4,5,6), 2)

// lets first print out the contents of the RDD with partition labels
def myfunc(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}

z.mapPartitionsWithIndex(myfunc).collect
res28: Array[String] = Array([partID:0, val: 1], [partID:0, val: 2], [partID:0, val: 3], [partID:1, val: 4], [partID:1, val: 5], [partID:1, val: 6])

z.aggregate(0)(math.max(_, _), _ + _)
res40: Int = 9

// This example returns 16 since the initial value is 5
// reduce of partition 0 will be max(5, 1, 2, 3) = 5
// reduce of partition 1 will be max(5, 4, 5, 6) = 6
// final reduce across partitions will be 5 + 5 + 6 = 16
// note the final reduce include the initial value
z.aggregate(5)(math.max(_, _), _ + _)
res29: Int = 16


val z = sc.parallelize(List(&#34;a&#34;,&#34;b&#34;,&#34;c&#34;,&#34;d&#34;,&#34;e&#34;,&#34;f&#34;),2)

//lets first print out the contents of the RDD with partition labels
def myfunc(index: Int, iter: Iterator[(String)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}

z.mapPartitionsWithIndex(myfunc).collect
res31: Array[String] = Array([partID:0, val: a], [partID:0, val: b], [partID:0, val: c], [partID:1, val: d], [partID:1, val: e], [partID:1, val: f])

z.aggregate(&#34;&#34;)(_ + _, _+_)
res115: String = abcdef

// See here how the initial value &#34;x&#34; is applied three times.
//  - once for each partition
//  - once when combining all the partitions in the second reduce function.
z.aggregate(&#34;x&#34;)(_ + _, _+_)
res116: String = xxdefxabc

// Below are some more advanced examples. Some are quite tricky to work out.

val z = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;345&#34;,&#34;4567&#34;),2)
z.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.max(x.length, y.length).toString, (x,y) =&amp;gt; x + y)
res141: String = 42

z.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.min(x.length, y.length).toString, (x,y) =&amp;gt; x + y)
res142: String = 11

val z = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;345&#34;,&#34;&#34;),2)
z.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.min(x.length, y.length).toString, (x,y) =&amp;gt; x + y)
res143: String = 10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;The main issue with the code above is that the result of the inner min is a string of length 1.
The zero in the output is due to the empty string being the last string in the list. We see this result because we are not recursively reducing any further within the partition for the final string.&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Examples 2 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;&#34;,&#34;345&#34;),2)
z.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.min(x.length, y.length).toString, (x,y) =&amp;gt; x + y)
res144: String = 11&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;In contrast to the previous example, this example has the empty string at the beginning of the second partition. This results in length of zero being input to the second reduce which then upgrades it a length of 1. (Warning: The above example shows bad design since the output is dependent on the order of the data inside the partitions.)&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_aggregatebykey_pair&#34;&gt;1.2. aggregateByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Works like the aggregate function except the aggregation is applied to the values with the same key. Also unlike the aggregate function the initial value is not applied to the second reduce.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def aggregateByKey[U](zeroValue: U)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
def aggregateByKey[U](zeroValue: U, numPartitions: Int)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
def aggregateByKey[U](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val pairRDD = sc.parallelize(List( (&#34;cat&#34;,2), (&#34;cat&#34;, 5), (&#34;mouse&#34;, 4),(&#34;cat&#34;, 12), (&#34;dog&#34;, 12), (&#34;mouse&#34;, 2)), 2)

// lets have a look at what is in the partitions
def myfunc(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
pairRDD.mapPartitionsWithIndex(myfunc).collect

res2: Array[String] = Array([partID:0, val: (cat,2)], [partID:0, val: (cat,5)], [partID:0, val: (mouse,4)], [partID:1, val: (cat,12)], [partID:1, val: (dog,12)], [partID:1, val: (mouse,2)])

pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect
res3: Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6))

pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).collect
res4: Array[(String, Int)] = Array((dog,100), (cat,200), (mouse,200))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_cartesian&#34;&gt;1.3. cartesian&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Computes the cartesian product between two RDDs (i.e. Each item of the first RDD is joined with each item of the second RDD) and returns them as a new RDD. (Warning: Be careful when using this function.! Memory consumption can quickly become an issue!)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(List(1,2,3,4,5))
val y = sc.parallelize(List(6,7,8,9,10))
x.cartesian(y).collect
res0: Array[(Int, Int)] = Array((1,6), (1,7), (1,8), (1,9), (1,10), (2,6), (2,7), (2,8), (2,9), (2,10), (3,6), (3,7), (3,8), (3,9), (3,10), (4,6), (5,6), (4,7), (5,7), (4,8), (5,8), (4,9), (4,10), (5,9), (5,10))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_checkpoint&#34;&gt;1.4. checkpoint&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Will create a checkpoint when the RDD is computed next. Checkpointed RDDs are stored as a binary file within the checkpoint directory which can be specified using the Spark context. (Warning: Spark applies lazy evaluation. Checkpointing will not occur until an action is invoked.)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Important note: the directory  &#34;my_directory_name&#34; should exist in all slaves. As an alternative you could use an HDFS directory URL as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def checkpoint()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sc.setCheckpointDir(&#34;my_directory_name&#34;)
val a = sc.parallelize(1 to 4)
a.checkpoint
a.count
14/02/25 18:13:53 INFO SparkContext: Starting job: count at &amp;lt;console&amp;gt;:15
...
14/02/25 18:13:53 INFO MemoryStore: Block broadcast_5 stored as values to memory (estimated size 115.7 KB, free 296.3 MB)
14/02/25 18:13:53 INFO RDDCheckpointData: Done checkpointing RDD 11 to file:/home/cloudera/Documents/spark-0.9.0-incubating-bin-cdh4/bin/my_directory_name/65407913-fdc6-4ec1-82c9-48a1656b95d6/rdd-11, new parent is RDD 12
res23: Long = 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_coalesce_repartition&#34;&gt;1.5. coalesce, repartition&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Coalesces the associated data into a given number of partitions. repartition(numPartitions) is simply an abbreviation for coalesce(numPartitions, shuffle = true).&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def coalesce ( numPartitions : Int , shuffle : Boolean = false ): RDD [T]
def repartition ( numPartitions : Int ): RDD [T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(1 to 10, 10)
val z = y.coalesce(2, false)
z.partitions.length
res9: Int = 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_cogroup_pair_groupwith_pair&#34;&gt;1.6. cogroup [Pair], groupWith [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;A very powerful set of functions that allow grouping up to 3 key-value RDDs together using their keys.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]
def cogroup[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Iterable[V], Iterable[W]))]
def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Iterable[V], Iterable[W]))]
def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)]): RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]
def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)], numPartitions: Int): RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]
def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)], partitioner: Partitioner): RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]
def groupWith[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]
def groupWith[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)]): RDD[(K, (Iterable[V], IterableW1], Iterable[W2]))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Examples &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1, 2, 1, 3), 1)
val b = a.map((_, &#34;b&#34;))
val c = a.map((_, &#34;c&#34;))
b.cogroup(c).collect
res7: Array[(Int, (Iterable[String], Iterable[String]))] = Array(
(2,(ArrayBuffer(b),ArrayBuffer(c))),
(3,(ArrayBuffer(b),ArrayBuffer(c))),
(1,(ArrayBuffer(b, b),ArrayBuffer(c, c)))
)

val d = a.map((_, &#34;d&#34;))
b.cogroup(c, d).collect
res9: Array[(Int, (Iterable[String], Iterable[String], Iterable[String]))] = Array(
(2,(ArrayBuffer(b),ArrayBuffer(c),ArrayBuffer(d))),
(3,(ArrayBuffer(b),ArrayBuffer(c),ArrayBuffer(d))),
(1,(ArrayBuffer(b, b),ArrayBuffer(c, c),ArrayBuffer(d, d)))
)

val x = sc.parallelize(List((1, &#34;apple&#34;), (2, &#34;banana&#34;), (3, &#34;orange&#34;), (4, &#34;kiwi&#34;)), 2)
val y = sc.parallelize(List((5, &#34;computer&#34;), (1, &#34;laptop&#34;), (1, &#34;desktop&#34;), (4, &#34;iPad&#34;)), 2)
x.cogroup(y).collect
res23: Array[(Int, (Iterable[String], Iterable[String]))] = Array(
(4,(ArrayBuffer(kiwi),ArrayBuffer(iPad))),
(2,(ArrayBuffer(banana),ArrayBuffer())),
(3,(ArrayBuffer(orange),ArrayBuffer())),
(1,(ArrayBuffer(apple),ArrayBuffer(laptop, desktop))),
(5,(ArrayBuffer(),ArrayBuffer(computer))))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_collect_toarray&#34;&gt;1.7. collect, toArray&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Converts the RDD into a Scala array and returns it. If you provide a standard map-function (i.e. f = T &amp;#8594; U) it will be applied before inserting the values into the result array.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def collect(): Array[T]
def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U]
def toArray(): Array[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;, &#34;Gnu&#34;, &#34;Rat&#34;), 2)
c.collect
res29: Array[String] = Array(Gnu, Cat, Rat, Dog, Gnu, Rat)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_collectasmap_pair&#34;&gt;1.8. collectAsMap [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to collect, but works on key-value RDDs and converts them into Scala maps to preserve their key-value structure.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def collectAsMap(): Map[K, V]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1, 2, 1, 3), 1)
val b = a.zip(a)
b.collectAsMap
res1: scala.collection.Map[Int,Int] = Map(2 -&amp;gt; 2, 1 -&amp;gt; 1, 3 -&amp;gt; 3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_combinebykey_pair&#34;&gt;1.9. combineByKey[Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very efficient implementation that combines the values of a RDD consisting of two-component tuples by applying multiple aggregators one after another.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def combineByKey[C](createCombiner: V =&amp;gt; C, mergeValue: (C, V) =&amp;gt; C, mergeCombiners: (C, C) =&amp;gt; C): RDD[(K, C)]
def combineByKey[C](createCombiner: V =&amp;gt; C, mergeValue: (C, V) =&amp;gt; C, mergeCombiners: (C, C) =&amp;gt; C, numPartitions: Int): RDD[(K, C)]
def combineByKey[C](createCombiner: V =&amp;gt; C, mergeValue: (C, V) =&amp;gt; C, mergeCombiners: (C, C) =&amp;gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializerClass: String = null): RDD[(K, C)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;,&#34;cat&#34;,&#34;gnu&#34;,&#34;salmon&#34;,&#34;rabbit&#34;,&#34;turkey&#34;,&#34;wolf&#34;,&#34;bear&#34;,&#34;bee&#34;), 3)
val b = sc.parallelize(List(1,1,2,2,2,1,2,2,2), 3)
val c = b.zip(a)
val d = c.combineByKey(List(_), (x:List[String], y:String) =&amp;gt; y :: x, (x:List[String], y:List[String]) =&amp;gt; x ::: y)
d.collect
res16: Array[(Int, List[String])] = Array((1,List(cat, dog, turkey)), (2,List(gnu, rabbit, salmon, bee, bear, wolf)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_compute&#34;&gt;1.10. compute&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Executes dependencies and computes the actual representation of the RDD. This function should not be called directly by users.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def compute(split: Partition, context: TaskContext): Iterator[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_context_sparkcontext&#34;&gt;1.11. context, sparkContext&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the SparkContext that was used to create the RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def compute(split: Partition, context: TaskContext): Iterator[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;), 2)
c.context
res8: org.apache.spark.SparkContext = org.apache.spark.SparkContext@58c1c2f1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_count&#34;&gt;1.12. count&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the number of items stored within a RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def count(): Long&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;), 2)
c.count
res2: Long = 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countapprox&#34;&gt;1.13. countApprox&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Marked as experimental feature! Experimental features are currently not covered by this document!&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def (timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countapproxdistinct&#34;&gt;1.14. countApproxDistinct&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Computes the approximate number of distinct values. For large RDDs which are spread across many nodes, this function may execute faster than other counting methods. The parameter relativeSD controls the accuracy of the computation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countApproxDistinct(relativeSD: Double = 0.05): Long&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 10000, 20)
val b = a++a++a++a++a
b.countApproxDistinct(0.1)
res14: Long = 8224

b.countApproxDistinct(0.05)
res15: Long = 9750

b.countApproxDistinct(0.01)
res16: Long = 9947

b.countApproxDistinct(0.001)
res0: Long = 10000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countapproxdistinctbykey_pair&#34;&gt;1.15. countApproxDistinctByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to countApproxDistinct, but computes the approximate number of distinct values for each distinct key. Hence, the RDD must consist of two-component tuples. For large RDDs which are spread across many nodes, this function may execute faster than other counting methods. The parameter relativeSD controls the accuracy of the computation.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countApproxDistinctByKey(relativeSD: Double = 0.05): RDD[(K, Long)]
def countApproxDistinctByKey(relativeSD: Double, numPartitions: Int): RDD[(K, Long)]
def countApproxDistinctByKey(relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;), 2)
val b = sc.parallelize(a.takeSample(true, 10000, 0), 20)
val c = sc.parallelize(1 to b.count().toInt, 20)
val d = b.zip(c)
d.countApproxDistinctByKey(0.1).collect
res15: Array[(String, Long)] = Array((Rat,2567), (Cat,3357), (Dog,2414), (Gnu,2494))

d.countApproxDistinctByKey(0.01).collect
res16: Array[(String, Long)] = Array((Rat,2555), (Cat,2455), (Dog,2425), (Gnu,2513))

d.countApproxDistinctByKey(0.001).collect
res0: Array[(String, Long)] = Array((Rat,2562), (Cat,2464), (Dog,2451), (Gnu,2521))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_part2&#34;&gt;2. part2&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countbykey_pair&#34;&gt;2.1. countByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very similar to count, but counts the values of a RDD consisting of two-component tuples for each distinct key separately.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countByKey(): Map[K, Long]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List((3, &#34;Gnu&#34;), (3, &#34;Yak&#34;), (5, &#34;Mouse&#34;), (3, &#34;Dog&#34;)), 2)
c.countByKey
res3: scala.collection.Map[Int,Long] = Map(3 -&amp;gt; 3, 5 -&amp;gt; 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countbykeyapprox_pair&#34;&gt;2.2. countByKeyApprox [Pair]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Marked as experimental feature! Experimental features are currently not covered by this document!&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countByKeyApprox(timeout: Long, confidence: Double = 0.95): PartialResult[Map[K, BoundedDouble]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countbyvalue&#34;&gt;2.3. countByValue&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns a map that contains all unique values of the RDD and their respective occurrence counts. (Warning: This operation will finally aggregate the information in a single reducer.)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countByValue(): Map[T, Long]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1))
b.countByValue
res27: scala.collection.Map[Int,Long] = Map(5 -&amp;gt; 1, 8 -&amp;gt; 1, 3 -&amp;gt; 1, 6 -&amp;gt; 1, 1 -&amp;gt; 6, 2 -&amp;gt; 3, 4 -&amp;gt; 2, 7 -&amp;gt; 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_countbyvalueapprox&#34;&gt;2.4. countByValueApprox&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Marked as experimental feature! Experimental features are currently not covered by this document!&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def countByValueApprox(timeout: Long, confidence: Double = 0.95): PartialResult[Map[T, BoundedDouble]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_dependencies&#34;&gt;2.5. dependencies&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the RDD on which this RDD depends.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;final def dependencies: Seq[Dependency[_]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1))
b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[32] at parallelize at &amp;lt;console&amp;gt;:12
b.dependencies.length
Int = 0

b.map(a =&amp;gt; a).dependencies.length
res40: Int = 1

b.cartesian(a).dependencies.length
res41: Int = 2

b.cartesian(a).dependencies
res42: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.rdd.CartesianRDD$$anon$1@576ddaaa, org.apache.spark.rdd.CartesianRDD$$anon$2@6d2efbbd)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_distinct&#34;&gt;2.6. distinct&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns a new RDD that contains each unique value only once.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def distinct(): RDD[T]
def distinct(numPartitions: Int): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;, &#34;Gnu&#34;, &#34;Rat&#34;), 2)
c.distinct.collect
res6: Array[String] = Array(Dog, Gnu, Cat, Rat)

val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10))
a.distinct(2).partitions.length
res16: Int = 2

a.distinct(3).partitions.length
res17: Int = 3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_first&#34;&gt;2.7. first&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Looks for the very first data item of the RDD and returns it.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def first(): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;), 2)
c.first
res1: String = Gnu&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_filter&#34;&gt;2.8. filter&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Evaluates a boolean function for each data item of the RDD and puts the items for which the function returned true into the resulting RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def filter(f: T =&amp;gt; Boolean): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 10, 3)
val b = a.filter(_ % 2 == 0)
b.collect
res3: Array[Int] = Array(2, 4, 6, 8, 10)
When you provide a filter function, it must be able to handle all data items contained in the RDD. Scala provides so-called partial functions to deal with mixed data-types. (Tip: Partial functions are very useful if you have some data which may be bad and you do not want to handle but for the good data (matching data) you want to apply some kind of map function. The following article is good. It teaches you about partial functions in a very nice way and explains why case has to be used for partial functions:  article)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Examples for mixed data without partial functions  &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(1 to 8)
b.filter(_ &amp;lt; 4).collect
res15: Array[Int] = Array(1, 2, 3)

val a = sc.parallelize(List(&#34;cat&#34;, &#34;horse&#34;, 4.0, 3.5, 2, &#34;dog&#34;))
a.filter(_ &amp;lt; 4).collect
&amp;lt;console&amp;gt;:15: error: value &amp;lt; is not a member of Any&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This fails because some components of a are not implicitly comparable against integers. Collect uses the isDefinedAt property of a function-object to determine whether the test-function is compatible with each data item. Only data items that pass this test (=filter) are then mapped using the function-object.&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Examples for mixed data with partial functions &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;cat&#34;, &#34;horse&#34;, 4.0, 3.5, 2, &#34;dog&#34;))
a.collect({case a: Int    =&amp;gt; &#34;is integer&#34; |
           case b: String =&amp;gt; &#34;is string&#34; }).collect
res17: Array[String] = Array(is string, is string, is integer, is string)

val myfunc: PartialFunction[Any, Any] = {
  case a: Int    =&amp;gt; &#34;is integer&#34; |
  case b: String =&amp;gt; &#34;is string&#34; }
myfunc.isDefinedAt(&#34;&#34;)
res21: Boolean = true

myfunc.isDefinedAt(1)
res22: Boolean = true

myfunc.isDefinedAt(1.5)
res23: Boolean = false&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Be careful! The above code works because it only checks the type itself! If you use operations on this type, you have to explicitly declare what type you want instead of any. Otherwise the compiler does (apparently) not know what bytecode it should produce:&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val myfunc2: PartialFunction[Any, Any] = {case x if (x &amp;lt; 4) =&amp;gt; &#34;x&#34;}
&amp;lt;console&amp;gt;:10: error: value &amp;lt; is not a member of Any

val myfunc2: PartialFunction[Int, Any] = {case x if (x &amp;lt; 4) =&amp;gt; &#34;x&#34;}
myfunc2: PartialFunction[Int,Any] = &amp;lt;function1&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_filterbyrange_ordered&#34;&gt;2.9. filterByRange [Ordered]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns an RDD containing only the items in the key range specified. From our testing, it appears this only works if your data is in key value pairs and it has already been sorted by key.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def filterByRange(lower: K, upper: K): RDD[P]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val randRDD = sc.parallelize(List( (2,&#34;cat&#34;), (6, &#34;mouse&#34;),(7, &#34;cup&#34;), (3, &#34;book&#34;), (4, &#34;tv&#34;), (1, &#34;screen&#34;), (5, &#34;heater&#34;)), 3)
val sortedRDD = randRDD.sortByKey()

sortedRDD.filterByRange(1, 3).collect
res66: Array[(Int, String)] = Array((1,screen), (2,cat), (3,book))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_filterwith_deprecated&#34;&gt;2.10. filterWith  (deprecated)&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is an extended version of filter. It takes two function arguments. The first argument must conform to Int &amp;#8594; T and is executed once per partition. It will transform the partition index to type T. The second function looks like (U, T) &amp;#8594; Boolean. T is the transformed partition index and U are the data items from the RDD. Finally the function has to return either true or false (i.e. Apply the filter).&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def filterWith[A: ClassTag](constructA: Int =&amp;gt; A)(p: (T, A) =&amp;gt; Boolean): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
val b = a.filterWith(i =&amp;gt; i)((x,i) =&amp;gt; x % 2 == 0 || i % 2 == 0)
b.collect
res37: Array[Int] = Array(1, 2, 3, 4, 6, 7, 8, 9)

val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 5)
a.filterWith(x=&amp;gt; x)((a, b) =&amp;gt;  b == 0).collect
res30: Array[Int] = Array(1, 2)

a.filterWith(x=&amp;gt; x)((a, b) =&amp;gt;  a % (b+1) == 0).collect
res33: Array[Int] = Array(1, 2, 4, 6, 8, 10)

a.filterWith(x=&amp;gt; x.toString)((a, b) =&amp;gt;  b == &#34;2&#34;).collect
res34: Array[Int] = Array(5, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_flatmap&#34;&gt;2.11. flatMap&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to map, but allows emitting more than one item in the map function.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def flatMap[U: ClassTag](f: T =&amp;gt; TraversableOnce[U]): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 10, 5)
a.flatMap(1 to _).collect
res47: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

sc.parallelize(List(1, 2, 3), 2).flatMap(x =&amp;gt; List(x, x, x)).collect
res85: Array[Int] = Array(1, 1, 1, 2, 2, 2, 3, 3, 3)

// The program below generates a random number of copies (up to 10) of the items in the list.
val x  = sc.parallelize(1 to 10, 3)
x.flatMap(List.fill(scala.util.Random.nextInt(10))(_)).collect

res1: Array[Int] = Array(1, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_flatmapvalues&#34;&gt;2.12. flatMapValues&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very similar to mapValues, but collapses the inherent structure of the values during mapping.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def flatMapValues[U](f: V =&amp;gt; TraversableOnce[U]): RDD[(K, U)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.flatMapValues(&#34;x&#34; + _ + &#34;x&#34;).collect
res6: Array[(Int, Char)] = Array((3,x), (3,d), (3,o), (3,g), (3,x), (5,x), (5,t), (5,i), (5,g), (5,e), (5,r), (5,x), (4,x), (4,l), (4,i), (4,o), (4,n), (4,x), (3,x), (3,c), (3,a), (3,t), (3,x), (7,x), (7,p), (7,a), (7,n), (7,t), (7,h), (7,e), (7,r), (7,x), (5,x), (5,e), (5,a), (5,g), (5,l), (5,e), (5,x))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_flatmapwith_deprecated&#34;&gt;2.13. flatMapWith (deprecated)&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to flatMap, but allows accessing the partition index or a derivative of the partition index from within the flatMap-function.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def flatMapWith[A: ClassTag, U: ClassTag](constructA: Int =&amp;gt; A, preservesPartitioning: Boolean = false)(f: (T, A) =&amp;gt; Seq[U]): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 3)
a.flatMapWith(x =&amp;gt; x, true)((x, y) =&amp;gt; List(y, x)).collect
res58: Array[Int] = Array(0, 1, 0, 2, 0, 3, 1, 4, 1, 5, 1, 6, 2, 7, 2, 8, 2, 9)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_fold&#34;&gt;2.14. fold&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Aggregates the values of each partition. The aggregation variable within each partition is initialized with zeroValue.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def fold(zeroValue: T)(op: (T, T) =&amp;gt; T): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1,2,3), 3)
a.fold(0)(_ + _)
res59: Int = 6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_foldbykey_pair&#34;&gt;2.15. foldByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very similar to fold, but performs the folding separately for each key of the RDD. This function is only available if the RDD consists of two-component tuples.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def foldByKey(zeroValue: V)(func: (V, V) =&amp;gt; V): RDD[(K, V)]
def foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) =&amp;gt; V): RDD[(K, V)]
def foldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) =&amp;gt; V): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;cat&#34;, &#34;owl&#34;, &#34;gnu&#34;, &#34;ant&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.foldByKey(&#34;&#34;)(_ + _).collect
res84: Array[(Int, String)] = Array((3,dogcatowlgnuant)

val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.foldByKey(&#34;&#34;)(_ + _).collect
res85: Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_foreach&#34;&gt;2.16. foreach&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Executes an parameterless function for each data item.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def foreach(f: T =&amp;gt; Unit)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;cat&#34;, &#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;gnu&#34;, &#34;crocodile&#34;, &#34;ant&#34;, &#34;whale&#34;, &#34;dolphin&#34;, &#34;spider&#34;), 3)
c.foreach(x =&amp;gt; println(x + &#34;s are yummy&#34;))
lions are yummy
gnus are yummy
crocodiles are yummy
ants are yummy
whales are yummy
dolphins are yummy
spiders are yummy&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_foreachpartition&#34;&gt;2.17. foreachPartition&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Executes an parameterless function for each partition. Access to the data items contained in the partition is provided via the iterator argument.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def foreachPartition(f: Iterator[T] =&amp;gt; Unit)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)
b.foreachPartition(x =&amp;gt; println(x.reduce(_ + _)))
6
15
24&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_foreachwith_deprecated&#34;&gt;2.18. foreachWith (Deprecated)&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Executes an parameterless function for each partition. Access to the data items contained in the partition is provided via the iterator argument.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def foreachWith[A: ClassTag](constructA: Int =&amp;gt; A)(f: (T, A) =&amp;gt; Unit)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
a.foreachWith(i =&amp;gt; i)((x,i) =&amp;gt; if (x % 2 == 1 &amp;amp;&amp;amp; i % 2 == 0) println(x) )
1
3
7
9&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_fullouterjoin_pair&#34;&gt;2.19. fullOuterJoin [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs the full outer join between two paired RDDs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def fullOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Option[V], Option[W]))]
def fullOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], Option[W]))]
def fullOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Option[V], Option[W]))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val pairRDD1 = sc.parallelize(List( (&#34;cat&#34;,2), (&#34;cat&#34;, 5), (&#34;book&#34;, 4),(&#34;cat&#34;, 12)))
val pairRDD2 = sc.parallelize(List( (&#34;cat&#34;,2), (&#34;cup&#34;, 5), (&#34;mouse&#34;, 4),(&#34;cat&#34;, 12)))
pairRDD1.fullOuterJoin(pairRDD2).collect

res5: Array[(String, (Option[Int], Option[Int]))] = Array((book,(Some(4),None)), (mouse,(None,Some(4))), (cup,(None,Some(5))), (cat,(Some(2),Some(2))), (cat,(Some(2),Some(12))), (cat,(Some(5),Some(2))), (cat,(Some(5),Some(12))), (cat,(Some(12),Some(2))), (cat,(Some(12),Some(12))))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_part3&#34;&gt;3. part3&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_generator_setgenerator&#34;&gt;3.1. generator, setGenerator&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Allows setting a string that is attached to the end of the RDD&amp;#8217;s name when printing the dependency graph.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;@transient var generator
def setGenerator(_generator: String)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_getcheckpointfile&#34;&gt;3.2. getCheckpointFile&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the path to the checkpoint file or null if RDD has not yet been checkpointed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def getCheckpointFile: Option[String]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sc.setCheckpointDir(&#34;/home/cloudera/Documents&#34;)
val a = sc.parallelize(1 to 500, 5)
val b = a++a++a++a++a
b.getCheckpointFile
res49: Option[String] = None

b.checkpoint
b.getCheckpointFile
res54: Option[String] = None

b.collect
b.getCheckpointFile
res57: Option[String] = Some(file:/home/cloudera/Documents/cb978ffb-a346-4820-b3ba-d56580787b20/rdd-40)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_preferredlocations&#34;&gt;3.3. preferredLocations&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the hosts which are preferred by this RDD. The actual preference of a specific host depends on various assumptions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;final def preferredLocations(split: Partition): Seq[String]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_getstoragelevel&#34;&gt;3.4. getStorageLevel&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Retrieves the currently set storage level of the RDD. This can only be used to assign a new storage level if the RDD does not have a storage level set yet. The example &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;below shows the error you will get, when you try to reassign the storage level.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def getStorageLevel&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 100000, 2)
a.persist(org.apache.spark.storage.StorageLevel.DISK_ONLY)
a.getStorageLevel.description
String = Disk Serialized 1x Replicated

a.cache
java.lang.UnsupportedOperationException: Cannot change storage level of an RDD after it was already assigned a level&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_glom&#34;&gt;3.5. glom&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Assembles an array that contains all elements of the partition and embeds it in an RDD. Each returned array contains the contents of one partition.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def glom(): RDD[Array[T]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 100, 3)
a.glom.collect
res8: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33), Array(34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66), Array(67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_groupby&#34;&gt;3.6. groupBy&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def groupBy[K: ClassTag](f: T =&amp;gt; K): RDD[(K, Iterable[T])]
def groupBy[K: ClassTag](f: T =&amp;gt; K, numPartitions: Int): RDD[(K, Iterable[T])]
def groupBy[K: ClassTag](f: T =&amp;gt; K, p: Partitioner): RDD[(K, Iterable[T])]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
a.groupBy(x =&amp;gt; { if (x % 2 == 0) &#34;even&#34; else &#34;odd&#34; }).collect
res42: Array[(String, Seq[Int])] = Array((even,ArrayBuffer(2, 4, 6, 8)), (odd,ArrayBuffer(1, 3, 5, 7, 9)))

val a = sc.parallelize(1 to 9, 3)
def myfunc(a: Int) : Int =
{
  a % 2
}
a.groupBy(myfunc).collect
res3: Array[(Int, Seq[Int])] = Array((0,ArrayBuffer(2, 4, 6, 8)), (1,ArrayBuffer(1, 3, 5, 7, 9)))

val a = sc.parallelize(1 to 9, 3)
def myfunc(a: Int) : Int =
{
  a % 2
}
a.groupBy(x =&amp;gt; myfunc(x), 3).collect
a.groupBy(myfunc(_), 1).collect
res7: Array[(Int, Seq[Int])] = Array((0,ArrayBuffer(2, 4, 6, 8)), (1,ArrayBuffer(1, 3, 5, 7, 9)))

import org.apache.spark.Partitioner
class MyPartitioner extends Partitioner {
def numPartitions: Int = 2
def getPartition(key: Any): Int =
{
    key match
    {
      case null     =&amp;gt; 0
      case key: Int =&amp;gt; key          % numPartitions
      case _        =&amp;gt; key.hashCode % numPartitions
    }
  }
  override def equals(other: Any): Boolean =
  {
    other match
    {
      case h: MyPartitioner =&amp;gt; true
      case _                =&amp;gt; false
    }
  }
}
val a = sc.parallelize(1 to 9, 3)
val p = new MyPartitioner()
val b = a.groupBy((x:Int) =&amp;gt; { x }, p)
val c = b.mapWith(i =&amp;gt; i)((a, b) =&amp;gt; (b, a))
c.collect
res42: Array[(Int, (Int, Seq[Int]))] = Array((0,(4,ArrayBuffer(4))), (0,(2,ArrayBuffer(2))), (0,(6,ArrayBuffer(6))), (0,(8,ArrayBuffer(8))), (1,(9,ArrayBuffer(9))), (1,(3,ArrayBuffer(3))), (1,(1,ArrayBuffer(1))), (1,(7,ArrayBuffer(7))), (1,(5,ArrayBuffer(5))))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_groupbykey_pair&#34;&gt;3.7. groupByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very similar to groupBy, but instead of supplying a function, the key-component of each pair will automatically be presented to the partitioner.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def groupByKey(): RDD[(K, Iterable[V])]
def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;spider&#34;, &#34;eagle&#34;), 2)
val b = a.keyBy(_.length)
b.groupByKey.collect
res11: Array[(Int, Seq[String])] = Array((4,ArrayBuffer(lion)), (6,ArrayBuffer(spider)), (3,ArrayBuffer(dog, cat)), (5,ArrayBuffer(tiger, eagle)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_histogram_double&#34;&gt;3.8. histogram [Double]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;These functions take an RDD of doubles and create a histogram with either even spacing (the number of buckets equals to bucketCount) or arbitrary spacing based on  custom bucket boundaries supplied by the user via an array of double values. The result type of both variants is slightly different, the first function will return a tuple consisting of two arrays. The first array contains the computed bucket boundary values and the second array contains the corresponding count of values (i.e. the histogram). The second variant of the function will just return the histogram as an array of integers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def histogram(bucketCount: Int): Pair[Array[Double], Array[Long]]
def histogram(buckets: Array[Double], evenBuckets: Boolean = false): Array[Long]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example  with even spacing &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1.1, 1.2, 1.3, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 9.0), 3)
a.histogram(5)
res11: (Array[Double], Array[Long]) = (Array(1.1, 2.68, 4.26, 5.84, 7.42, 9.0),Array(5, 0, 0, 1, 4))

val a = sc.parallelize(List(9.1, 1.0, 1.2, 2.1, 1.3, 5.0, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 10.0, 8.9, 5.5), 3)
a.histogram(6)
res18: (Array[Double], Array[Long]) = (Array(1.0, 2.5, 4.0, 5.5, 7.0, 8.5, 10.0),Array(6, 0, 1, 1, 3, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example with custom spacing &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(1.1, 1.2, 1.3, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 9.0), 3)
a.histogram(Array(0.0, 3.0, 8.0))
res14: Array[Long] = Array(5, 3)

val a = sc.parallelize(List(9.1, 1.0, 1.2, 2.1, 1.3, 5.0, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 10.0, 8.9, 5.5), 3)
a.histogram(Array(0.0, 5.0, 10.0))
res1: Array[Long] = Array(6, 9)

a.histogram(Array(0.0, 5.0, 10.0, 15.0))
res1: Array[Long] = Array(6, 8, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_id&#34;&gt;3.9. id&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Retrieves the ID which has been assigned to the RDD by its device context.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val id: Int&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(1 to 10, 10)
y.id
res16: Int = 19&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_intersection&#34;&gt;3.10. intersection&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the elements in the two RDDs which are the same.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def intersection(other: RDD[T], numPartitions: Int): RDD[T]
def intersection(other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]
def intersection(other: RDD[T]): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(1 to 20)
val y = sc.parallelize(10 to 30)
val z = x.intersection(y)

z.collect
res74: Array[Int] = Array(16, 12, 20, 13, 17, 14, 18, 10, 19, 15, 11)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_ischeckpointed&#34;&gt;3.11. isCheckpointed&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Indicates whether the RDD has been checkpointed. The flag will only raise once the checkpoint has really been created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def isCheckpointed: Boolean&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sc.setCheckpointDir(&#34;/home/cloudera/Documents&#34;)
c.isCheckpointed
res6: Boolean = false

c.checkpoint
c.isCheckpointed
res8: Boolean = false

c.collect
c.isCheckpointed
res9: Boolean = true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_iterator&#34;&gt;3.12. iterator&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns a compatible iterator object for a partition of this RDD. This function should never be called directly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;final def iterator(split: Partition, context: TaskContext): Iterator[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_join_pair&#34;&gt;3.13. join [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs an inner join using two key-value RDDs. Please note that the keys must be generally comparable to make this work.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
def join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]
def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val b = a.keyBy(_.length)
val c = sc.parallelize(List(&#34;dog&#34;,&#34;cat&#34;,&#34;gnu&#34;,&#34;salmon&#34;,&#34;rabbit&#34;,&#34;turkey&#34;,&#34;wolf&#34;,&#34;bear&#34;,&#34;bee&#34;), 3)
val d = c.keyBy(_.length)
b.join(d).collect

res0: Array[(Int, (String, String))] = Array((6,(salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (6,(salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (3,(dog,dog)), (3,(dog,cat)), (3,(dog,gnu)), (3,(dog,bee)), (3,(rat,dog)), (3,(rat,cat)), (3,(rat,gnu)), (3,(rat,bee)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_keyby&#34;&gt;3.14. keyBy&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Constructs two-component tuples (key-value pairs) by applying a function on each data item. The result of the function becomes the key and the original data item becomes the value of the newly created tuples.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def keyBy[K](f: T =&amp;gt; K): RDD[(K, T)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val b = a.keyBy(_.length)
b.collect
res26: Array[(Int, String)] = Array((3,dog), (6,salmon), (6,salmon), (3,rat), (8,elephant))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_keys_pair&#34;&gt;3.15. keys [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Extracts the keys from all contained tuples and returns them in a new RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def keys: RDD[K]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.keys.collect
res2: Array[Int] = Array(3, 5, 4, 3, 7, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_leftouterjoin_pair&#34;&gt;3.16. leftOuterJoin [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs an left outer join using two key-value RDDs. Please note that the keys must be generally comparable to make this work correctly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]
def leftOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, Option[W]))]
def leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val b = a.keyBy(_.length)
val c = sc.parallelize(List(&#34;dog&#34;,&#34;cat&#34;,&#34;gnu&#34;,&#34;salmon&#34;,&#34;rabbit&#34;,&#34;turkey&#34;,&#34;wolf&#34;,&#34;bear&#34;,&#34;bee&#34;), 3)
val d = c.keyBy(_.length)
b.leftOuterJoin(d).collect

res1: Array[(Int, (String, Option[String]))] = Array((6,(salmon,Some(salmon))), (6,(salmon,Some(rabbit))), (6,(salmon,Some(turkey))), (6,(salmon,Some(salmon))), (6,(salmon,Some(rabbit))), (6,(salmon,Some(turkey))), (3,(dog,Some(dog))), (3,(dog,Some(cat))), (3,(dog,Some(gnu))), (3,(dog,Some(bee))), (3,(rat,Some(dog))), (3,(rat,Some(cat))), (3,(rat,Some(gnu))), (3,(rat,Some(bee))), (8,(elephant,None)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_lookup&#34;&gt;3.17. lookup&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scans the RDD for all keys that match the provided value and returns their values as a Scala sequence.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def lookup(key: K): Seq[V]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.lookup(5)
res0: Seq[String] = WrappedArray(tiger, eagle)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_map&#34;&gt;3.18. map&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Applies a transformation function on each item of the RDD and returns the result as a new RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def map[U: ClassTag](f: T =&amp;gt; U): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val b = a.map(_.length)
val c = a.zip(b)
c.collect
res0: Array[(String, Int)] = Array((dog,3), (salmon,6), (salmon,6), (rat,3), (elephant,8))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mappartitions&#34;&gt;3.19. mapPartitions&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is a specialized map that is called only once for each partition. The entire content of the respective partitions is available as a sequential stream of values via the input argument (Iterarator[T]). The custom function must return yet another Iterator[U]. The combined result iterators are automatically converted into a new RDD. Please note, that the tuples (3,4) and (6,7) are missing from the following result due to the partitioning we chose.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapPartitions[U: ClassTag](f: Iterator[T] =&amp;gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example 1&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
def myfunc[T](iter: Iterator[T]) : Iterator[(T, T)] = {
  var res = List[(T, T)]()
  var pre = iter.next
  while (iter.hasNext)
  {
    val cur = iter.next;
    res .::= (pre, cur)
    pre = cur;
  }
  res.iterator
}
a.mapPartitions(myfunc).collect
res0: Array[(Int, Int)] = Array((2,3), (1,2), (5,6), (4,5), (8,9), (7,8))
Example :: 2

val x = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9,10), 3)
def myfunc(iter: Iterator[Int]) : Iterator[Int] = {
  var res = List[Int]()
  while (iter.hasNext) {
    val cur = iter.next;
    res = res ::: List.fill(scala.util.Random.nextInt(10))(cur)
  }
  res.iterator
}
x.mapPartitions(myfunc).collect
// some of the number are not outputted at all. This is because the random number generated for it is zero.
res8: Array[Int] = Array(1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 7, 7, 7, 9, 9, 10)
The above program can also be written using flatMap as follows.

Example :: 2 using flatmap

val x  = sc.parallelize(1 to 10, 3)
x.flatMap(List.fill(scala.util.Random.nextInt(10))(_)).collect

res1: Array[Int] = Array(1, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mappartitionswithcontext_deprecated_and_developer_api&#34;&gt;3.20. mapPartitionsWithContext   (deprecated and developer API)&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to mapPartitions, but allows accessing information about the processing state within the mapper.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapPartitionsWithContext[U: ClassTag](f: (TaskContext, Iterator[T]) =&amp;gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]

Example ::

val a = sc.parallelize(1 to 9, 3)
import org.apache.spark.TaskContext
def myfunc(tc: TaskContext, iter: Iterator[Int]) : Iterator[Int] = {
  tc.addOnCompleteCallback(() =&amp;gt; println(
    &#34;Partition: &#34;     + tc.partitionId +
    &#34;, AttemptID: &#34;   + tc.attemptId ))

  iter.toList.filter(_ % 2 == 0).iterator
}
a.mapPartitionsWithContext(myfunc).collect

14/04/01 23:05:48 INFO SparkContext: Starting job: collect at &amp;lt;console&amp;gt;:20
...
14/04/01 23:05:48 INFO Executor: Running task ID 0
Partition: 0, AttemptID: 0, Interrupted: false
...
14/04/01 23:05:48 INFO Executor: Running task ID 1
14/04/01 23:05:48 INFO TaskSetManager: Finished TID 0 in 470 ms on localhost (progress: 0/3)
...
14/04/01 23:05:48 INFO Executor: Running task ID 2
14/04/01 23:05:48 INFO TaskSetManager: Finished TID 1 in 23 ms on localhost (progress: 1/3)
14/04/01 23:05:48 INFO DAGScheduler: Completed ResultTask(0, 1)

?
res0: Array[Int] = Array(2, 6, 4, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mappartitionswithindex&#34;&gt;3.21. mapPartitionsWithIndex&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to mapPartitions, but takes two parameters. The first parameter is the index of the partition and the second is an iterator through all the items within this partition. The output is an iterator containing the list of items after applying whatever transformation the function encodes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapPartitionsWithIndex[U: ClassTag](f: (Int, Iterator[T]) =&amp;gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 3)
def myfunc(index: Int, iter: Iterator[Int]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; index + &#34;,&#34; + x).iterator
}
x.mapPartitionsWithIndex(myfunc).collect()
res10: Array[String] = Array(0,1, 0,2, 0,3, 1,4, 1,5, 1,6, 2,7, 2,8, 2,9, 2,10)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mappartitionswithsplit&#34;&gt;3.22. mapPartitionsWithSplit&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This method has been marked as deprecated in the API. So, you should not use this method anymore. Deprecated methods will not be covered in this document.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapPartitionsWithSplit[U: ClassTag](f: (Int, Iterator[T]) =&amp;gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mapvalues_pair&#34;&gt;3.23. mapValues [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Takes the values of a RDD that consists of two-component tuples, and applies the provided function to transform each value. Then, it forms new two-component tuples using the key and the transformed value and stores them in a new RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapValues[U](f: V =&amp;gt; U): RDD[(K, U)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.mapValues(&#34;x&#34; + _ + &#34;x&#34;).collect
res5: Array[(Int, String)] = Array((3,xdogx), (5,xtigerx), (4,xlionx), (3,xcatx), (7,xpantherx), (5,xeaglex))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mapwith_deprecated&#34;&gt;3.24. mapWith  (deprecated)&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is an extended version of map. It takes two function arguments. The first argument must conform to Int &amp;#8594; T and is executed once per partition. It will map the partition index to some transformed partition index of type T. This is where it is nice to do some kind of initialization code once per partition. Like create a Random number generator object. The second function must conform to (U, T) &amp;#8594; U. T is the transformed partition index and U is a data item of the RDD. Finally the function has to return a transformed data item of type U.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mapWith[A: ClassTag, U: ClassTag](constructA: Int =&amp;gt; A, preservesPartitioning: Boolean = false)(f: (T, A) =&amp;gt; U): RDD[U]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;// generates 9 random numbers less than 1000.
val x = sc.parallelize(1 to 9, 3)
x.mapWith(a =&amp;gt; new scala.util.Random)((x, r) =&amp;gt; r.nextInt(1000)).collect
res0: Array[Int] = Array(940, 51, 779, 742, 757, 982, 35, 800, 15)

val a = sc.parallelize(1 to 9, 3)
val b = a.mapWith(&#34;Index:&#34; + _)((a, b) =&amp;gt; (&#34;Value:&#34; + a, b))
b.collect
res0: Array[(String, String)] = Array((Value:1,Index:0), (Value:2,Index:0), (Value:3,Index:0), (Value:4,Index:1), (Value:5,Index:1), (Value:6,Index:1), (Value:7,Index:2), (Value:8,Index:2), (Value:9,Index:2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_part4&#34;&gt;4. part4&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_max&#34;&gt;4.1. max&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the largest element in the RDD&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def max()(implicit ord: Ordering[T]): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(10 to 30)
y.max
res75: Int = 30

val a = sc.parallelize(List((10, &#34;dog&#34;), (3, &#34;tiger&#34;), (9, &#34;lion&#34;), (18, &#34;cat&#34;)))
a.max
res6: (Int, String) = (18,cat)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_mean_double_meanapprox_double&#34;&gt;4.2. mean [Double], meanApprox [Double]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Calls stats and extracts the mean component. The approximate version of the function can finish somewhat faster in some scenarios. However, it trades accuracy for speed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def mean(): Double
def meanApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(9.1, 1.0, 1.2, 2.1, 1.3, 5.0, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 10.0, 8.9, 5.5), 3)
a.mean
res0: Double = 5.3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_min&#34;&gt;4.3. min&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns the smallest element in the RDD&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def min()(implicit ord: Ordering[T]): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(10 to 30)
y.min
res75: Int = 10


val a = sc.parallelize(List((10, &#34;dog&#34;), (3, &#34;tiger&#34;), (9, &#34;lion&#34;), (8, &#34;cat&#34;)))
a.min
res4: (Int, String) = (3,tiger)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_name_setname&#34;&gt;4.4. name, setName&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Allows a RDD to be tagged with a custom name.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;@transient var name: String
def setName(_name: String)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(1 to 10, 10)
y.name
res13: String = null
y.setName(&#34;Fancy RDD Name&#34;)
y.name
res15: String = Fancy RDD Name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_partitionby_pair&#34;&gt;4.5. partitionBy [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Repartitions as key-value RDD using its keys. The partitioner implementation can be supplied as the first argument.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def partitionBy(partitioner: Partitioner): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_partitioner&#34;&gt;4.6. partitioner&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Specifies a function pointer to the default partitioner that will be used for groupBy, subtract, reduceByKey (from PairedRDDFunctions), etc. functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;@transient val partitioner: Option[Partitioner]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_partitions&#34;&gt;4.7. partitions&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns an array of the partition objects associated with this RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;final def partitions: Array[Partition]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;, &#34;Gnu&#34;, &#34;Rat&#34;), 2)
b.partitions
res48: Array[org.apache.spark.Partition] = Array(org.apache.spark.rdd.ParallelCollectionPartition@18aa, org.apache.spark.rdd.ParallelCollectionPartition@18ab)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_persist_cache&#34;&gt;4.8. persist, cache&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;These functions can be used to adjust the storage level of a RDD. When freeing up memory, Spark will use the storage level identifier to decide which partitions should be kept. The parameterless variants persist() and cache() are just abbreviations for persist(StorageLevel.MEMORY_ONLY). (Warning: Once the storage level has been changed, it cannot be changed again!)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def cache(): RDD[T]
def persist(): RDD[T]
def persist(newLevel: StorageLevel): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;, &#34;Gnu&#34;, &#34;Rat&#34;), 2)
c.getStorageLevel
res0: org.apache.spark.storage.StorageLevel = StorageLevel(false, false, false, false, 1)
c.cache
c.getStorageLevel
res2: org.apache.spark.storage.StorageLevel = StorageLevel(false, true, false, true, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_pipe&#34;&gt;4.9. pipe&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Takes the RDD data of each partition and sends it via stdin to a shell-command. The resulting output of the command is captured and returned as a RDD of string values.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def pipe(command: String): RDD[String]
def pipe(command: String, env: Map[String, String]): RDD[String]
def pipe(command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String =&amp;gt; Unit) =&amp;gt; Unit = null, printRDDElement: (T, String =&amp;gt; Unit) =&amp;gt; Unit = null): RDD[String]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
a.pipe(&#34;head -n 1&#34;).collect
res2: Array[String] = Array(1, 4, 7)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_randomsplit&#34;&gt;4.10. randomSplit&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Randomly splits an RDD into multiple smaller RDDs according to a weights Array which specifies the percentage of the total data elements that is assigned to each smaller RDD. Note the actual size of each smaller RDD is only approximately equal to the percentages specified by the weights Array. The second example &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;below shows the number of items in each smaller RDD does not exactly match the weights Array.   A random optional seed can be specified. This function is useful for spliting data into a training set and a testing set for machine learning.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def randomSplit(weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(1 to 10)
val splits = y.randomSplit(Array(0.6, 0.4), seed = 11L)
val training = splits(0)
val test = splits(1)
training.collect
res:85 Array[Int] = Array(1, 4, 5, 6, 8, 10)
test.collect
res86: Array[Int] = Array(2, 3, 7, 9)

val y = sc.parallelize(1 to 10)
val splits = y.randomSplit(Array(0.1, 0.3, 0.6))

val rdd1 = splits(0)
val rdd2 = splits(1)
val rdd3 = splits(2)

rdd1.collect
res87: Array[Int] = Array(4, 10)
rdd2.collect
res88: Array[Int] = Array(1, 3, 5, 8)
rdd3.collect
res91: Array[Int] = Array(2, 6, 7, 9)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_reduce&#34;&gt;4.11. reduce&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This function provides the well-known reduce functionality in Spark. Please note that any function f you provide, should be commutative in order to generate reproducible results.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def reduce(f: (T, T) =&amp;gt; T): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 100, 3)
a.reduce(_ + _)
res41: Int = 5050&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_reducebykey_pair_reducebykeylocally_pair_reducebykeytodriver_pair&#34;&gt;4.12. reduceByKey [Pair],  reduceByKeyLocally [Pair], reduceByKeyToDriver [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This function provides the well-known reduce functionality in Spark. Please note that any function f you provide, should be commutative in order to generate reproducible results.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def reduceByKey(func: (V, V) =&amp;gt; V): RDD[(K, V)]
def reduceByKey(func: (V, V) =&amp;gt; V, numPartitions: Int): RDD[(K, V)]
def reduceByKey(partitioner: Partitioner, func: (V, V) =&amp;gt; V): RDD[(K, V)]
def reduceByKeyLocally(func: (V, V) =&amp;gt; V): Map[K, V]
def reduceByKeyToDriver(func: (V, V) =&amp;gt; V): Map[K, V]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;cat&#34;, &#34;owl&#34;, &#34;gnu&#34;, &#34;ant&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.reduceByKey(_ + _).collect
res86: Array[(Int, String)] = Array((3,dogcatowlgnuant))

val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.reduceByKey(_ + _).collect
res87: Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_repartition&#34;&gt;4.13. repartition&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This function changes the number of partitions to the number specified by the numPartitions parameter&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd = sc.parallelize(List(1, 2, 10, 4, 5, 2, 1, 1, 1), 3)
rdd.partitions.length
res2: Int = 3
val rdd2  = rdd.repartition(5)
rdd2.partitions.length
res6: Int = 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_repartitionandsortwithinpartitions_ordered&#34;&gt;4.14. repartitionAndSortWithinPartitions [Ordered]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def repartitionAndSortWithinPartitions(partitioner: Partitioner): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;// first we will do range partitioning which is not sorted
val randRDD = sc.parallelize(List( (2,&#34;cat&#34;), (6, &#34;mouse&#34;),(7, &#34;cup&#34;), (3, &#34;book&#34;), (4, &#34;tv&#34;), (1, &#34;screen&#34;), (5, &#34;heater&#34;)), 3)
val rPartitioner = new org.apache.spark.RangePartitioner(3, randRDD)
val partitioned = randRDD.partitionBy(rPartitioner)
def myfunc(index: Int, iter: Iterator[(Int, String)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
partitioned.mapPartitionsWithIndex(myfunc).collect

res0: Array[String] = Array([partID:0, val: (2,cat)], [partID:0, val: (3,book)], [partID:0, val: (1,screen)], [partID:1, val: (4,tv)], [partID:1, val: (5,heater)], [partID:2, val: (6,mouse)], [partID:2, val: (7,cup)])


// now lets repartition but this time have it sorted
val partitioned = randRDD.repartitionAndSortWithinPartitions(rPartitioner)
def myfunc(index: Int, iter: Iterator[(Int, String)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
partitioned.mapPartitionsWithIndex(myfunc).collect

res1: Array[String] = Array([partID:0, val: (1,screen)], [partID:0, val: (2,cat)], [partID:0, val: (3,book)], [partID:1, val: (4,tv)], [partID:1, val: (5,heater)], [partID:2, val: (6,mouse)], [partID:2, val: (7,cup)])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_rightouterjoin_pair&#34;&gt;4.15. rightOuterJoin [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs an right outer join using two key-value RDDs. Please note that the keys must be generally comparable to make this work correctly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def rightOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], W))]
def rightOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Option[V], W))]
def rightOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Option[V], W))]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val b = a.keyBy(_.length)
val c = sc.parallelize(List(&#34;dog&#34;,&#34;cat&#34;,&#34;gnu&#34;,&#34;salmon&#34;,&#34;rabbit&#34;,&#34;turkey&#34;,&#34;wolf&#34;,&#34;bear&#34;,&#34;bee&#34;), 3)
val d = c.keyBy(_.length)
b.rightOuterJoin(d).collect

res2: Array[(Int, (Option[String], String))] = Array((6,(Some(salmon),salmon)), (6,(Some(salmon),rabbit)), (6,(Some(salmon),turkey)), (6,(Some(salmon),salmon)), (6,(Some(salmon),rabbit)), (6,(Some(salmon),turkey)), (3,(Some(dog),dog)), (3,(Some(dog),cat)), (3,(Some(dog),gnu)), (3,(Some(dog),bee)), (3,(Some(rat),dog)), (3,(Some(rat),cat)), (3,(Some(rat),gnu)), (3,(Some(rat),bee)), (4,(None,wolf)), (4,(None,bear)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sample&#34;&gt;4.16. sample&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Randomly selects a fraction of the items of a RDD and returns them in a new RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sample(withReplacement: Boolean, fraction: Double, seed: Int): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 10000, 3)
a.sample(false, 0.1, 0).count
res24: Long = 960

a.sample(true, 0.3, 0).count
res25: Long = 2888

a.sample(true, 0.3, 13).count
res26: Long = 2985&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_samplebykey_pair&#34;&gt;4.17. sampleByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Randomly samples the key value pair RDD according to the fraction of each key you want to appear in the final RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sampleByKey(withReplacement: Boolean, fractions: Map[K, Double], seed: Long = Utils.random.nextLong): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val randRDD = sc.parallelize(List( (7,&#34;cat&#34;), (6, &#34;mouse&#34;),(7, &#34;cup&#34;), (6, &#34;book&#34;), (7, &#34;tv&#34;), (6, &#34;screen&#34;), (7, &#34;heater&#34;)))
val sampleMap = List((7, 0.4), (6, 0.6)).toMap
randRDD.sampleByKey(false, sampleMap,42).collect

res6: Array[(Int, String)] = Array((7,cat), (6,mouse), (6,book), (6,screen), (7,heater))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_samplebykeyexact_pair_experimental&#34;&gt;4.18. sampleByKeyExact [Pair, experimental]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is labelled as experimental and so we do not document it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sampleByKeyExact(withReplacement: Boolean, fractions: Map[K, Double], seed: Long = Utils.random.nextLong): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_saveashadoopfile_pair_saveashadoopdataset_pair_saveasnewapihadoopfile_pair&#34;&gt;4.19. saveAsHadoopFile [Pair], saveAsHadoopDataset [Pair], saveAsNewAPIHadoopFile [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Saves the RDD in a Hadoop compatible format using any Hadoop outputFormat class the user specifies.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def saveAsHadoopDataset(conf: JobConf)
def saveAsHadoopFile[F &amp;lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F])
def saveAsHadoopFile[F &amp;lt;: OutputFormat[K, V]](path: String, codec: Class[_ &amp;lt;: CompressionCodec]) (implicit fm: ClassTag[F])
def saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &amp;lt;: OutputFormat[_, _]], codec: Class[_ &amp;lt;: CompressionCodec])
def saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &amp;lt;: OutputFormat[_, _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[_ &amp;lt;: CompressionCodec]] = None)
def saveAsNewAPIHadoopFile[F &amp;lt;: NewOutputFormat[K, V]](path: String)(implicit fm: ClassTag[F])
def saveAsNewAPIHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &amp;lt;: NewOutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_saveasobjectfile&#34;&gt;4.20. saveAsObjectFile&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Saves the RDD in binary format.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def saveAsObjectFile(path: String)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(1 to 100, 3)
x.saveAsObjectFile(&#34;objFile&#34;)
val y = sc.objectFile[Int](&#34;objFile&#34;)
y.collect
res52: Array[Int] =  Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_saveassequencefile_seqfile&#34;&gt;4.21. saveAsSequenceFile [SeqFile]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Saves the RDD as a Hadoop sequence file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def saveAsSequenceFile(path: String, codec: Option[Class[_ &amp;lt;: CompressionCodec]] = None)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val v = sc.parallelize(Array((&#34;owl&#34;,3), (&#34;gnu&#34;,4), (&#34;dog&#34;,1), (&#34;cat&#34;,2), (&#34;ant&#34;,5)), 2)
v.saveAsSequenceFile(&#34;hd_seq_file&#34;)
14/04/19 05:45:43 INFO FileOutputCommitter: Saved output of task &#39;attempt_201404190545_0000_m_000001_191&#39; to file:/home/cloudera/hd_seq_file

[cloudera@localhost ~]$ ll ~/hd_seq_file
total 8
-rwxr-xr-x 1 cloudera cloudera 117 Apr 19 05:45 part-00000
-rwxr-xr-x 1 cloudera cloudera 133 Apr 19 05:45 part-00001
-rwxr-xr-x 1 cloudera cloudera   0 Apr 19 05:45 _SUCCESS&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_saveastextfile&#34;&gt;4.22. saveAsTextFile&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Saves the RDD as text files. One line at a time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def saveAsTextFile(path: String)
def saveAsTextFile(path: String, codec: Class[_ &amp;lt;: CompressionCodec])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;without compression&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 10000, 3)
a.saveAsTextFile(&#34;mydata_a&#34;)
14/04/03 21:11:36 INFO FileOutputCommitter: Saved output of task &#39;attempt_201404032111_0000_m_000002_71&#39; to file:/home/cloudera/Documents/spark-0.9.0-incubating-bin-cdh4/bin/mydata_a


[cloudera@localhost ~]$ head -n 5 ~/Documents/spark-0.9.0-incubating-bin-cdh4/bin/mydata_a/part-00000
1
2
3
4
5

// Produces 3 output files since we have created the a RDD with 3 partitions
[cloudera@localhost ~]$ ll ~/Documents/spark-0.9.0-incubating-bin-cdh4/bin/mydata_a/
-rwxr-xr-x 1 cloudera cloudera 15558 Apr  3 21:11 part-00000
-rwxr-xr-x 1 cloudera cloudera 16665 Apr  3 21:11 part-00001
-rwxr-xr-x 1 cloudera cloudera 16671 Apr  3 21:11 part-00002

Example :: with compression

import org.apache.hadoop.io.compress.GzipCodec
a.saveAsTextFile(&#34;mydata_b&#34;, classOf[GzipCodec])

[cloudera@localhost ~]$ ll ~/Documents/spark-0.9.0-incubating-bin-cdh4/bin/mydata_b/
total 24
-rwxr-xr-x 1 cloudera cloudera 7276 Apr  3 21:29 part-00000.gz
-rwxr-xr-x 1 cloudera cloudera 6517 Apr  3 21:29 part-00001.gz
-rwxr-xr-x 1 cloudera cloudera 6525 Apr  3 21:29 part-00002.gz

val x = sc.textFile(&#34;mydata_b&#34;)
x.count
res2: Long = 10000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example writing into HDFS &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(List(1,2,3,4,5,6,6,7,9,8,10,21), 3)
x.saveAsTextFile(&#34;hdfs://localhost:8020/user/cloudera/test&#34;);

val sp = sc.textFile(&#34;hdfs://localhost:8020/user/cloudera/sp_data&#34;)
sp.flatMap(_.split(&#34; &#34;)).saveAsTextFile(&#34;hdfs://localhost:8020/user/cloudera/sp_x&#34;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_part5&#34;&gt;5. part5&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_stats_double&#34;&gt;5.1. stats [Double]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Simultaneously computes the mean, variance and the standard deviation of all values in the RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def stats(): StatCounter&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(List(1.0, 2.0, 3.0, 5.0, 20.0, 19.02, 19.29, 11.09, 21.0), 2)
x.stats
res16: org.apache.spark.util.StatCounter = (count: 9, mean: 11.266667, stdev: 8.126859)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sortby&#34;&gt;5.2. sortBy&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This function sorts the input RDD&amp;#8217;s data and stores it in a new RDD. The first parameter requires you to specify a function which  maps the input data into the key that you want to sortBy. The second parameter (optional) specifies whether you want the data to be sorted in ascending or descending order.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sortBy[K](f: (T) ⇒ K, ascending: Boolean = true, numPartitions: Int = this.partitions.size)(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(Array(5, 7, 1, 3, 2, 1))
y.sortBy(c =&amp;gt; c, true).collect
res101: Array[Int] = Array(1, 1, 2, 3, 5, 7)

y.sortBy(c =&amp;gt; c, false).collect
res102: Array[Int] = Array(7, 5, 3, 2, 1, 1)

val z = sc.parallelize(Array((&#34;H&#34;, 10), (&#34;A&#34;, 26), (&#34;Z&#34;, 1), (&#34;L&#34;, 5)))
z.sortBy(c =&amp;gt; c._1, true).collect
res109: Array[(String, Int)] = Array((A,26), (H,10), (L,5), (Z,1))

z.sortBy(c =&amp;gt; c._2, true).collect
res108: Array[(String, Int)] = Array((Z,1), (L,5), (H,10), (A,26))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sortbykey_ordered&#34;&gt;5.3. sortByKey [Ordered]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This function sorts the input RDD&amp;#8217;s data and stores it in a new RDD. The output RDD is a shuffled RDD because it stores data that is output by a reducer which has been shuffled. The implementation of this function is actually very clever. First, it uses a range partitioner to partition the data in ranges within the shuffled RDD. Then it sorts these ranges individually with mapPartitions using standard sort mechanisms.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.size): RDD[P]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;cat&#34;, &#34;owl&#34;, &#34;gnu&#34;, &#34;ant&#34;), 2)
val b = sc.parallelize(1 to a.count.toInt, 2)
val c = a.zip(b)
c.sortByKey(true).collect
res74: Array[(String, Int)] = Array((ant,5), (cat,2), (dog,1), (gnu,4), (owl,3))
c.sortByKey(false).collect
res75: Array[(String, Int)] = Array((owl,3), (gnu,4), (dog,1), (cat,2), (ant,5))

val a = sc.parallelize(1 to 100, 5)
val b = a.cartesian(a)
val c = sc.parallelize(b.takeSample(true, 5, 13), 2)
val d = c.sortByKey(false)
res56: Array[(Int, Int)] = Array((96,9), (84,76), (59,59), (53,65), (52,4))




stdev [Double], sampleStdev [Double]

Calls stats and extracts either stdev-component or corrected sampleStdev-component.

Listing Variants ::

def stdev(): Double
def sampleStdev(): Double&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val d = sc.parallelize(List(0.0, 0.0, 0.0), 3)
d.stdev
res10: Double = 0.0
d.sampleStdev
res11: Double = 0.0

val d = sc.parallelize(List(0.0, 1.0), 3)
d.stdev
d.sampleStdev
res18: Double = 0.5
res19: Double = 0.7071067811865476

val d = sc.parallelize(List(0.0, 0.0, 1.0), 3)
d.stdev
res14: Double = 0.4714045207910317
d.sampleStdev
res15: Double = 0.5773502691896257&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_subtract&#34;&gt;5.4. subtract&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs the well known standard set subtraction operation: A - B&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def subtract(other: RDD[T]): RDD[T]
def subtract(other: RDD[T], numPartitions: Int): RDD[T]
def subtract(other: RDD[T], p: Partitioner): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
val b = sc.parallelize(1 to 3, 3)
val c = a.subtract(b)
c.collect
res3: Array[Int] = Array(6, 9, 4, 7, 5, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_subtractbykey_pair&#34;&gt;5.5. subtractByKey [Pair]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Very similar to subtract, but instead of supplying a function, the key-component of each pair will be automatically used as criterion for removing items from the first RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def subtractByKey[W: ClassTag](other: RDD[(K, W)]): RDD[(K, V)]
def subtractByKey[W: ClassTag](other: RDD[(K, W)], numPartitions: Int): RDD[(K, V)]
def subtractByKey[W: ClassTag](other: RDD[(K, W)], p: Partitioner): RDD[(K, V)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;spider&#34;, &#34;eagle&#34;), 2)
val b = a.keyBy(_.length)
val c = sc.parallelize(List(&#34;ant&#34;, &#34;falcon&#34;, &#34;squid&#34;), 2)
val d = c.keyBy(_.length)
b.subtractByKey(d).collect
res15: Array[(Int, String)] = Array((4,lion))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sum_double_sumapprox_double&#34;&gt;5.6. sum [Double], sumApprox [Double]&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Computes the sum of all values contained in the RDD. The approximate version of the function can finish somewhat faster in some scenarios. However, it trades accuracy for speed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def sum(): Double
def sumApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(List(1.0, 2.0, 3.0, 5.0, 20.0, 19.02, 19.29, 11.09, 21.0), 2)
x.sum
res17: Double = 101.39999999999999&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_take&#34;&gt;5.7. take&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Extracts the first n items of the RDD and returns them as an array. (Note: This sounds very easy, but it is actually quite a tricky problem for the implementors of Spark because the items in question can be in many different partitions.)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def take(num: Int): Array[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(&#34;dog&#34;, &#34;cat&#34;, &#34;ape&#34;, &#34;salmon&#34;, &#34;gnu&#34;), 2)
b.take(2)
res18: Array[String] = Array(dog, cat)

val b = sc.parallelize(1 to 10000, 5000)
b.take(100)
res6: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_takeordered&#34;&gt;5.8. takeOrdered&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Orders the data items of the RDD using their inherent implicit ordering function and returns the first n items as an array.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val b = sc.parallelize(List(&#34;dog&#34;, &#34;cat&#34;, &#34;ape&#34;, &#34;salmon&#34;, &#34;gnu&#34;), 2)
b.takeOrdered(2)
res19: Array[String] = Array(ape, cat)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_takesample&#34;&gt;5.9. takeSample&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Behaves different from sample in the following respects:
  It will return an exact number of samples (Hint: 2nd parameter)
  It returns an Array instead of RDD.
  It internally randomizes the order of the items returned.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def takeSample(withReplacement: Boolean, num: Int, seed: Int): Array[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val x = sc.parallelize(1 to 1000, 3)
x.takeSample(true, 100, 1)
res3: Array[Int] = Array(339, 718, 810, 105, 71, 268, 333, 360, 341, 300, 68, 848, 431, 449, 773, 172, 802, 339, 431, 285, 937, 301, 167, 69, 330, 864, 40, 645, 65, 349, 613, 468, 982, 314, 160, 675, 232, 794, 577, 571, 805, 317, 136, 860, 522, 45, 628, 178, 321, 482, 657, 114, 332, 728, 901, 290, 175, 876, 227, 130, 863, 773, 559, 301, 694, 460, 839, 952, 664, 851, 260, 729, 823, 880, 792, 964, 614, 821, 683, 364, 80, 875, 813, 951, 663, 344, 546, 918, 436, 451, 397, 670, 756, 512, 391, 70, 213, 896, 123, 858)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_todebugstring&#34;&gt;5.10. toDebugString&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Returns a string that contains debug information about the RDD and its dependencies.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def toDebugString: String&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 9, 3)
val b = sc.parallelize(1 to 3, 3)
val c = a.subtract(b)
c.toDebugString
res6: String =
MappedRDD[15] at subtract at &amp;lt;console&amp;gt;:16 (3 partitions)
  SubtractedRDD[14] at subtract at &amp;lt;console&amp;gt;:16 (3 partitions)
    MappedRDD[12] at subtract at &amp;lt;console&amp;gt;:16 (3 partitions)
      ParallelCollectionRDD[10] at parallelize at &amp;lt;console&amp;gt;:12 (3 partitions)
    MappedRDD[13] at subtract at &amp;lt;console&amp;gt;:16 (3 partitions)
      ParallelCollectionRDD[11] at parallelize at &amp;lt;console&amp;gt;:12 (3 partitions)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;toJavaRDD&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Embeds this RDD object within a JavaRDD object and returns it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def toJavaRDD() : JavaRDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(List(&#34;Gnu&#34;, &#34;Cat&#34;, &#34;Rat&#34;, &#34;Dog&#34;), 2)
c.toJavaRDD
res3: org.apache.spark.api.java.JavaRDD[String] = ParallelCollectionRDD[6] at parallelize at &amp;lt;console&amp;gt;:12&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_tolocaliterator&#34;&gt;5.11. toLocalIterator&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Converts the RDD into a scala iterator at the master node.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def toLocalIterator: Iterator[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(1,2,3,4,5,6), 2)
val iter = z.toLocalIterator

iter.next
res51: Int = 1

iter.next
res52: Int = 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_top&#34;&gt;5.12. top&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Utilizes the implicit ordering of $T$ to determine the top $k$ values and returns them as an array.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ddef top(num: Int)(implicit ord: Ordering[T]): Array[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val c = sc.parallelize(Array(6, 9, 4, 7, 5, 8), 2)
c.top(2)
res28: Array[Int] = Array(9, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_tostring&#34;&gt;5.13. toString&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Assembles a human-readable textual description of the RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;override def toString: String&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(1,2,3,4,5,6), 2)
z.toString
res61: String = ParallelCollectionRDD[80] at parallelize at &amp;lt;console&amp;gt;:21

val randRDD = sc.parallelize(List( (7,&#34;cat&#34;), (6, &#34;mouse&#34;),(7, &#34;cup&#34;), (6, &#34;book&#34;), (7, &#34;tv&#34;), (6, &#34;screen&#34;), (7, &#34;heater&#34;)))
val sortedRDD = randRDD.sortByKey()
sortedRDD.toString
res64: String = ShuffledRDD[88] at sortByKey at &amp;lt;console&amp;gt;:23&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_treeaggregate&#34;&gt;5.14. treeAggregate&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Computes the same thing as aggregate, except it aggregates the elements of the RDD in a multi-level tree pattern. Another difference is that it does not use the initial value for the second reduce function (combOp).  By default a tree of depth 2 is used, but this can be changed via the depth parameter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def treeAggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U, depth: Int = 2)(implicit arg0: ClassTag[U]): U&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(1,2,3,4,5,6), 2)

// lets first print out the contents of the RDD with partition labels
def myfunc(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}

z.mapPartitionsWithIndex(myfunc).collect
res28: Array[String] = Array([partID:0, val: 1], [partID:0, val: 2], [partID:0, val: 3], [partID:1, val: 4], [partID:1, val: 5], [partID:1, val: 6])

z.treeAggregate(0)(math.max(_, _), _ + _)
res40: Int = 9

// Note unlike normal aggregrate. Tree aggregate does not apply the initial value for the second reduce
// This example :: returns 11 since the initial value is 5
// reduce of partition 0 will be max(5, 1, 2, 3) = 5
// reduce of partition 1 will be max(4, 5, 6) = 6
// final reduce across partitions will be 5 + 6 = 11
// note the final reduce does not include the initial value
z.treeAggregate(5)(math.max(_, _), _ + _)
res42: Int = 11&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_treereduce&#34;&gt;5.15. treeReduce&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Works like reduce except reduces the elements of the RDD in a multi-level tree pattern.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def  treeReduce(f: (T, T) ⇒ T, depth: Int = 2): T&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(List(1,2,3,4,5,6), 2)
z.treeReduce(_+_)
res49: Int = 21&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_union&#34;&gt;5.16. union, ++&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Performs the standard set operation: A union B&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def ++(other: RDD[T]): RDD[T]
def union(other: RDD[T]): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 3, 1)
val b = sc.parallelize(5 to 7, 1)
(a ++ b).collect
res0: Array[Int] = Array(1, 2, 3, 5, 6, 7)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_unpersist&#34;&gt;5.17. unpersist&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Dematerializes the RDD (i.e. Erases all data items from hard-disk and memory). However, the RDD object remains. If it is referenced in a computation, Spark will regenerate it automatically using the stored dependency graph.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def unpersist(blocking: Boolean = true): RDD[T]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val y = sc.parallelize(1 to 10, 10)
val z = (y++y)
z.collect
z.unpersist(true)
14/04/19 03:04:57 INFO UnionRDD: Removing RDD 22 from persistence list
14/04/19 03:04:57 INFO BlockManager: Removing RDD 22&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_values&#34;&gt;5.18. values&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&amp;gt;Extracts the values from all contained tuples and returns them in a new RDD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def values: RDD[V]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val b = a.map(x =&amp;gt; (x.length, x))
b.values.collect
res3: Array[String] = Array(dog, tiger, lion, cat, panther, eagle)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_variance_double_samplevariance_double&#34;&gt;5.19. variance [Double], sampleVariance [Double]&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Calls stats and extracts either variance-component or corrected sampleVariance-component.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def variance(): Double
def sampleVariance(): Double&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(List(9.1, 1.0, 1.2, 2.1, 1.3, 5.0, 2.0, 2.1, 7.4, 7.5, 7.6, 8.8, 10.0, 8.9, 5.5), 3)
a.variance
res70: Double = 10.605333333333332

val x = sc.parallelize(List(1.0, 2.0, 3.0, 5.0, 20.0, 19.02, 19.29, 11.09, 21.0), 2)
x.variance
res14: Double = 66.04584444444443

x.sampleVariance
res13: Double = 74.30157499999999&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_zip&#34;&gt;5.20. zip&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Joins two RDDs by combining the i-th of either partition with each other. The resulting RDD will consist of two-component tuples which are interpreted as key-value pairs by the methods provided by the PairRDDFunctions extension.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(1 to 100, 3)
val b = sc.parallelize(101 to 200, 3)
a.zip(b).collect
res1: Array[(Int, Int)] = Array((1,101), (2,102), (3,103), (4,104), (5,105), (6,106), (7,107), (8,108), (9,109), (10,110), (11,111), (12,112), (13,113), (14,114), (15,115), (16,116), (17,117), (18,118), (19,119), (20,120), (21,121), (22,122), (23,123), (24,124), (25,125), (26,126), (27,127), (28,128), (29,129), (30,130), (31,131), (32,132), (33,133), (34,134), (35,135), (36,136), (37,137), (38,138), (39,139), (40,140), (41,141), (42,142), (43,143), (44,144), (45,145), (46,146), (47,147), (48,148), (49,149), (50,150), (51,151), (52,152), (53,153), (54,154), (55,155), (56,156), (57,157), (58,158), (59,159), (60,160), (61,161), (62,162), (63,163), (64,164), (65,165), (66,166), (67,167), (68,168), (69,169), (70,170), (71,171), (72,172), (73,173), (74,174), (75,175), (76,176), (77,177), (78,...

val a = sc.parallelize(1 to 100, 3)
val b = sc.parallelize(101 to 200, 3)
val c = sc.parallelize(201 to 300, 3)
a.zip(b).zip(c).map((x) =&amp;gt; (x._1._1, x._1._2, x._2 )).collect
res12: Array[(Int, Int, Int)] = Array((1,101,201), (2,102,202), (3,103,203), (4,104,204), (5,105,205), (6,106,206), (7,107,207), (8,108,208), (9,109,209), (10,110,210), (11,111,211), (12,112,212), (13,113,213), (14,114,214), (15,115,215), (16,116,216), (17,117,217), (18,118,218), (19,119,219), (20,120,220), (21,121,221), (22,122,222), (23,123,223), (24,124,224), (25,125,225), (26,126,226), (27,127,227), (28,128,228), (29,129,229), (30,130,230), (31,131,231), (32,132,232), (33,133,233), (34,134,234), (35,135,235), (36,136,236), (37,137,237), (38,138,238), (39,139,239), (40,140,240), (41,141,241), (42,142,242), (43,143,243), (44,144,244), (45,145,245), (46,146,246), (47,147,247), (48,148,248), (49,149,249), (50,150,250), (51,151,251), (52,152,252), (53,153,253), (54,154,254), (55,155,255)...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;=== zipParititions&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Similar to zip. But provides more control over the zipping process.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def zipPartitions[B: ClassTag, V: ClassTag](rdd2: RDD[B])(f: (Iterator[T], Iterator[B]) =&amp;gt; Iterator[V]): RDD[V]
def zipPartitions[B: ClassTag, V: ClassTag](rdd2: RDD[B], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B]) =&amp;gt; Iterator[V]): RDD[V]
def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C])(f: (Iterator[T], Iterator[B], Iterator[C]) =&amp;gt; Iterator[V]): RDD[V]
def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B], Iterator[C]) =&amp;gt; Iterator[V]): RDD[V]
def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D])(f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) =&amp;gt; Iterator[V]): RDD[V]
def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag](rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) =&amp;gt; Iterator[V]): RDD[V]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val a = sc.parallelize(0 to 9, 3)
val b = sc.parallelize(10 to 19, 3)
val c = sc.parallelize(100 to 109, 3)
def myfunc(aiter: Iterator[Int], biter: Iterator[Int], citer: Iterator[Int]): Iterator[String] =
{
  var res = List[String]()
  while (aiter.hasNext &amp;amp;&amp;amp; biter.hasNext &amp;amp;&amp;amp; citer.hasNext)
  {
    val x = aiter.next + &#34; &#34; + biter.next + &#34; &#34; + citer.next
    res ::= x
  }
  res.iterator
}
a.zipPartitions(b, c)(myfunc).collect
res50: Array[String] = Array(2 12 102, 1 11 101, 0 10 100, 5 15 105, 4 14 104, 3 13 103, 9 19 109, 8 18 108, 7 17 107, 6 16 106)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_zipwithindex&#34;&gt;5.21. zipWithIndex&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Zips the elements of the RDD with its element indexes. The indexes start from 0. If the RDD is spread across multiple partitions then a spark Job is started to perform this operation.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def zipWithIndex(): RDD[(T, Long)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(Array(&#34;A&#34;, &#34;B&#34;, &#34;C&#34;, &#34;D&#34;))
val r = z.zipWithIndex
res110: Array[(String, Long)] = Array((A,0), (B,1), (C,2), (D,3))

val z = sc.parallelize(100 to 120, 5)
val r = z.zipWithIndex
r.collect
res11: Array[(Int, Long)] = Array((100,0), (101,1), (102,2), (103,3), (104,4), (105,5), (106,6), (107,7), (108,8), (109,9), (110,10), (111,11), (112,12), (113,13), (114,14), (115,15), (116,16), (117,17), (118,18), (119,19), (120,20))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_zipwithuniqueid&#34;&gt;5.22. zipWithUniqueId&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;This is different from zipWithIndex since just gives a unique id to each data element but the ids may not match the index number of the data element. This operation does not start a spark job even if the RDD is spread across multiple partitions.
Compare the results of the example below with that of the 2nd example of zipWithIndex. You should be able to see the difference.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Listing Variants&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def zipWithUniqueId(): RDD[(T, Long)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Example&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val z = sc.parallelize(100 to 120, 5)
val r = z.zipWithUniqueId
r.collect

res12: Array[(Int, Long)] = Array((100,0), (101,5), (102,10), (103,15), (104,1), (105,6), (106,11), (107,16), (108,2), (109,7), (110,12), (111,17), (112,3), (113,8), (114,13), (115,18), (116,4), (117,9), (118,14), (119,19), (120,24))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>spark-rdd</title>
      <link>/post/bigdata/spark/spark-rdd/</link>
      <pubDate>Sat, 08 Apr 2017 18:42:52 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-rdd/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;spark rdd&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_mappartitionswithindex&#34;&gt;1. mapPartitionsWithIndex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_aggregate&#34;&gt;2. aggregate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_aggregatebykey&#34;&gt;3. aggregateByKey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_checkpoint&#34;&gt;4. checkpoint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_combinebykey&#34;&gt;5. combineByKey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_countbykey&#34;&gt;6. countByKey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_filterbyrange&#34;&gt;7. filterByRange&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foldbykey&#34;&gt;8. foldByKey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_foreachpartition&#34;&gt;9. foreachPartition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_keys_values&#34;&gt;10. keys values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_tmp&#34;&gt;tmp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_mappartitionswithindex&#34;&gt;1. mapPartitionsWithIndex&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;asciinema-player src=&#34;/src/records/spark/2017-04-10.json&#34; cols=&#34;95&#34; rows=&#34;22&#34; speed=&#34;2&#34;&gt;&lt;/asciinema-player&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;map是对每个元素操作, mapPartitions是对其中的每个partition操作

mapPartitionsWithIndex : 把每个partition中的分区号和对应的值拿出来, 看源码
val func = (index: Int, iter: Iterator[(Int)]) =&amp;gt; {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
rdd1.mapPartitionsWithIndex(func).collect&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_aggregate&#34;&gt;2. aggregate&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;asciinema-player src=&#34;/src/records/spark/aggregate.json&#34; cols=&#34;95&#34; rows=&#34;22&#34; speed=&#34;2&#34;&gt;&lt;/asciinema-player&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;def func1(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
rdd1.mapPartitionsWithIndex(func1).collect
###是action操作, 第一个参数是初始值, 二:是2个函数[每个函数都是2个参数(第一个参数:先对个个分区进行合并, 第二个:对个个分区合并后的结果再进行合并), 输出一个参数]
###0 + (0+1+2+3+4   +   0+5+6+7+8+9)
rdd1.aggregate(0)(_+_, _+_)
rdd1.aggregate(0)(math.max(_, _), _ + _)
###5和1比, 得5再和234比得5 --&amp;gt; 5和6789比,得9 --&amp;gt; 5 + (5+9)
rdd1.aggregate(5)(math.max(_, _), _ + _)


val rdd2 = sc.parallelize(List(&#34;a&#34;,&#34;b&#34;,&#34;c&#34;,&#34;d&#34;,&#34;e&#34;,&#34;f&#34;),2)
def func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
rdd2.aggregate(&#34;&#34;)(_ + _, _ + _)
rdd2.aggregate(&#34;=&#34;)(_ + _, _ + _)

val rdd3 = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;345&#34;,&#34;4567&#34;),2)
rdd3.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.max(x.length, y.length).toString, (x,y) =&amp;gt; x + y)

val rdd4 = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;345&#34;,&#34;&#34;),2)
rdd4.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.min(x.length, y.length).toString, (x,y) =&amp;gt; x + y)

val rdd5 = sc.parallelize(List(&#34;12&#34;,&#34;23&#34;,&#34;&#34;,&#34;345&#34;),2)
rdd5.aggregate(&#34;&#34;)((x,y) =&amp;gt; math.min(x.length, y.length).toString, (x,y) =&amp;gt; x + y)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_aggregatebykey&#34;&gt;3. aggregateByKey&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;asciinema-player src=&#34;/src/records/spark/aggregateByKey.json&#34; cols=&#34;95&#34; rows=&#34;22&#34; speed=&#34;2&#34;&gt;&lt;/asciinema-player&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val pairRDD = sc.parallelize(List( (&#34;cat&#34;,2), (&#34;cat&#34;, 5), (&#34;mouse&#34;, 4),(&#34;cat&#34;, 12), (&#34;dog&#34;, 12), (&#34;mouse&#34;, 2)), 2)
def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {
  iter.toList.map(x =&amp;gt; &#34;[partID:&#34; +  index + &#34;, val: &#34; + x + &#34;]&#34;).iterator
}
pairRDD.mapPartitionsWithIndex(func2).collect
pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect
pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).collect&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_checkpoint&#34;&gt;4. checkpoint&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sc.setCheckpointDir(&#34;hdfs://node-1.itcast.cn:9000/ck&#34;)
val rdd = sc.textFile(&#34;hdfs://node-1.itcast.cn:9000/wc&#34;).flatMap(_.split(&#34; &#34;)).map((_, 1)).reduceByKey(_+_)
rdd.checkpoint
rdd.isCheckpointed
rdd.count
rdd.isCheckpointed
rdd.getCheckpointFile

coalesce, repartition
val rdd1 = sc.parallelize(1 to 10, 10)
val rdd2 = rdd1.coalesce(2, false)
rdd2.partitions.length

collectAsMap : Map(b -&amp;gt; 2, a -&amp;gt; 1)
val rdd = sc.parallelize(List((&#34;a&#34;, 1), (&#34;b&#34;, 2)))
rdd.collectAsMap&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_combinebykey&#34;&gt;5. combineByKey&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;asciinema-player src=&#34;/src/records/spark/combineByKey.json&#34; cols=&#34;95&#34; rows=&#34;22&#34; speed=&#34;2&#34;&gt;&lt;/asciinema-player&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;和reduceByKey是相同的效果
###第一个参数x:原封不动取出来, 第二个参数:是函数, 局部运算, 第三个:是函数, 对局部运算后的结果再做运算
###每个分区中每个key中value中的第一个值, (hello,1)(hello,1)(good,1)--&amp;gt;(hello(1,1),good(1))--&amp;gt;x就相当于hello的第一个1, good中的1



val rdd1 = sc.textFile(&#34;hdfs://master:9000/wordcount/input/&#34;).flatMap(_.split(&#34; &#34;)).map((_, 1))
val rdd2 = rdd1.combineByKey(x =&amp;gt; x, (a: Int, b: Int) =&amp;gt; a + b, (m: Int, n: Int) =&amp;gt; m + n)
rdd1.collect
rdd2.collect

###当input下有3个文件时(有3个block块, 不是有3个文件就有3个block, ), 每个会多加3个10
val rdd3 = rdd1.combineByKey(x =&amp;gt; x + 10, (a: Int, b: Int) =&amp;gt; a + b, (m: Int, n: Int) =&amp;gt; m + n)
rdd3.collect


val rdd4 = sc.parallelize(List(&#34;dog&#34;,&#34;cat&#34;,&#34;gnu&#34;,&#34;salmon&#34;,&#34;rabbit&#34;,&#34;turkey&#34;,&#34;wolf&#34;,&#34;bear&#34;,&#34;bee&#34;), 3)
val rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2), 3)

val rdd7 = rdd6.combineByKey(List(_), (x: List[String], y: String) =&amp;gt; x :+ y, (m: List[String], n: List[String]) =&amp;gt; m ++ n)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_countbykey&#34;&gt;6. countByKey&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd1 = sc.parallelize(List((&#34;a&#34;, 1), (&#34;b&#34;, 2), (&#34;b&#34;, 2), (&#34;c&#34;, 2), (&#34;c&#34;, 1)))
rdd1.countByKey
rdd1.countByValue&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_filterbyrange&#34;&gt;7. filterByRange&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd1 = sc.parallelize(List((&#34;e&#34;, 5), (&#34;c&#34;, 3), (&#34;d&#34;, 4), (&#34;c&#34;, 2), (&#34;a&#34;, 1)))
val rdd2 = rdd1.filterByRange(&#34;b&#34;, &#34;d&#34;)
rdd2.collect

flatMapValues  :  Array((a,1), (a,2), (b,3), (b,4))
val rdd3 = sc.parallelize(List((&#34;a&#34;, &#34;1 2&#34;), (&#34;b&#34;, &#34;3 4&#34;)))
val rdd4 = rdd3.flatMapValues(_.split(&#34; &#34;))
rdd4.collect&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_foldbykey&#34;&gt;8. foldByKey&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd1 = sc.parallelize(List(&#34;dog&#34;, &#34;wolf&#34;, &#34;cat&#34;, &#34;bear&#34;), 2)
val rdd2 = rdd1.map(x =&amp;gt; (x.length, x))
val rdd3 = rdd2.foldByKey(&#34;&#34;)(_+_)

val rdd = sc.textFile(&#34;hdfs://node-1.itcast.cn:9000/wc&#34;).flatMap(_.split(&#34; &#34;)).map((_, 1))
rdd.foldByKey(0)(_+_)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_foreachpartition&#34;&gt;9. foreachPartition&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)
rdd1.foreachPartition(x =&amp;gt; println(x.reduce(_ + _)))

keyBy : 以传入的参数做key
val rdd1 = sc.parallelize(List(&#34;dog&#34;, &#34;salmon&#34;, &#34;salmon&#34;, &#34;rat&#34;, &#34;elephant&#34;), 3)
val rdd2 = rdd1.keyBy(_.length)
rdd2.collect&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_keys_values&#34;&gt;10. keys values&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val rdd1 = sc.parallelize(List(&#34;dog&#34;, &#34;tiger&#34;, &#34;lion&#34;, &#34;cat&#34;, &#34;panther&#34;, &#34;eagle&#34;), 2)
val rdd2 = rdd1.map(x =&amp;gt; (x.length, x))
rdd2.keys.collect
rdd2.values.collect&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_tmp&#34; class=&#34;sect0&#34;&gt;tmp&lt;/h1&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;ssh root@196.168.1.34

docker exec -it spark-master /bin/bash

cd $SPARK_HOME \
&amp;amp;&amp;amp; bin/spark-shell --master spark://master:7077&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;(1,(CompactBuffer(b, b),CompactBuffer(c, c))),
(3,(CompactBuffer(b),CompactBuffer(c))),
(2,(CompactBuffer(b),CompactBuffer(c)))&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>spark-基础</title>
      <link>/post/bigdata/spark/spark-%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Sat, 08 Apr 2017 18:42:52 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-%E5%9F%BA%E7%A1%80/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;spark-基础&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_spark集群安装&#34;&gt;1. Spark集群安装&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_安装&#34;&gt;1.1. 安装&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_机器部署&#34;&gt;1.1.1. 机器部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_下载spark安装包&#34;&gt;1.1.2. 下载Spark安装包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_配置spark&#34;&gt;1.1.3. 配置Spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_执行第一个spark程序&#34;&gt;1.2. 执行第一个spark程序&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_启动spark_shell&#34;&gt;1.3. 启动Spark Shell&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_启动spark_shell_2&#34;&gt;1.3.1. 启动spark shell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_在spark_shell中编写wordcount程序&#34;&gt;1.3.2. 在spark shell中编写WordCount程序&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_在idea中编写wordcount程序&#34;&gt;1.4. 在IDEA中编写WordCount程序&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_spark集群安装&#34;&gt;1. Spark集群安装&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装&#34;&gt;1.1. 安装&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_机器部署&#34;&gt;1.1.1. 机器部署&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;准备两台以上Linux服务器，安装好JDK1.7&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_下载spark安装包&#34;&gt;1.1.2. 下载Spark安装包&lt;/h4&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;下载&lt;br&gt;
&lt;a href=&#34;http://www.apache.org/dyn/closer.lua/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz&#34; class=&#34;bare&#34;&gt;http://www.apache.org/dyn/closer.lua/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传解压安装包&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;上传 &lt;strong&gt;spark-1.5.2-bin-hadoop2.6.tgz&lt;/strong&gt; 安装包到Linux上&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压安装包到指定位置&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf spark-1.5.2-bin-hadoop2.6.tgz -C /usr/local&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_配置spark&#34;&gt;1.1.3. 配置Spark&lt;/h4&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;进入到Spark安装目录&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /usr/local/spark-1.5.2-bin-hadoop2.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;进入conf目录并重命名并修改spark-env.sh.template文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd conf/
mv spark-env.sh.template spark-env.sh
vi spark-env.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在该配置文件中添加如下配置&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;export JAVA_HOME=/usr/java/jdk1.7.0_45
export SPARK_MASTER_IP=node1.itcast.cn
export SPARK_MASTER_PORT=7077&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重命名并修改slaves.template文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mv slaves.template slaves
vi slaves
//在该文件中添加子节点所在的位置（Worker节点）
node2.itcast.cn
node3.itcast.cn
node4.itcast.cn&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将配置好的Spark拷贝到其他节点上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;scp -r spark-1.5.2-bin-hadoop2.6/ node2:/usr/local/
scp -r spark-1.5.2-bin-hadoop2.6/ node3:/usr/local/
scp -r spark-1.5.2-bin-hadoop2.6/ node4:/usr/local/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spark集群配置完毕，目前是1个 &lt;strong&gt;Master&lt;/strong&gt; ，3个 &lt;strong&gt;Work&lt;/strong&gt; ，在 &lt;strong&gt;node1&lt;/strong&gt; 上启动 &lt;strong&gt;Spark&lt;/strong&gt; 集群&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/sbin/start-all.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动后执行jps命令&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;主节点上有Master进程，其他子节点上有Work进行，登录Spark管理界面查看集群状态（主节点）：
http://node1:8080/
到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spark集群规划&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;node1，node2是Master；node3，node4，node5是Worker
安装配置zk集群，并启动zk集群
停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;export SPARK_DAEMON_JAVA_OPTS=&#34;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark&#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;node1&lt;/strong&gt; 节点上修改 &lt;strong&gt;slaves&lt;/strong&gt; 配置文件内容指定 &lt;strong&gt;worker&lt;/strong&gt; 节点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;node1&lt;/strong&gt; 上执行 &lt;strong&gt;sbin/start-all.sh&lt;/strong&gt; 脚本，然后在 &lt;strong&gt;node2&lt;/strong&gt; 上执行 &lt;strong&gt;sbin/start-master.sh&lt;/strong&gt; 启动第二个 &lt;strong&gt;Master&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;执行Spark程序&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_执行第一个spark程序&#34;&gt;1.2. 执行第一个spark程序&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://node1.itcast.cn:7077 \
--executor-memory 1G \
--total-executor-cores 2 \
/usr/local/spark-1.5.2-bin-hadoop2.6/lib/spark-examples-1.5.2-hadoop2.6.0.jar \
100&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;该算法是利用蒙特·卡罗算法求PI&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_启动spark_shell&#34;&gt;1.3. 启动Spark Shell&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_启动spark_shell_2&#34;&gt;1.3.1. 启动spark shell&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-shell \
--master spark://node1.itcast.cn:7077 \ &lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
--executor-memory 2g \ &lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
--total-executor-cores 2 &lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;colist arabic&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;master spark://node1.itcast.cn:7077 指定Master的地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;executor-memory 2g 指定每个worker可用内存为2G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;total-executor-cores 2 指定整个集群使用的cup核数为2个&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock tip&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-tip&#34; title=&#34;Tip&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_在spark_shell中编写wordcount程序&#34;&gt;1.3.2. 在spark shell中编写WordCount程序&lt;/h4&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;首先启动hdfs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向hdfs上传一个文件到hdfs://node1.itcast.cn:9000/words.txt&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在spark shell中用scala语言编写spark程序&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sc.textFile(&#34;hdfs://node1.itcast.cn:9000/words.txt&#34;).flatMap(_.split(&#34; &#34;))
.map((_,1)).reduceByKey(_+_).saveAsTextFile(&#34;hdfs://node1.itcast.cn:9000/out&#34;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用hdfs命令查看结果&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hdfs dfs -ls hdfs://node1.itcast.cn:9000/out/p*&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;admonitionblock tip&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-tip&#34; title=&#34;Tip&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
 &lt;strong&gt;sc&lt;/strong&gt; 是 &lt;strong&gt;SparkContext&lt;/strong&gt; 对象，该对象时提交 &lt;strong&gt;spark&lt;/strong&gt; 程序的入口
&lt;strong&gt;textFile(hdfs://node1.itcast.cn:9000/words.txt)&lt;/strong&gt; 是hdfs中读取数据
&lt;strong&gt;flatMap(&lt;em&gt;.split(&#34; &#34;))&lt;/strong&gt; 先map在压平
&lt;strong&gt;map&lt;/em&gt;,1&lt;/strong&gt; 将单词和1构成元组
&lt;strong&gt;reduceByKey(&lt;em&gt;+&lt;/em&gt;)&lt;/strong&gt; 按照 &lt;strong&gt;key&lt;/strong&gt; 进行r &lt;strong&gt;educe&lt;/strong&gt; ，并将v &lt;strong&gt;alue&lt;/strong&gt; 累加
&lt;strong&gt;saveAsTextFile(&#34;hdfs://node1.itcast.cn:9000/out&#34;)&lt;/strong&gt; 将结果写入到hdfs中
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_在idea中编写wordcount程序&#34;&gt;1.4. 在IDEA中编写WordCount程序&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;spark shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理jar包的依赖。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建一个项目&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;选择Maven项目，然后点击next&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;填写maven的GAV，然后点击next&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;填写项目名称，然后点击finish&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建好maven项目后，点击Enable Auto-Import&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置Maven的pom.xml&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&amp;gt;
&amp;lt;project xmlns=&#34;http://maven.apache.org/POM/4.0.0&#34;
         xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34;
         xsi:schemaLocation=&#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&#34;&amp;gt;
    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;

    &amp;lt;groupId&amp;gt;cn.itcast.spark&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spark-mvn&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;1.0-SNAPSHOT&amp;lt;/version&amp;gt;

    &amp;lt;properties&amp;gt;
        &amp;lt;maven.compiler.source&amp;gt;1.7&amp;lt;/maven.compiler.source&amp;gt;
        &amp;lt;maven.compiler.target&amp;gt;1.7&amp;lt;/maven.compiler.target&amp;gt;
        &amp;lt;encoding&amp;gt;UTF-8&amp;lt;/encoding&amp;gt;
        &amp;lt;scala.version&amp;gt;2.10.6&amp;lt;/scala.version&amp;gt;
        &amp;lt;scala.compat.version&amp;gt;2.10&amp;lt;/scala.compat.version&amp;gt;
    &amp;lt;/properties&amp;gt;

    &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.scala-lang&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;scala-library&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;${scala.version}&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;1.5.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spark-streaming_2.10&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;1.5.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;hadoop-client&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;2.6.2&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;

    &amp;lt;build&amp;gt;
        &amp;lt;sourceDirectory&amp;gt;src/main/scala&amp;lt;/sourceDirectory&amp;gt;
        &amp;lt;testSourceDirectory&amp;gt;src/test/scala&amp;lt;/testSourceDirectory&amp;gt;
        &amp;lt;plugins&amp;gt;
            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;net.alchim31.maven&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;scala-maven-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;3.2.0&amp;lt;/version&amp;gt;
                &amp;lt;executions&amp;gt;
                    &amp;lt;execution&amp;gt;
                        &amp;lt;goals&amp;gt;
                            &amp;lt;goal&amp;gt;compile&amp;lt;/goal&amp;gt;
                            &amp;lt;goal&amp;gt;testCompile&amp;lt;/goal&amp;gt;
                        &amp;lt;/goals&amp;gt;
                        &amp;lt;configuration&amp;gt;
                            &amp;lt;args&amp;gt;
                                &amp;lt;arg&amp;gt;-make:transitive&amp;lt;/arg&amp;gt;
                                &amp;lt;arg&amp;gt;-dependencyfile&amp;lt;/arg&amp;gt;
                                &amp;lt;arg&amp;gt;${project.build.directory}/.scala_dependencies&amp;lt;/arg&amp;gt;
                            &amp;lt;/args&amp;gt;
                        &amp;lt;/configuration&amp;gt;
                    &amp;lt;/execution&amp;gt;
                &amp;lt;/executions&amp;gt;
            &amp;lt;/plugin&amp;gt;
            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;maven-surefire-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;2.18.1&amp;lt;/version&amp;gt;
                &amp;lt;configuration&amp;gt;
                    &amp;lt;useFile&amp;gt;false&amp;lt;/useFile&amp;gt;
                    &amp;lt;disableXmlReport&amp;gt;true&amp;lt;/disableXmlReport&amp;gt;
                    &amp;lt;includes&amp;gt;
                        &amp;lt;include&amp;gt;**/*Test.*&amp;lt;/include&amp;gt;
                        &amp;lt;include&amp;gt;**/*Suite.*&amp;lt;/include&amp;gt;
                    &amp;lt;/includes&amp;gt;
                &amp;lt;/configuration&amp;gt;
            &amp;lt;/plugin&amp;gt;

            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;maven-shade-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;version&amp;gt;2.3&amp;lt;/version&amp;gt;
                &amp;lt;executions&amp;gt;
                    &amp;lt;execution&amp;gt;
                        &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;
                        &amp;lt;goals&amp;gt;
                            &amp;lt;goal&amp;gt;shade&amp;lt;/goal&amp;gt;
                        &amp;lt;/goals&amp;gt;
                        &amp;lt;configuration&amp;gt;
                            &amp;lt;filters&amp;gt;
                                &amp;lt;filter&amp;gt;
                                    &amp;lt;artifact&amp;gt;*:*&amp;lt;/artifact&amp;gt;
                                    &amp;lt;excludes&amp;gt;
                                        &amp;lt;exclude&amp;gt;META-INF/*.SF&amp;lt;/exclude&amp;gt;
                                        &amp;lt;exclude&amp;gt;META-INF/*.DSA&amp;lt;/exclude&amp;gt;
                                        &amp;lt;exclude&amp;gt;META-INF/*.RSA&amp;lt;/exclude&amp;gt;
                                    &amp;lt;/excludes&amp;gt;
                                &amp;lt;/filter&amp;gt;
                            &amp;lt;/filters&amp;gt;
                            &amp;lt;transformers&amp;gt;
                                &amp;lt;transformer implementation=&#34;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&#34;&amp;gt;
                                    &amp;lt;mainClass&amp;gt;cn.itcast.spark.WordCount&amp;lt;/mainClass&amp;gt;
                                &amp;lt;/transformer&amp;gt;
                            &amp;lt;/transformers&amp;gt;
                        &amp;lt;/configuration&amp;gt;
                    &amp;lt;/execution&amp;gt;
                &amp;lt;/executions&amp;gt;
            &amp;lt;/plugin&amp;gt;
        &amp;lt;/plugins&amp;gt;
    &amp;lt;/build&amp;gt;
&amp;lt;/project&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将src/main/java和src/test/java分别修改成src/main/scala和src/test/scala，与pom.xml中的配置保持一致&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;新建一个scala class，类型为Object&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编写spark程序&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;package cn.itcast.spark

import org.apache.spark.{SparkContext, SparkConf}

object WordCount {
  def main(args: Array[String]) {
    //创建SparkConf()并设置App名称
    val conf = new SparkConf().setAppName(&#34;WC&#34;)
    //创建SparkContext，该对象是提交spark App的入口
    val sc = new SparkContext(conf)
    //使用sc创建RDD并执行相应的transformation和action
    sc.textFile(args(0)).flatMap(_.split(&#34; &#34;)).map((_, 1)).reduceByKey(_+_, 1).sortBy(_._2, false).saveAsTextFile(args(1))
    //停止sc，结束该任务
    sc.stop()
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用Maven打包：首先修改pom.xml中的main class&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;点击idea右侧的Maven Project选项&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;点击Lifecycle,选择clean和package，然后点击Run Maven Build&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;选择编译成功的jar包，并将该jar上传到Spark集群中的某个节点上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;首先启动hdfs和Spark集群
启动hdfs&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/hadoop-2.6.1/sbin/start-dfs.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;启动spark&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/sbin/start-all.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用spark-submit命令提交Spark应用（注意参数的顺序）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \
--class cn.itcast.spark.WordCount \
--master spark://node1.itcast.cn:7077 \
--executor-memory 2G \
--total-executor-cores 4 \
/root/spark-mvn-1.0-SNAPSHOT.jar \
hdfs://node1.itcast.cn:9000/words.txt \
hdfs://node1.itcast.cn:9000/out&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看程序执行结果&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hdfs dfs -cat hdfs://node1.itcast.cn:9000/out/part-00000
(hello,6)
(tom,3)
(kitty,2)
(jerry,1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>spark参考</title>
      <link>/post/bigdata/spark/spark-%E5%8F%82%E8%80%83/</link>
      <pubDate>Sat, 08 Apr 2017 18:42:52 +0000</pubDate>
      
      <guid>/post/bigdata/spark/spark-%E5%8F%82%E8%80%83/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;spark&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_rdd&#34;&gt;1. RDD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_rdd&#34;&gt;1. RDD&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html&#34;&gt;SparkRDDAPIExamples&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/21483985&#34; class=&#34;bare&#34;&gt;https://zhuanlan.zhihu.com/p/21483985&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://tech.meituan.com/spark-tuning-basic.html&#34; class=&#34;bare&#34;&gt;http://tech.meituan.com/spark-tuning-basic.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/23655827?rf=27642986&#34; class=&#34;bare&#34;&gt;https://www.zhihu.com/question/23655827?rf=27642986&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/hseagle/category/569175.html&#34;&gt;Apache Spark源码走读&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scala实战</title>
      <link>/post/bigdata/scala/scala%E5%AE%9E%E6%88%98/</link>
      <pubDate>Fri, 07 Apr 2017 15:06:47 +0000</pubDate>
      
      <guid>/post/bigdata/scala/scala%E5%AE%9E%E6%88%98/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Scala实战&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_scala实战&#34;&gt;1. Scala实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_项目概述&#34;&gt;2. 项目概述&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_需求&#34;&gt;2.1. 需求&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_akka简介&#34;&gt;2.2. Akka简介&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_项目实现&#34;&gt;3. 项目实现&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_架构图&#34;&gt;3.1. 架构图&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_重要类介绍&#34;&gt;3.2. 重要类介绍&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_actorsystem&#34;&gt;3.2.1. ActorSystem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_actor&#34;&gt;3.2.2. Actor&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_master类&#34;&gt;3.3. Master类&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_worker类&#34;&gt;3.4. Worker类&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_scala实战&#34;&gt;1. Scala实战&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_项目概述&#34;&gt;2. 项目概述&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_需求&#34;&gt;2.1. 需求&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;目前大多数的分布式架构底层通信都是通过RPC实现的，RPC框架非常多，比如前我们学过的Hadoop项目的RPC通信框架，但是Hadoop在设计之初就是为了运行长达数小时的批量而设计的，在某些极端的情况下，任务提交的延迟很高，所有Hadoop的RPC显得有些笨重。

Spark 的RPC是通过Akka类库实现的，Akka用Scala语言开发，基于Actor并发模型实现，Akka具有高可靠、高性能、可扩展等特点，使用Akka可以轻松实现分布式RPC功能。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_akka简介&#34;&gt;2.2. Akka简介&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Akka基于Actor模型，提供了一个用于构建可扩展的（Scalable）、弹性的（Resilient）、快速响应的（Responsive）应用程序的平台。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Actor模型&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在计算机科学领域，Actor模型是一个并行计算（Concurrent Computation）模型，它把actor作为并行计算的基本元素来对待：为响应一个接收到的消息，一个actor能够自己做出一些决策，如创建更多的actor，或发送更多的消息，或者确定如何去响应接收到的下一个消息。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_151513.png&#34; alt=&#34;2017 04 07 151513&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Actor&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;是Akka中最核心的概念，它是一个封装了状态和行为的对象，Actor之间可以通过交换消息的方式进行通信，每个Actor都有自己的收件箱（Mailbox）。通过Actor能够简化锁及线程管理，可以非常容易地开发出正确地并发程序和并行系统，Actor具有如下特性：&lt;/p&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;提供了一种高级抽象，能够简化在并发（Concurrency）/并行（Parallelism）应用场景下的编程开发&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;提供了异步非阻塞的、高性能的事件驱动编程模型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;超级轻量级事件处理（每GB堆内存几百万Actor）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_项目实现&#34;&gt;3. 项目实现&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_架构图&#34;&gt;3.1. 架构图&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_151523.png&#34; alt=&#34;2017 04 07 151523&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_重要类介绍&#34;&gt;3.2. 重要类介绍&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_actorsystem&#34;&gt;3.2.1. ActorSystem&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在 &lt;strong&gt;Akka&lt;/strong&gt; 中， &lt;strong&gt;ActorSystem&lt;/strong&gt; 是一个重量级的结构，他需要分配多个线程，所以在实际应用中， &lt;strong&gt;ActorSystem&lt;/strong&gt; 通常是一个单例对象，我们可以使用这个 &lt;strong&gt;ActorSystem&lt;/strong&gt; 创建很多 &lt;strong&gt;Actor&lt;/strong&gt; 。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_actor&#34;&gt;3.2.2. Actor&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Akka中，Actor负责通信，在Actor中有一些重要的生命周期方法。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;preStart()&lt;/strong&gt; 方法：该方法在 &lt;strong&gt;Actor&lt;/strong&gt; 对象构造方法执行后执行，整个 &lt;strong&gt;Actor&lt;/strong&gt; 生命周期中仅执行一次。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;receive()&lt;/strong&gt; 方法：该方法在 &lt;strong&gt;Actor&lt;/strong&gt; 的 &lt;strong&gt;preStart&lt;/strong&gt; 方法执行完成后执行，用于接收消息，会被反复执行。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_master类&#34;&gt;3.3. Master类&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.concurrent.duration._
import akka.actor.{Props, ActorSystem, Actor}
import akka.actor.Actor.Receive
import com.typesafe.config.ConfigFactory

import scala.collection.mutable

/**
  * Master为整个集群中的主节点
  * Master继承了Actor
  */
class Master extends Actor{

  //保存WorkerID和Work信息的map
  val idToWorker = new mutable.HashMap[String, WorkerInfo]
  //保存所有Worker信息的Set
  val workers = new mutable.HashSet[WorkerInfo]
  //Worker超时时间
  val WORKER_TIMEOUT = 10 * 1000
  //重新receive方法

  //导入隐式转换，用于启动定时器
  import context.dispatcher

  //构造方法执行完执行一次
  override def preStart(): Unit = {
    //启动定时器，定时执行
    context.system.scheduler.schedule(0 millis, WORKER_TIMEOUT millis, self, CheckOfTimeOutWorker)
  }

  //该方法会被反复执行，用于接收消息，通过case class模式匹配接收消息
  override def receive: Receive = {
    //Worker向Master发送的注册消息
    case RegisterWorker(id, workerHost, memory, cores) =&amp;gt; {
      if(!idToWorker.contains(id)) {
        val worker = new WorkerInfo(id, workerHost, memory, cores)
        workers.add(worker)
        idToWorker(id) = worker
        sender ! RegisteredWorker(&#34;192.168.10.1&#34;)
      }
    }

    //Worker向Master发送的心跳消息
    case HeartBeat(workerId) =&amp;gt; {
      val workerInfo = idToWorker(workerId)
      workerInfo.lastHeartbeat = System.currentTimeMillis()
    }

    //Master自己向自己发送的定期检查超时Worker的消息
    case CheckOfTimeOutWorker =&amp;gt; {
      val currentTime = System.currentTimeMillis()
      val toRemove = workers.filter(w =&amp;gt; currentTime - w.lastHeartbeat &amp;gt;WORKER_TIMEOUT).toArray
      for(worker &amp;lt;- toRemove){
        workers -= worker
        idToWorker.remove(worker.id)
      }
      println(&#34;worker size: &#34;+ workers.size)
    }
  }
}

object Master {
  //程序执行入口
  def main(args: Array[String]) {

    val host = &#34;192.168.10.1&#34;
    val port = 8888
    //创建ActorSystem的必要参数
    val configStr =
      s&#34;&#34;&#34;
         |akka.actor.provider = &#34;akka.remote.RemoteActorRefProvider&#34;
         |akka.remote.netty.tcp.hostname = &#34;$host&#34;
         |akka.remote.netty.tcp.port = &#34;$port&#34;
&#34;&#34;&#34;.stripMargin
    val config = ConfigFactory.parseString(configStr)
    //ActorSystem是单例的，用来创建Actor
    val actorSystem = ActorSystem.create(&#34;MasterActorSystem&#34;, config)
    //启动Actor，Master会被实例化，生命周期方法会被调用
    actorSystem.actorOf(Props[Master], &#34;Master&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_worker类&#34;&gt;3.4. Worker类&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import java.util.UUID
import scala.concurrent.duration._
import akka.actor.{ActorSelection, Props, ActorSystem, Actor}
import akka.actor.Actor.Receive
import com.typesafe.config.ConfigFactory

/**
  * Worker为整个集群的从节点
  * Worker继承了Actor
  */
class Worker extends Actor{

  //Worker端持有Master端的引用（代理对象）
  var master: ActorSelection = null
  //生成一个UUID，作为Worker的标识
  val id = UUID.randomUUID().toString

  //构造方法执行完执行一次
  override def preStart(): Unit = {
    //Worker向MasterActorSystem发送建立连接请求
    master = context.system.actorSelection(&#34;akka.tcp://MasterActorSystem@192.168.10.1:8888/user/Master&#34;)
    //Worker向Master发送注册消息
    master ! RegisterWorker(id, &#34;192.168.10.1&#34;, 10240, 8)
  }

  //该方法会被反复执行，用于接收消息，通过case class模式匹配接收消息
  override def receive: Receive = {
    //Master向Worker的反馈信息
    case RegisteredWorker(masterUrl) =&amp;gt; {
      import context.dispatcher
      //启动定时任务，向Master发送心跳
      context.system.scheduler.schedule(0 millis, 5000 millis, self, SendHeartBeat)
    }

    case SendHeartBeat =&amp;gt; {
      println(&#34;worker send heartbeat&#34;)
      master ! HeartBeat(id)
    }
  }
}

object Worker {
  def main(args: Array[String]) {
    val clientPort = 2552
    //创建WorkerActorSystem的必要参数
    val configStr =
      s&#34;&#34;&#34;
         |akka.actor.provider = &#34;akka.remote.RemoteActorRefProvider&#34;
         |akka.remote.netty.tcp.port = $clientPort
&#34;&#34;&#34;.stripMargin
    val config = ConfigFactory.parseString(configStr)
    val actorSystem = ActorSystem(&#34;WorkerActorSystem&#34;, config)
    //启动Actor，Master会被实例化，生命周期方法会被调用
    actorSystem.actorOf(Props[Worker], &#34;Worker&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scala高级特性</title>
      <link>/post/bigdata/scala/scala%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/</link>
      <pubDate>Fri, 07 Apr 2017 14:47:38 +0000</pubDate>
      
      <guid>/post/bigdata/scala/scala%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Scala高级特性&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_高阶函数&#34;&gt;1. 高阶函数&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_概念&#34;&gt;1.1. 概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_作为值的函数&#34;&gt;1.2. 作为值的函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_匿名函数&#34;&gt;1.3. 匿名函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_将方法转换成函数&#34;&gt;1.4. 将方法转换成函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_柯里化&#34;&gt;1.5. 柯里化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_例子&#34;&gt;1.6. 例子&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_隐式转换和隐式参数&#34;&gt;2. 隐式转换和隐式参数&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_概念_2&#34;&gt;2.1. 概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_作用&#34;&gt;2.2. 作用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_隐式转换函数&#34;&gt;2.3. 隐式转换函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_隐式转换例子&#34;&gt;2.4. 隐式转换例子&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_隐式转换变量值&#34;&gt;2.4.1. 隐式转换变量值&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_隐式转换方法&#34;&gt;2.4.2. 隐式转换方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_高阶函数&#34;&gt;1. 高阶函数&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_概念&#34;&gt;1.1. 概念&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala混合了面向对象和函数式的特性，我们通常将可以做为参数传递到方法中的表达式叫做函数。在函数式编程语言中，函数是“头等公民”，高阶函数包含：作为值的函数、匿名函数、闭包、柯里化等等。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_作为值的函数&#34;&gt;1.2. 作为值的函数&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;可以像任何其他数据类型一样被传递和操作的函数，每当你想要给算法传入具体动作时这个特性就会变得非常有用。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145546.png&#34; alt=&#34;2017 04 07 145546&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;定义函数时格式：val 变量名 =(输入参数类型和个数)&amp;#8658;函数实现和返回值类型和个数
“=”表示将函数赋给一个变量
“&amp;#8658;”左面表示输入参数名称、类型和个数，右边表示函数的实现和返回值类型和参数个数&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_匿名函数&#34;&gt;1.3. 匿名函数&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中，你不需要给每一个函数命名，没有将函数赋给变量的函数叫做匿名函数&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145555.png&#34; alt=&#34;2017 04 07 145555&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;由于Scala可以自动推断出参数的类型，所有可以写的跟精简一些&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145603.png&#34; alt=&#34;2017 04 07 145603&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;还记得神奇的下划线吗？这才是终极方式&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145612.png&#34; alt=&#34;2017 04 07 145612&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_将方法转换成函数&#34;&gt;1.4. 将方法转换成函数&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中，方法和函数是不一样的，最本质的区别是函数可以做为参数传递到方法中
但是方法可以被转换成函数，神奇的下划线又出场了&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145620.png&#34; alt=&#34;2017 04 07 145620&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_柯里化&#34;&gt;1.5. 柯里化&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;柯里化指的是将原来接受两个参数的方法变成新的接受一个参数的方法的过程&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-07_145627.png&#34; alt=&#34;2017 04 07 145627&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_例子&#34;&gt;1.6. 例子&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object FunDemo {
  def main(args: Array[String]) {
    def f2(x: Int) = x * 2
    val f3 = (x: Int) =&amp;gt; x * 3
    val f4: (Int) =&amp;gt; Int = { x =&amp;gt; x * 4 }
    val f4a: (Int) =&amp;gt; Int = _ * 4
    val f5 = (_: Int) * 5
    val list = List(1, 2, 3, 4, 5)
    var new_list: List[Int] = null
    //第一种：最直观的方式 (Int) =&amp;gt; Int
    //new_list = list.map((x: Int) =&amp;gt; x * 3)

    //第二种：由于map方法知道你会传入一个类型为(Int) =&amp;gt; Int的函数，你可以简写
    //new_list = list.map((x) =&amp;gt; x * 3)

    //第三种：对于只有一个参数的函数，你可以省去参数外围的()
    //new_list = list.map(x =&amp;gt; x * 3)

    //第四种：(终极方式)如果参数在=&amp;gt;右侧只出现一次，可以使用_
    new_list = list.map(_ * 3)

    new_list.foreach(println(_))

    var a = Array(1,2,3)
    a.map(_* 3)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_隐式转换和隐式参数&#34;&gt;2. 隐式转换和隐式参数&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_概念_2&#34;&gt;2.1. 概念&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;隐式转换和隐式参数是Scala中两个非常强大的功能，利用隐式转换和隐式参数，你可以提供优雅的类库，对类库的使用者隐匿掉那些枯燥乏味的细节。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_作用&#34;&gt;2.2. 作用&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;隐式的对类的方法进行增强，丰富现有类库的功能&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_隐式转换函数&#34;&gt;2.3. 隐式转换函数&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;是指那种以implicit关键字声明的带有单个参数的函数&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_隐式转换例子&#34;&gt;2.4. 隐式转换例子&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;//隐式的增强File类的方法
class RichFile(val from: File) {
  def read = Source.fromFile(from.getPath).mkString
}

object RichFile {
  //隐式转换方法
  implicit def file2RichFile(from: File) = new RichFile(from)

}

object MainApp{
  def main(args: Array[String]): Unit = {
    //导入隐式转换
    import RichFile._
    //import RichFile.file2RichFile
    println(new File(&#34;c://words.txt&#34;).read)

  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import java.awt.GridLayout

object ImplicitContext {

  //implicit def girl2Ordered(g : Girl) = new Ordered[Girl]{
  //  override def compare(that: Girl): Int = if (g.faceValue &amp;gt; that.faceValue) 1 else -1
  //}

  implicit object OrderingGirl extends Ordering[Girl] {
    override def compare(x: Girl, y: Girl): Int = if (x.faceValue &amp;gt; y.faceValue) 1 else -1
  }

}

class Girl(var name: String, var faceValue: Double) {
  override def toString: String = s&#34;name : $name, faveValue : $faceValue&#34;
}

//class MissRight[T &amp;lt;% Ordered[T]](f: T, s: T){
//  def choose() = if(f &amp;gt; s) f else s
//}
//class MissRight[T](f: T, s: T){
//  def choose()(implicit ord: T =&amp;gt; Ordered[T]) = if (f &amp;gt; s) f else s
//}

class MissRight[T: Ordering](val f: T, val s: T) {
  def choose()(implicit ord: Ordering[T]) = if (ord.gt(f, s)) f else s
}

object MissRight {
  def main(args: Array[String]) {
    import ImplicitContext.OrderingGirl
    val g1 = new Girl(&#34;yuihatano&#34;, 99)
    val g2 = new Girl(&#34;jzmb&#34;, 98)
    val mr = new MissRight(g1, g2)
    val result = mr.choose()
    println(result)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_隐式转换变量值&#34;&gt;2.4.1. 隐式转换变量值&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/svg/scala-implic.svg&#34; alt=&#34;scala implic&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_隐式转换方法&#34;&gt;2.4.2. 隐式转换方法&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/svg/scala-implic2.svg&#34; alt=&#34;scala implic2&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scala Actor编程</title>
      <link>/post/bigdata/scala/scala-Actor%E7%BC%96%E7%A8%8B/</link>
      <pubDate>Wed, 05 Apr 2017 09:47:03 +0000</pubDate>
      
      <guid>/post/bigdata/scala/scala-Actor%E7%BC%96%E7%A8%8B/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Scala Actor编程&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_scala_actor编程&#34;&gt;1. Scala Actor编程&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_什么是scala_actor&#34;&gt;1.1. 什么是Scala Actor&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_概念&#34;&gt;1.1.1. 概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_传统java并发编程与scala_actor编程的区别&#34;&gt;1.1.2. 传统java并发编程与Scala Actor编程的区别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_actor方法执行顺序&#34;&gt;1.1.3. Actor方法执行顺序&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_发送消息的方式&#34;&gt;1.1.4. 发送消息的方式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_actor实战&#34;&gt;1.2. Actor实战&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_第一个例子&#34;&gt;1.2.1. 第一个例子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_第二个例子_可以不断地接收消息&#34;&gt;1.2.2. 第二个例子（可以不断地接收消息）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_第三个例子_react方式会复用线程_比receive更高效&#34;&gt;1.2.3. 第三个例子（react方式会复用线程，比receive更高效）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_第四个例子_结合case_class发送消息&#34;&gt;1.2.4. 第四个例子（结合case class发送消息）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_练习&#34;&gt;1.3. 练习&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_scala_actor编程&#34;&gt;1. Scala Actor编程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_什么是scala_actor&#34;&gt;1.1. 什么是Scala Actor&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_概念&#34;&gt;1.1.1. 概念&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala中的Actor能够实现并行编程的强大功能，它是基于事件模型的并发机制，Scala是运用消息（message）的发送、接收来实现多线程的。使用Scala能够更容易地实现多线程应用的开发。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_传统java并发编程与scala_actor编程的区别&#34;&gt;1.1.2. 传统java并发编程与Scala Actor编程的区别&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-05_095545.png&#34; alt=&#34;2017 04 05 095545&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;对于Java，我们都知道它的多线程实现需要对共享资源（变量、对象等）使用synchronized 关键字进行代码块同步、对象锁互斥等等。而且，常常一大块的try…catch语句块中加上wait方法、notify方法、notifyAll方法是让人很头疼的。原因就在于Java中多数使用的是可变状态的对象资源，对这些资源进行共享来实现多线程编程的话，控制好资源竞争与防止对象状态被意外修改是非常重要的，而对象状态的不变性也是较难以保证的。 而在Scala中，我们可以通过复制不可变状态的资源（即对象，Scala中一切都是对象，连函数、方法也是）的一个副本，再基于Actor的消息发送、接收机制进行并行编程&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_actor方法执行顺序&#34;&gt;1.1.3. Actor方法执行顺序&lt;/h4&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;首先调用start()方法启动Actor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调用start()方法后其act()方法会被执行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向Actor发送消息&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_发送消息的方式&#34;&gt;1.1.4. 发送消息的方式&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;!
发送异步消息，没有返回值。
!?
发送同步消息，等待返回值。
!!
发送异步消息，返回值是 Future[Any]。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_actor实战&#34;&gt;1.2. Actor实战&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_第一个例子&#34;&gt;1.2.1. 第一个例子&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object MyActor1 extends Actor{
  //重新act方法
  def act(){
    for(i &amp;lt;- 1 to 10){
      println(&#34;actor-1 &#34;+ i)
      Thread.sleep(2000)
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object MyActor2 extends Actor{
  //重新act方法
  def act(){
    for(i &amp;lt;- 1 to 10){
      println(&#34;actor-2 &#34;+ i)
      Thread.sleep(2000)
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ActorTest extends App{
  //启动Actor
  MyActor1.start()
  MyActor2.start()
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：上面分别调用了两个单例对象的start()方法，他们的act()方法会被执行，相同与在java中开启了两个线程，线程的run()方法会被执行
注意：这两个Actor是并行执行的，act()方法中的for循环执行完成后actor程序就退出了&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_第二个例子_可以不断地接收消息&#34;&gt;1.2.2. 第二个例子（可以不断地接收消息）&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class MyActor extends Actor {

  override def act(): Unit = {
    while (true) {
      receive {
        case &#34;start&#34;=&amp;gt; {
          println(&#34;starting ...&#34;)
          Thread.sleep(5000)
          println(&#34;started&#34;)
        }
        case &#34;stop&#34;=&amp;gt; {
          println(&#34;stopping ...&#34;)
          Thread.sleep(5000)
          println(&#34;stopped ...&#34;)
        }
      }
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object MyActor {
  def main(args: Array[String]) {
    val actor = new MyActor
    actor.start()
    actor ! &#34;start&#34;
    actor ! &#34;stop&#34;
    println(&#34;消息发送完成！&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：在act()方法中加入了while (true) 循环，就可以不停的接收消息
注意：发送start消息和stop的消息是异步的，但是Actor接收到消息执行的过程是同步的按顺序执行&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_第三个例子_react方式会复用线程_比receive更高效&#34;&gt;1.2.3. 第三个例子（react方式会复用线程，比receive更高效）&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class YourActor extends Actor {

  override def act(): Unit = {
    loop {
      react {
        case &#34;start&#34;=&amp;gt; {
          println(&#34;starting ...&#34;)
          Thread.sleep(5000)
          println(&#34;started&#34;)
        }
        case &#34;stop&#34;=&amp;gt; {
          println(&#34;stopping ...&#34;)
          Thread.sleep(8000)
          println(&#34;stopped ...&#34;)
        }
      }
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object YourActor {
  def main(args: Array[String]) {
    val actor = new YourActor
    actor.start()
    actor ! &#34;start&#34;
    actor ! &#34;stop&#34;
    println(&#34;消息发送完成！&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：?react 如果要反复执行消息处理，react外层要用loop，不能用while&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_第四个例子_结合case_class发送消息&#34;&gt;1.2.4. 第四个例子（结合case class发送消息）&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class AppleActor extends Actor {

  def act(): Unit = {
    while (true) {
      receive {
        case &#34;start&#34;=&amp;gt;println(&#34;starting ...&#34;)
        case SyncMsg(id, msg) =&amp;gt; {
          println(id + &#34;,sync &#34;+ msg)
          Thread.sleep(5000)
          sender ! ReplyMsg(3,&#34;finished&#34;)
        }
        case AsyncMsg(id, msg) =&amp;gt; {
          println(id + &#34;,async &#34;+ msg)
          Thread.sleep(5000)
        }
      }
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object AppleActor {
  def main(args: Array[String]) {
    val a = new AppleActor
    a.start()
    //异步消息
    a ! AsyncMsg(1, &#34;hello actor&#34;)
    println(&#34;异步消息发送完成&#34;)
    //同步消息
    //val content = a.!?(1000, SyncMsg(2, &#34;hello actor&#34;))
    //println(content)
    val reply = a !! SyncMsg(2, &#34;hello actor&#34;)
    println(reply.isSet)
    //println(&#34;123&#34;)
    val c = reply.apply()
    println(reply.isSet)
    println(c)
  }
}
case class SyncMsg(id : Int, msg: String)
case class AsyncMsg(id : Int, msg: String)
case class ReplyMsg(id : Int, msg: String)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_练习&#34;&gt;1.3. 练习&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;用actor并发编程写一个单机版的WorldCount，将多个文件作为输入，计算完成后将多个任务汇总，得到最终的结果&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class Task extends Actor {

  override def act(): Unit = {
    loop {
      react {
        case SubmitTask(fileName) =&amp;gt; {
          val contents = Source.fromFile(new File(fileName)).mkString
          val arr = contents.split(&#34;\r\n&#34;)
          val result = arr.flatMap(_.split(&#34;&#34;)).map((_, 1)).groupBy(_._1).mapValues(_.length)
          //val result = arr.flatMap(_.split(&#34;&#34;)).map((_, 1)).groupBy(_._1).mapValues(_.foldLeft(0)(_ + _._2))
          sender ! ResultTask(result)
        }
        case StopTask =&amp;gt; {
          exit()
        }
      }
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object WorkCount {
  def main(args: Array[String]) {
    val files = Array(&#34;c://words.txt&#34;, &#34;c://words.log&#34;)

    val replaySet = new mutable.HashSet[Future[Any]]
    val resultList = new mutable.ListBuffer[ResultTask]

    for(f &amp;lt;- files) {
      val t = new Task
      val replay = t.start() !! SubmitTask(f)
      replaySet += replay
    }

    while(replaySet.size &amp;gt;0){
      val toCumpute = replaySet.filter(_.isSet)
      for(r &amp;lt;- toCumpute){
        val result = r.apply()
        resultList += result.asInstanceOf[ResultTask]
        replaySet.remove(r)
      }
      Thread.sleep(100)
    }
    val finalResult = resultList.map(_.result).flatten.groupBy(_._1).mapValues(x =&amp;gt; x.foldLeft(0)(_ + _._2))
    println(finalResult)
  }
}

case class SubmitTask(fileName: String)
case object StopTask
case class ResultTask(result: Map[String, Int])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>scala基础</title>
      <link>/post/bigdata/scala/scala%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Tue, 04 Apr 2017 15:25:28 +0000</pubDate>
      
      <guid>/post/bigdata/scala/scala%E5%9F%BA%E7%A1%80/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;scala基础&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_scala基础&#34;&gt;1. Scala基础&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_声明变量&#34;&gt;1.1. 声明变量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_常用类型&#34;&gt;1.2. 常用类型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_条件表达式&#34;&gt;1.3. 条件表达式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_块表达式&#34;&gt;1.4. 块表达式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_循环&#34;&gt;1.5. 循环&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_调用方法和函数&#34;&gt;1.6. 调用方法和函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_定义方法和函数&#34;&gt;1.7. 定义方法和函数&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_定义方法&#34;&gt;1.7.1. 定义方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_定义函数&#34;&gt;1.7.2. 定义函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_方法和函数的区别&#34;&gt;1.7.3. 方法和函数的区别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_将方法转换成函数_神奇的下划线&#34;&gt;1.7.4. 将方法转换成函数（神奇的下划线）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_数组_映射_元组_集合&#34;&gt;2. 数组、映射、元组、集合&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_数组&#34;&gt;2.1. 数组&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_定长数组和变长数组&#34;&gt;2.1.1. 定长数组和变长数组&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_遍历数组&#34;&gt;2.1.2. 遍历数组&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_数组转换&#34;&gt;2.1.3. 数组转换&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_数组常用算法&#34;&gt;2.1.4. 数组常用算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_映射&#34;&gt;2.2. 映射&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_构建映射&#34;&gt;2.2.1. 构建映射&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_获取和修改映射中的值&#34;&gt;2.2.2. 获取和修改映射中的值&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_元组&#34;&gt;2.3. 元组&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_创建元组&#34;&gt;2.3.1. 创建元组&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_获取元组中的值&#34;&gt;2.3.2. 获取元组中的值&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_将对偶的集合转换成映射&#34;&gt;2.3.3. 将对偶的集合转换成映射&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_拉链操作&#34;&gt;2.3.4. 拉链操作&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_集合&#34;&gt;2.4. 集合&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_序列&#34;&gt;2.4.1. 序列&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_set&#34;&gt;2.5. Set&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_map&#34;&gt;2.6. Map&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_类_对象_继承_特质&#34;&gt;3. 类、对象、继承、特质&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_类&#34;&gt;3.1. 类&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_类的定义&#34;&gt;3.1.1. 类的定义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_构造器&#34;&gt;3.1.2. 构造器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_对象&#34;&gt;3.2. 对象&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_单例对象&#34;&gt;3.2.1. 单例对象&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_伴生对象&#34;&gt;3.2.2. 伴生对象&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_apply方法&#34;&gt;3.2.3. apply方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_应用程序对象&#34;&gt;3.2.4. 应用程序对象&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_继承&#34;&gt;3.3. 继承&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_扩展类&#34;&gt;3.3.1. 扩展类&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_重写方法&#34;&gt;3.3.2. 重写方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_类型检查和转换&#34;&gt;3.3.3. 类型检查和转换&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_超类的构造&#34;&gt;3.3.4. 超类的构造&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_模式匹配和样例类&#34;&gt;4. 模式匹配和样例类&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_匹配字符串&#34;&gt;4.1. 匹配字符串&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_匹配类型&#34;&gt;4.2. 匹配类型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_匹配数组_元组&#34;&gt;4.3. 匹配数组、元组&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_样例类&#34;&gt;4.4. 样例类&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_option类型&#34;&gt;4.5. Option类型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_偏函数&#34;&gt;4.6. 偏函数&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_单机版_wordcount&#34;&gt;5. 单机版 wordcount&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_参考&#34;&gt;6. 参考&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sbt_idea搭建阅读spark源码&#34;&gt;7. SBT+IDEA搭建阅读Spark源码&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_配置sbt&#34;&gt;7.1. 配置SBT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_idea&#34;&gt;7.2. IDEA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_scala基础&#34;&gt;1. Scala基础&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_声明变量&#34;&gt;1.1. 声明变量&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object VariableDemo {
  def main(args: Array[String]) {
    //使用val定义的变量值是不可变的，相当于java里用final修饰的变量
    val i = 1
    //使用var定义的变量是可变得，在Scala中鼓励使用val
    var s = &#34;hello&#34;
    //Scala编译器会自动推断变量的类型，必要的时候可以指定类型
    //变量名在前，类型在后
    val str: String = &#34;itcast&#34;
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_常用类型&#34;&gt;1.2. 常用类型&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;Scala&lt;/strong&gt; 和 &lt;strong&gt;Java&lt;/strong&gt; 一样，有7种数值类型 &lt;strong&gt;Byte&lt;/strong&gt; 、 &lt;strong&gt;Char&lt;/strong&gt; 、 &lt;strong&gt;Short&lt;/strong&gt; 、 &lt;strong&gt;Int&lt;/strong&gt; 、 &lt;strong&gt;Long&lt;/strong&gt; 、 &lt;strong&gt;Float&lt;/strong&gt; 和 &lt;strong&gt;Double&lt;/strong&gt; （无包装类型）和一个 &lt;strong&gt;Boolean&lt;/strong&gt; 类型&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_条件表达式&#34;&gt;1.3. 条件表达式&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala的的条件表达式比较简洁，例如：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ConditionDemo {
  def main(args: Array[String]) {
    val x = 1
    //判断x的值，将结果赋给y
    val y = if (x &amp;gt; 0) 1 else -1
    //打印y的值
    println(y)

    //支持混合类型表达式
    val z = if (x &amp;gt; 1) 1 else &#34;error&#34;
    //打印z的值
    println(z)

    //如果缺失else，相当于if (x &amp;gt; 2) 1 else ()
    val m = if (x &amp;gt; 2) 1
    println(m)

    //在scala中每个表达式都有值，scala中有个Unit类，写做(),相当于Java中的void
    val n = if (x &amp;gt; 2) 1 else ()
    println(n)

    //if和else if
    val k = if (x &amp;lt; 0) 0
    else if (x &amp;gt;= 1) 1 else -1
    println(k)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_块表达式&#34;&gt;1.4. 块表达式&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object BlockExpressionDemo {
  def main(args: Array[String]) {
    val x = 0
    //在scala中{}中课包含一系列表达式，块中最后一个表达式的值就是块的值
    //下面就是一个块表达式
    val result = {
      if (x &amp;lt; 0){
        -1
      } else if(x &amp;gt;= 1) {
        1
      } else {
        &#34;error&#34;
      }
    }
    //result的值就是块表达式的结果
    println(result)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_循环&#34;&gt;1.5. 循环&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在scala中有for循环和while循环，用for循环比较多
for循环语法结构：for (i &amp;#8592; 表达式/数组/集合)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ForDemo {
  def main(args: Array[String]) {
    //for(i &amp;lt;- 表达式),表达式1 to 10返回一个Range（区间）
    //每次循环将区间中的一个值赋给i
    for (i &amp;lt;- 1 to 10)
      println(i)

    //for(i &amp;lt;- 数组)
    val arr = Array(&#34;a&#34;, &#34;b&#34;, &#34;c&#34;)
    for (i &amp;lt;- arr)
      println(i)

    //高级for循环
    //每个生成器都可以带一个条件，注意：if前面没有分号
    for(i &amp;lt;- 1 to 3; j &amp;lt;- 1 to 3 if i != j)
      print((10 * i + j) + &#34; &#34;)
    println()

    //for推导式：如果for循环的循环体以yield开始，则该循环会构建出一个集合
    //每次迭代生成集合中的一个值
    val v = for (i &amp;lt;- 1 to 10) yield i * 10
    println(v)

  }

}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_调用方法和函数&#34;&gt;1.6. 调用方法和函数&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala中的+ - * / %等操作符的作用与Java一样，位操作符 &amp;amp; | ^ &amp;gt;&amp;gt; &amp;lt;&amp;lt;也一样。只是有
一点特别的：这些操作符实际上是方法。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;例如：
　　a + b
是如下方法调用的简写：
　　a. +(b)
a 方法 b可以写成 a.方法(b)&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_定义方法和函数&#34;&gt;1.7. 定义方法和函数&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_定义方法&#34;&gt;1.7.1. 定义方法&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162650.png&#34; alt=&#34;2017 04 04 162650&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;方法的返回值类型可以不写，编译器可以自动推断出来，但是对于递归函数，必须指定返回类型&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_定义函数&#34;&gt;1.7.2. 定义函数&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162701.png&#34; alt=&#34;2017 04 04 162701&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_方法和函数的区别&#34;&gt;1.7.3. 方法和函数的区别&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在函数式编程语言中，函数是“头等公民”，它可以像任何其他数据类型一样被传递和操作&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+
image::/src/img/scala/2017-04-04_162709.png[]
---
案例：首先定义一个方法，再定义一个函数，然后将函数传递到方法里面&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object MethodAndFunctionDemo {
  //定义一个方法
  //方法m2参数要求是一个函数，函数的参数必须是两个Int类型
  //返回值类型也是Int类型
  def m1(f: (Int, Int) =&amp;gt; Int) : Int = {
    f(2, 6)
  }

  //定义一个函数f1，参数是两个Int类型，返回值是一个Int类型
  val f1 = (x: Int, y: Int) =&amp;gt; x + y
  //再定义一个函数f2
  val f2 = (m: Int, n: Int) =&amp;gt; m * n

  //main方法
  def main(args: Array[String]) {

    //调用m1方法，并传入f1函数
    val r1 = m1(f1)
    println(r1)

    //调用m1方法，并传入f2函数
    val r2 = m1(f2)
    println(r2)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_将方法转换成函数_神奇的下划线&#34;&gt;1.7.4. 将方法转换成函数（神奇的下划线）&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162717.png&#34; alt=&#34;2017 04 04 162717&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_数组_映射_元组_集合&#34;&gt;2. 数组、映射、元组、集合&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_数组&#34;&gt;2.1. 数组&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_定长数组和变长数组&#34;&gt;2.1.1. 定长数组和变长数组&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.mutable.ArrayBuffer
object ArrayDemo {

  def main(args: Array[String]) {

    //初始化一个长度为8的定长数组，其所有元素均为0
    val arr1 = new Array[Int](8)
    //直接打印定长数组，内容为数组的hashcode值
    println(arr1)
    //将数组转换成数组缓冲，就可以看到原数组中的内容了
    //toBuffer会将数组转换长数组缓冲
    println(arr1.toBuffer)

    //注意：如果new，相当于调用了数组的apply方法，直接为数组赋值
    //初始化一个长度为1的定长数组
    val arr2 = Array[Int](10)
    println(arr2.toBuffer)

    //定义一个长度为3的定长数组
    val arr3 = Array(&#34;hadoop&#34;, &#34;storm&#34;, &#34;spark&#34;)
    //使用()来访问元素
    println(arr3(2))

    //////////////////////////////////////////////////
    //变长数组（数组缓冲）
    //如果想使用数组缓冲，需要导入import scala.collection.mutable.ArrayBuffer包
    val ab = ArrayBuffer[Int]()
    //向数组缓冲的尾部追加一个元素
    //+=尾部追加元素
    ab += 1
    //追加多个元素
    ab += (2, 3, 4, 5)
    //追加一个数组++=
    ab ++= Array(6, 7)
    //追加一个数组缓冲
    ab ++= ArrayBuffer(8,9)
    //打印数组缓冲ab

    //在数组某个位置插入元素用insert
    ab.insert(0, -1, 0)
    //删除数组某个位置的元素用remove
    ab.remove(8, 2)
    println(ab)

  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_遍历数组&#34;&gt;2.1.2. 遍历数组&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1.增强for循环
2.好用的until会生成脚标，0 until 10 包含0不包含10&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162727.png&#34; alt=&#34;2017 04 04 162727&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ForArrayDemo {

  def main(args: Array[String]) {
    //初始化一个数组
    val arr = Array(1,2,3,4,5,6,7,8)
    //增强for循环
    for(i &amp;lt;- arr)
      println(i)

    //好用的until会生成一个Range
    //reverse是将前面生成的Range反转
    for(i &amp;lt;- (0 until arr.length).reverse)
      println(arr(i))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_数组转换&#34;&gt;2.1.3. 数组转换&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;yield关键字将原始的数组进行转换会产生一个新的数组，原始的数组不变&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162736.png&#34; alt=&#34;2017 04 04 162736&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ArrayYieldDemo {
  def main(args: Array[String]) {
    //定义一个数组
    val arr = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)
    //将偶数取出乘以10后再生成一个新的数组
    val res = for (e &amp;lt;- arr if e % 2 == 0) yield e * 10
    println(res.toBuffer)

    //更高级的写法,用着更爽
    //filter是过滤，接收一个返回值为boolean的函数
    //map相当于将数组中的每一个元素取出来，应用传进去的函数
    val r = arr.filter(_ % 2 == 0).map(_ * 10)
    println(r.toBuffer)

  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_数组常用算法&#34;&gt;2.1.4. 数组常用算法&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中，数组上的某些方法对数组进行相应的操作非常方便！&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162743.png&#34; alt=&#34;2017 04 04 162743&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_映射&#34;&gt;2.2. 映射&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中，把哈希表这种数据结构叫做映射&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_构建映射&#34;&gt;2.2.1. 构建映射&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162749.png&#34; alt=&#34;2017 04 04 162749&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_获取和修改映射中的值&#34;&gt;2.2.2. 获取和修改映射中的值&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162755.png&#34; alt=&#34;2017 04 04 162755&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;好用的getOrElse&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162802.png&#34; alt=&#34;2017 04 04 162802&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注意：在Scala中，有两种Map，一个是immutable包下的Map，该Map中的内容不可变；另一个是mutable包下的Map，该Map中的内容可变
例子：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162812.png&#34; alt=&#34;2017 04 04 162812&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注意：通常我们在创建一个集合是会用val这个关键字修饰一个变量（相当于java中的final），那么就意味着该变量的引用不可变，该引用中的内容是不是可变，取决于这个引用指向的集合的类型&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_元组&#34;&gt;2.3. 元组&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;映射是K/V对偶的集合，对偶是元组的最简单形式，元组可以装着多个不同类型的值。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_创建元组&#34;&gt;2.3.1. 创建元组&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162819.png&#34; alt=&#34;2017 04 04 162819&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_获取元组中的值&#34;&gt;2.3.2. 获取元组中的值&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162828.png&#34; alt=&#34;2017 04 04 162828&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_将对偶的集合转换成映射&#34;&gt;2.3.3. 将对偶的集合转换成映射&lt;/h4&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162835.png&#34; alt=&#34;2017 04 04 162835&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_拉链操作&#34;&gt;2.3.4. 拉链操作&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;zip命令可以将多个值绑定在一起&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/scala/2017-04-04_162841.png&#34; alt=&#34;2017 04 04 162841&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注意：如果两个数组的元素个数不一致，拉链操作后生成的数组的长度为较小的那个数组的元素个数&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_集合&#34;&gt;2.4. 集合&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Scala的集合有三大类&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;序列Seq、集Set、映射Map，所有的集合都扩展自Iterable特质
在Scala中集合有可变（mutable）和不可变（immutable）两种类型，immutable类型的集合初始化后就不能改变了（注意与val修饰的变量进行区别）&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_序列&#34;&gt;2.4.1. 序列&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;不可变的序列 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;code&gt;import scala.collection.immutable._&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;在Scala中列表要么为空（Nil表示空列表）要么是一个head元素加上一个tail列表。
9 :: List(5, 2)  :: 操作符是将给定的头和尾创建一个新的列表
注意：:: 操作符是右结合的，如9 :: 5 :: 2 :: Nil相当于 9 :: (5 :: (2 :: Nil))&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ImmutListDemo {

  def main(args: Array[String]) {
    //创建一个不可变的集合
    val lst1 = List(1,2,3)
    //将0插入到lst1的前面生成一个新的List
    val lst2 = 0 :: lst1
    val lst3 = lst1.::(0)
    val lst4 = 0 +: lst1
    val lst5 = lst1.+:(0)

    //将一个元素添加到lst1的后面产生一个新的集合
    val lst6 = lst1 :+ 3

    val lst0 = List(4,5,6)
    //将2个list合并成一个新的List
    val lst7 = lst1 ++ lst0
    //将lst1插入到lst0前面生成一个新的集合
    val lst8 = lst1 ++: lst0

    //将lst0插入到lst1前面生成一个新的集合
    val lst9 = lst1.:::(lst0)

    println(lst9)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;可变的序列 &lt;code&gt;import scala.collection.mutable._&lt;/code&gt; &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.mutable.ListBuffer

object MutListDemo extends App{
  //构建一个可变列表，初始有3个元素1,2,3
  val lst0 = ListBuffer[Int](1,2,3)
  //创建一个空的可变列表
  val lst1 = new ListBuffer[Int]
  //向lst1中追加元素，注意：没有生成新的集合
  lst1 += 4
  lst1.append(5)

  //将lst1中的元素最近到lst0中， 注意：没有生成新的集合
  lst0 ++= lst1

  //将lst0和lst1合并成一个新的ListBuffer 注意：生成了一个集合
  val lst2= lst0 ++ lst1

  //将元素追加到lst0的后面生成一个新的集合
  val lst3 = lst0 :+ 5
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_set&#34;&gt;2.5. Set&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;不可变的Set&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.immutable.HashSet

object ImmutSetDemo extends App{
  val set1 = new HashSet[Int]()
  //将元素和set1合并生成一个新的set，原有set不变
  val set2 = set1 + 4
  //set中元素不能重复
  val set3 = set1 ++ Set(5, 6, 7)
  val set0 = Set(1,3,4) ++ set1
  println(set0.getClass)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;可变的Set&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.mutable

object MutSetDemo extends App{
  //创建一个可变的HashSet
  val set1 = new mutable.HashSet[Int]()
  //向HashSet中添加元素
  set1 += 2
  //add等价于+=
  set1.add(4)
  set1 ++= Set(1,3,5)
  println(set1)
  //删除一个元素
  set1 -= 5
  set1.remove(2)
  println(set1)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_map&#34;&gt;2.6. Map&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.mutable

object MutMapDemo extends App{
  val map1 = new mutable.HashMap[String, Int]()
  //向map中添加数据
  map1(&#34;spark&#34;) = 1
  map1 += ((&#34;hadoop&#34;, 2))
  map1.put(&#34;storm&#34;, 3)
  println(map1)

  //从map中移除元素
  map1 -= &#34;spark&#34;
  map1.remove(&#34;hadoop&#34;)
  println(map1)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_类_对象_继承_特质&#34;&gt;3. 类、对象、继承、特质&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala的类与Java、C++的类比起来更简洁，学完之后你会更爱Scala！！！&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_类&#34;&gt;3.1. 类&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_类的定义&#34;&gt;3.1.1. 类的定义&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;//在Scala中，类并不用声明为public。
//Scala源文件中可以包含多个类，所有这些类都具有公有可见性。
class Person {
  //用val修饰的变量是只读属性，有getter但没有setter
  //（相当与Java中用final修饰的变量）
  val id = &#34;9527&#34;

  //用var修饰的变量既有getter又有setter
  var age: Int = 18

  //类私有字段,只能在类的内部使用
  private var name: String = &#34;唐伯虎&#34;

  //对象私有字段,访问权限更加严格的，Person类的方法只能访问到当前对象的字段
  private[this] val pet = &#34;小强&#34;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_构造器&#34;&gt;3.1.2. 构造器&lt;/h4&gt;
&lt;div class=&#34;admonitionblock important&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-important&#34; title=&#34;Important&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
主构造器会执行类定义中的所有语句
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/**
  *每个类都有主构造器，主构造器的参数直接放置类名后面，与类交织在一起
  */
class Student(val name: String, val age: Int){
  //主构造器会执行类定义中的所有语句
  println(&#34;执行主构造器&#34;)

  try {
    println(&#34;读取文件&#34;)
    throw new IOException(&#34;io exception&#34;)
  } catch {
    case e: NullPointerException =&amp;gt; println(&#34;打印异常Exception : &#34; + e)
    case e: IOException =&amp;gt; println(&#34;打印异常Exception : &#34; + e)
  } finally {
    println(&#34;执行finally部分&#34;)
  }

  private var gender = &#34;male&#34;

  //用this关键字定义辅助构造器
  def this(name: String, age: Int, gender: String){
    //每个辅助构造器必须以主构造器或其他的辅助构造器的调用开始
    this(name, age)
    println(&#34;执行辅助构造器&#34;)
    this.gender = gender
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/**
  *构造器参数可以不带val或var，如果不带val或var的参数至少被一个方法所使用，
  *那么它将会被提升为字段
  */
//在类名后面加private就变成了私有的
class Queen private(val name: String, prop: Array[String], private var age: Int = 18){

  println(prop.size)

  //prop被下面的方法使用后，prop就变成了不可变得对象私有字段，等同于private[this] val prop
  //如果没有被方法使用该参数将不被保存为字段，仅仅是一个可以被主构造器中的代码访问的普通参数
  def description = name + &#34; is &#34; + age + &#34; years old with &#34; + prop.toBuffer
}

object Queen{
  def main(args: Array[String]) {
    //私有的构造器，只有在其伴生对象中使用
    val q = new Queen(&#34;hatano&#34;, Array(&#34;蜡烛&#34;, &#34;皮鞭&#34;), 20)
    println(q.description())
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_对象&#34;&gt;3.2. 对象&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_单例对象&#34;&gt;3.2.1. 单例对象&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中没有静态方法和静态字段，但是可以使用object这个语法结构来达到同样的目的&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;存放工具方法和常量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;高效共享单个不可变的实例&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;单例模式&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.collection.mutable.ArrayBuffer

object SingletonDemo {
  def main(args: Array[String]) {
    //单例对象，不需要new，用【类名.方法】调用对象中的方法
    val session = SessionFactory.getSession()
    println(session)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object SessionFactory{
  //该部分相当于java中的静态块
  var counts = 5
  val sessions = new ArrayBuffer[Session]()
  while(counts &amp;gt; 0){
    sessions += new Session
    counts -= 1
  }

  //在object中的方法相当于java中的静态方法
  def getSession(): Session ={
    sessions.remove(0)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class Session{

}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_伴生对象&#34;&gt;3.2.2. 伴生对象&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala的类中，与类名相同的对象叫做伴生对象，类和伴生对象之间可以相互访问私有的方法和属性&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class Dog {
  val id = 1
  private var name = &#34;itcast&#34;

  def printName(): Unit ={
    //在Dog类中可以访问伴生对象Dog的私有属性
    println(Dog.CONSTANT + name )
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;/**
  * 伴生对象
  */
object Dog {

  //伴生对象中的私有属性
  private val CONSTANT = &#34;汪汪汪 : &#34;

  def main(args: Array[String]) {
    val p = new Dog
    //访问私有的字段name
    p.name = &#34;123&#34;
    p.printName()
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_apply方法&#34;&gt;3.2.3. apply方法&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;通常我们会在类的伴生对象中定义apply方法，当遇到类名(参数1,&amp;#8230;&amp;#8203;参数n)时apply方法会被调用&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ApplyDemo {
  def main(args: Array[String]) {
    //调用了Array伴生对象的apply方法
    //def apply(x: Int, xs: Int*): Array[Int]
    //arr1中只有一个元素5
    val arr1 = Array(5)
    println(arr1.toBuffer)

    //new了一个长度为5的array，数组里面包含5个null
    var arr2 = new Array(5)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_应用程序对象&#34;&gt;3.2.4. 应用程序对象&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala程序都必须从一个对象的main方法开始，可以通过扩展App特质，不写main方法。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object AppObjectDemo extends App{
  //不用写main方法
  println(&#34;I love you Scala&#34;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_继承&#34;&gt;3.3. 继承&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_扩展类&#34;&gt;3.3.1. 扩展类&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中扩展类的方式和Java一样都是使用extends关键字&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_重写方法&#34;&gt;3.3.2. 重写方法&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中重写一个非抽象的方法必须使用override修饰符&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_类型检查和转换&#34;&gt;3.3.3. 类型检查和转换&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;Scala
Java
obj.isInstanceOf[C]
obj instanceof C
obj.asInstanceOf[C]
(C)obj
classOf[C]
C.class&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_超类的构造&#34;&gt;3.3.4. 超类的构造&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object ClazzDemo {
  def main(args: Array[String]) {
    //val h = new Human
    //println(h.fight)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;trait Flyable{
  def fly(): Unit ={
    println(&#34;I can fly&#34;)
  }

  def fight(): String
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;abstract class Animal {
  def run(): Int
  val name: String
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;class Human extends Animal with Flyable{

  val name = &#34;abc&#34;

  //打印几次&#34;ABC&#34;?
  val t1,t2,(a, b, c) = {
    println(&#34;ABC&#34;)
    (1,2,3)
  }

  println(a)
  println(t1._1)

  //在Scala中重写一个非抽象方法必须用override修饰
  override def fight(): String = {
    &#34;fight with 棒子&#34;
  }
  //在子类中重写超类的抽象方法时，不需要使用override关键字，写了也可以
  def run(): Int = {
    1
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_模式匹配和样例类&#34;&gt;4. 模式匹配和样例类&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Scala有一个十分强大的模式匹配机制，可以应用到很多场合：如switch语句、类型检查等。
并且Scala还提供了样例类，对模式匹配进行了优化，可以快速进行匹配&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_匹配字符串&#34;&gt;4.1. 匹配字符串&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.util.Random

object CaseDemo01 extends App{
  val arr = Array(&#34;YoshizawaAkiho&#34;, &#34;YuiHatano&#34;, &#34;AoiSola&#34;)
  val name = arr(Random.nextInt(arr.length))
  name match {
    case &#34;YoshizawaAkiho&#34; =&amp;gt; println(&#34;吉泽老师...&#34;)
    case &#34;YuiHatano&#34; =&amp;gt; println(&#34;波多老师...&#34;)
    case _ =&amp;gt; println(&#34;真不知道你们在说什么...&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_匹配类型&#34;&gt;4.2. 匹配类型&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.util.Random

object CaseDemo01 extends App{
  //val v = if(x &amp;gt;= 5) 1 else if(x &amp;lt; 2) 2.0 else &#34;hello&#34;
  val arr = Array(&#34;hello&#34;, 1, 2.0, CaseDemo)
  val v = arr(Random.nextInt(4))
  println(v)
  v match {
    case x: Int =&amp;gt; println(&#34;Int &#34; + x)
    case y: Double if(y &amp;gt;= 0) =&amp;gt; println(&#34;Double &#34;+ y)
    case z: String =&amp;gt; println(&#34;String &#34; + z)
    case _ =&amp;gt; throw new Exception(&#34;not match exception&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注意：case y: Double if(y &amp;gt;= 0) &amp;#8658; &amp;#8230;&amp;#8203;
模式匹配的时候还可以添加守卫条件。如不符合守卫条件，将掉入case _中&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_匹配数组_元组&#34;&gt;4.3. 匹配数组、元组&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object CaseDemo03 extends App{

  val arr = Array(1, 3, 5)
  arr match {
    case Array(1, x, y) =&amp;gt; println(x + &#34; &#34; + y)
    case Array(0) =&amp;gt; println(&#34;only 0&#34;)
    case Array(0, _*) =&amp;gt; println(&#34;0 ...&#34;)
    case _ =&amp;gt; println(&#34;something else&#34;)
  }

  val lst = List(3, -1)
  lst match {
    case 0 :: Nil =&amp;gt; println(&#34;only 0&#34;)
    case x :: y :: Nil =&amp;gt; println(s&#34;x: $x y: $y&#34;)
    case 0 :: tail =&amp;gt; println(&#34;0 ...&#34;)
    case _ =&amp;gt; println(&#34;something else&#34;)
  }

  val tup = (2, 3, 7)
  tup match {
    case (1, x, y) =&amp;gt; println(s&#34;1, $x , $y&#34;)
    case (_, z, 5) =&amp;gt; println(z)
    case  _ =&amp;gt; println(&#34;else&#34;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;admonitionblock important&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-important&#34; title=&#34;Important&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
在Scala中列表要么为空（Nil表示空列表）要么是一个head元素加上一个tail列表。&lt;br&gt;
9 :: List(5, 2)  :: 操作符是将给定的头和尾创建一个新的列表&lt;br&gt;
:: 操作符是右结合的，如9 :: 5 :: 2 :: Nil相当于 9 :: (5 :: (2 :: Nil))
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_样例类&#34;&gt;4.4. 样例类&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中样例类是一中特殊的类，可用于模式匹配。case class是多例的，后面要跟构造参数，case object是单例的&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;import scala.util.Random

case class SubmitTask(id: String, name: String)
case class HeartBeat(time: Long)
case object CheckTimeOutTask

object CaseDemo04 extends App{
  val arr = Array(CheckTimeOutTask, HeartBeat(12333), SubmitTask(&#34;0001&#34;, &#34;task-0001&#34;))

  arr(Random.nextInt(arr.length)) match {
    case SubmitTask(id, name) =&amp;gt; {
      println(s&#34;$id, $name&#34;)//前面需要加上s, $id直接取id的值
    }
    case HeartBeat(time) =&amp;gt; {
      println(time)
    }
    case CheckTimeOutTask =&amp;gt; {
      println(&#34;check&#34;)
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_option类型&#34;&gt;4.5. Option类型&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在Scala中Option类型样例类用来表示可能存在或也可能不存在的值(Option的子类有Some和None)。Some包装了某个值，None表示没有值&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object OptionDemo {
  def main(args: Array[String]) {
    val map = Map(&#34;a&#34; -&amp;gt; 1, &#34;b&#34; -&amp;gt; 2)
    val v = map.get(&#34;b&#34;) match {
      case Some(i) =&amp;gt; i
      case None =&amp;gt; 0
    }
    println(v)
    //更好的方式
    val v1 = map.getOrElse(&#34;c&#34;, 0)
    println(v1)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_偏函数&#34;&gt;4.6. 偏函数&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;被包在花括号内没有match的一组case语句是一个偏函数，它是PartialFunction[A, B]的一个实例，A代表参数类型，B代表返回类型，常用作输入模式匹配&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;object PartialFuncDemo  {

  def func1: PartialFunction[String, Int] = {
    case &#34;one&#34; =&amp;gt; 1
    case &#34;two&#34; =&amp;gt; 2
    case _ =&amp;gt; -1
  }

  def func2(num: String) : Int = num match {
    case &#34;one&#34; =&amp;gt; 1
    case &#34;two&#34; =&amp;gt; 2
    case _ =&amp;gt; -1
  }

  def main(args: Array[String]) {
    println(func1(&#34;one&#34;))
    println(func2(&#34;one&#34;))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_单机版_wordcount&#34;&gt;5. 单机版 wordcount&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;val lines = List(&#34;hello tom hello jerry&#34;,&#34;hello tom kitty hello hello&#34;)

lines.flatMap(_.split(&#34; &#34;)) &lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
    .map((_,1)) &lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
    .groupBy(_._1) &lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
    .map(t =&amp;gt; (t._1,t._2.size)) &lt;i class=&#34;conum&#34; data-value=&#34;4&#34;&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
    .toList.sortBy(_._2) &lt;i class=&#34;conum&#34; data-value=&#34;5&#34;&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;colist arabic&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;res2: List[String] = List(hello, tom, hello, jerry, hello, tom, kitty, hello, hello)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;res4: List[(String, Int)] = Listhello,1), (tom,1), (hello,1), (jerry,1), (hello,1), (tom,1), (kitty,1), (hello,1), (hello,1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;res5: scala.collection.immutable.Map[String,List[(String, Int)]] = Map(tom &amp;#8594; Listtom,1), (tom,1, kitty &amp;#8594; Listkitty,1, jerry &amp;#8594; Listjerry,1, hello &amp;#8594; Listhello,1), (hello,1), (hello,1), (hello,1), (hello,1)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;4&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;res6: scala.collection.immutable.Map[String,Int] = Map(tom &amp;#8594; 2, kitty &amp;#8594; 1, jerry &amp;#8594; 1, hello &amp;#8594; 5)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;5&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;res7: List[(String, Int)] = Listkitty,1), (jerry,1), (tom,2), (hello,5&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt; lines.flatMap(_.split(&#34; &#34;)).map((_,1)).groupBy(_._1).mapValues(_.foldLeft(0)(_+_._2))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_参考&#34;&gt;6. 参考&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://hongjiang.info/scala/&#34; class=&#34;bare&#34;&gt;http://hongjiang.info/scala/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.scalatest.org/quick_start&#34;&gt;Scala单元测试&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://guozhongxin.com/pages/2014/10/15/spark_source_code.html&#34; class=&#34;bare&#34;&gt;http://guozhongxin.com/pages/2014/10/15/spark_source_code.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_sbt_idea搭建阅读spark源码&#34;&gt;7. SBT+IDEA搭建阅读Spark源码&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_配置sbt&#34;&gt;7.1. 配置SBT&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;下载+
&lt;a href=&#34;http://www.scala-sbt.org/download.html&#34;&gt;sbt-0.13.15&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;windows下配置环境变量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;阿里云仓库配置文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ ~/.sbt/repositories
[repositories]
  public: http://maven.aliyun.com/nexus/content/groups/public/
  typesafe:http://dl.bintray.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly
  ivy-sbt-plugin:http://dl.bintray.com/sbt/sbt-plugin-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]
  sonatype-oss-releases

  sonatype-oss-snapshots&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;指定仓库&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;sbt -Dsbt.override.build.repos=true -Dsbt.repository.config=C:\Users\dishui\.sbt\repositories&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_idea&#34;&gt;7.2. IDEA&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-05-04_112358.png&#34; alt=&#34;2017 05 04 112358&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/spark/2017-05-04_112513.png&#34; alt=&#34;2017 05 04 112513&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;-XX:MaxPermSize=384M
-Dsbt.override.build.repos=true
-Dsbt.repository.config=C:\Users\dishui\.sbt\repositories&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Kafka负载均衡-自定义Partition-文件存储机制</title>
      <link>/post/bigdata/storm/Kafka%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-%E8%87%AA%E5%AE%9A%E4%B9%89Partition-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/</link>
      <pubDate>Sat, 01 Apr 2017 09:31:48 +0000</pubDate>
      
      <guid>/post/bigdata/storm/Kafka%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-%E8%87%AA%E5%AE%9A%E4%B9%89Partition-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;kafka&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka整体结构图&#34;&gt;1. Kafka整体结构图&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_consumer与topic关系&#34;&gt;2. Consumer与topic关系&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka消息的分发&#34;&gt;3. Kafka消息的分发&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_consumer的负载均衡&#34;&gt;4. Consumer的负载均衡&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka文件存储机制&#34;&gt;5. kafka文件存储机制&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka文件存储基本结构&#34;&gt;5.1. Kafka文件存储基本结构&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka_partition_segment&#34;&gt;5.2. Kafka Partition Segment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka_查找message&#34;&gt;5.3. Kafka 查找message&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_查找segment_file&#34;&gt;5.3.1. 查找segment file&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka自定义partition&#34;&gt;6. Kafka自定义Partition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka整体结构图&#34;&gt;1. Kafka整体结构图&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka名词解释和工作方式&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Producer &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;消息生产者，就是向kafka broker发消息的客户端。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Consumer &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;消息消费者，向kafka broker取消息的客户端&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Topic &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;咋们可以理解为一个队列。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Consumer Group （CG）&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;这是 &lt;strong&gt;kafka&lt;/strong&gt; 用来实现一个 &lt;strong&gt;topic&lt;/strong&gt; 消息的广播（发给所有的 &lt;strong&gt;consumer&lt;/strong&gt; ）和单播（发给任意一个 &lt;strong&gt;consumer&lt;/strong&gt; ）的手段。一个 &lt;strong&gt;topic&lt;/strong&gt; 可以有多个 &lt;strong&gt;CG&lt;/strong&gt; 。&lt;strong&gt;topic&lt;/strong&gt; 的消息会复制（不是真的复制，是概念上的）到所有的 &lt;strong&gt;CG&lt;/strong&gt; ，但每个 &lt;strong&gt;partion&lt;/strong&gt; 只会把消息发给该 &lt;strong&gt;CG&lt;/strong&gt; 中的一个 &lt;strong&gt;consumer&lt;/strong&gt; 。如果需要实现广播，只要每个 &lt;strong&gt;consumer&lt;/strong&gt; 有一个独立的 &lt;strong&gt;CG&lt;/strong&gt; 就可以了。要实现单播只要所有的 &lt;strong&gt;consumer&lt;/strong&gt; 在同一个 &lt;strong&gt;CG&lt;/strong&gt; 。用 &lt;strong&gt;CG&lt;/strong&gt; 还可以将 &lt;strong&gt;consumer&lt;/strong&gt; 进行自由的分组而不需要多次发送消息到不同的 &lt;strong&gt;topic&lt;/strong&gt; 。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Broker &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;一台 &lt;strong&gt;kafka&lt;/strong&gt; 服务器就是一个 &lt;strong&gt;broker&lt;/strong&gt; 。一个集群由多个 &lt;strong&gt;broker&lt;/strong&gt; 组成。一个 &lt;strong&gt;broker&lt;/strong&gt; 可以容纳多个 &lt;strong&gt;topic&lt;/strong&gt; 。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Partition&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;为了实现扩展性，一个非常大的 &lt;strong&gt;topic&lt;/strong&gt; 可以分布到多个 &lt;strong&gt;broker&lt;/strong&gt; （即服务器）上，一个 &lt;strong&gt;topic&lt;/strong&gt; 可以分为多个 &lt;strong&gt;partition&lt;/strong&gt; ，每个 &lt;strong&gt;partition&lt;/strong&gt; 是一个有序的队列。 &lt;strong&gt;partition&lt;/strong&gt; 中的每条消息都会被分配一个有序的 &lt;strong&gt;id&lt;/strong&gt; （ &lt;strong&gt;offset&lt;/strong&gt; ）。 &lt;strong&gt;kafka&lt;/strong&gt; 只保证按一个 &lt;strong&gt;partition&lt;/strong&gt; 中的顺序将消息发给 &lt;strong&gt;consumer&lt;/strong&gt; ，不保证一个 &lt;strong&gt;topic&lt;/strong&gt; 的整体（多个 &lt;strong&gt;partition&lt;/strong&gt; 间）的顺序。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Offset&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;strong&gt;kafka&lt;/strong&gt; 的存储文件都是按照 &lt;strong&gt;offset.kafka&lt;/strong&gt; 来命名，用 &lt;strong&gt;offset&lt;/strong&gt; 做名字的好处是方便查找。例如你想找位于2049的位置，只要找到 &lt;strong&gt;2048.kafka&lt;/strong&gt; 的文件即可。当然 &lt;strong&gt;the first offset&lt;/strong&gt; 就是 &lt;strong&gt;00000000000.kafka&lt;/strong&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_consumer与topic关系&#34;&gt;2. Consumer与topic关系&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;本质上 &lt;strong&gt;kafka&lt;/strong&gt; 只支持 &lt;strong&gt;Topic&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;每个 &lt;strong&gt;group&lt;/strong&gt; 中可以有多个 &lt;strong&gt;consumer&lt;/strong&gt; ，每个 &lt;strong&gt;consumer&lt;/strong&gt; 属于一个 &lt;strong&gt;consumer&lt;/strong&gt;   &lt;strong&gt;group&lt;/strong&gt; ；
　　通常情况下，一个 &lt;strong&gt;group&lt;/strong&gt; 中会包含多个 &lt;strong&gt;consumer&lt;/strong&gt; ，这样不仅可以提高 &lt;strong&gt;topic&lt;/strong&gt; 中消息的并发消费能力，而且还能提高&#34;故障容错&#34;性，如果 &lt;strong&gt;group&lt;/strong&gt; 中的某个 &lt;strong&gt;consumer&lt;/strong&gt; 失效那么其消费的 &lt;strong&gt;partitions&lt;/strong&gt; 将会有其他 &lt;strong&gt;consumer&lt;/strong&gt; 自动接管。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于 &lt;strong&gt;Topic&lt;/strong&gt; 中的一条特定的消息，只会被订阅此 &lt;strong&gt;Topic&lt;/strong&gt; 的每个 &lt;strong&gt;group&lt;/strong&gt; 中的其中一个 &lt;strong&gt;consumer&lt;/strong&gt; 消费，此消息不会发送给一个 &lt;strong&gt;group&lt;/strong&gt; 的多个 &lt;strong&gt;consumer&lt;/strong&gt; ；
　　那么一个 &lt;strong&gt;group&lt;/strong&gt; 中所有的 &lt;strong&gt;consumer&lt;/strong&gt; 将会交错的消费整个 &lt;strong&gt;Topic&lt;/strong&gt; ，每个 &lt;strong&gt;group&lt;/strong&gt; 中 &lt;strong&gt;consumer&lt;/strong&gt; 消息消费互相独立，我们可以认为一个 &lt;strong&gt;group&lt;/strong&gt; 是一个&#34;订阅&#34;者。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;kafka&lt;/strong&gt; 中,一个 &lt;strong&gt;partition&lt;/strong&gt; 中的消息只会被 &lt;strong&gt;group&lt;/strong&gt; 中的一个 &lt;strong&gt;consumer&lt;/strong&gt; 消费(同一时刻)；
一个 &lt;strong&gt;Topic&lt;/strong&gt; 中的每个 &lt;strong&gt;partions&lt;/strong&gt; ，只会被一个&#34;订阅者&#34;中的一个 &lt;strong&gt;consumer&lt;/strong&gt; 消费，不过一个 &lt;strong&gt;consumer&lt;/strong&gt; 可以同时消费多个 &lt;strong&gt;partitions&lt;/strong&gt; 中的消息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;kafka&lt;/strong&gt; 的设计原理决定,对于一个 &lt;strong&gt;topic&lt;/strong&gt; ，同一个 &lt;strong&gt;group&lt;/strong&gt; 中不能有多于 &lt;strong&gt;partitions&lt;/strong&gt; 个数的 &lt;strong&gt;consumer&lt;/strong&gt; 同时消费，否则将意味着某些 &lt;strong&gt;consumer&lt;/strong&gt; 将无法得到消息。
　　 &lt;strong&gt;kafka&lt;/strong&gt; 只能保证一个 &lt;strong&gt;partition&lt;/strong&gt; 中的消息被某个 &lt;strong&gt;consumer&lt;/strong&gt; 消费时是顺序的；事实上，从 &lt;strong&gt;Topic&lt;/strong&gt; 角度来说,当有多个 &lt;strong&gt;partitions&lt;/strong&gt; 时,消息仍不是全局有序的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka消息的分发&#34;&gt;3. Kafka消息的分发&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;Producer&lt;/strong&gt; 客户端负责消息的分发&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;kafka&lt;/strong&gt; 集群中的任何一个 &lt;strong&gt;broker&lt;/strong&gt; 都可以向 &lt;strong&gt;producer&lt;/strong&gt; 提供 &lt;strong&gt;metadata&lt;/strong&gt; 信息,这些 &lt;strong&gt;metadata&lt;/strong&gt; 中包含&#34;集群中存活的 &lt;strong&gt;servers&lt;/strong&gt; 列表 &lt;strong&gt;&#34;/&#34;partitions leader&lt;/strong&gt; 列表&#34;等信息；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当 &lt;strong&gt;producer&lt;/strong&gt; 获取到 &lt;strong&gt;metadata&lt;/strong&gt; 信息之后,  &lt;strong&gt;producer&lt;/strong&gt; 将会和 &lt;strong&gt;Topic&lt;/strong&gt; 下所有 &lt;strong&gt;partition&lt;/strong&gt;   &lt;strong&gt;leader&lt;/strong&gt; 保持 &lt;strong&gt;socket&lt;/strong&gt; 连接；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;消息由 &lt;strong&gt;producer&lt;/strong&gt; 直接通过 &lt;strong&gt;socket&lt;/strong&gt; 发送到 &lt;strong&gt;broker&lt;/strong&gt; ，中间不会经过任何&#34;路由层&#34;，事实上，消息被路由到哪个 &lt;strong&gt;partition&lt;/strong&gt; 上由 &lt;strong&gt;producer&lt;/strong&gt; 客户端决定；
　　比如可以采用&#34; &lt;strong&gt;random&lt;/strong&gt; &#34;&#34; &lt;strong&gt;key&lt;/strong&gt; - &lt;strong&gt;hash&lt;/strong&gt; &#34;&#34;轮询&#34;等,如果一个 &lt;strong&gt;topic&lt;/strong&gt; 中有多个 &lt;strong&gt;partitions&lt;/strong&gt; ,那么在 &lt;strong&gt;producer&lt;/strong&gt; 端实现&#34;消息均衡分发&#34;是必要的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;producer&lt;/strong&gt; 端的配置文件中,开发者可以指定 &lt;strong&gt;partition&lt;/strong&gt; 路由的方式。&lt;/p&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　Producer消息发送的应答机制
　　设置发送数据是否需要服务端的反馈,有三个值0,1,-1
　　0: producer不会等待broker发送ack
　　1: 当leader接收到消息之后发送ack
　　-1: 当所有的follower都同步消息成功后发送ack
    request.required.acks=0&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_consumer的负载均衡&#34;&gt;4. Consumer的负载均衡&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;当一个 &lt;strong&gt;group&lt;/strong&gt; 中,有 &lt;strong&gt;consumer&lt;/strong&gt; 加入或者离开时,会触发 &lt;strong&gt;partitions&lt;/strong&gt; 均衡.均衡的最终目的,是提升 &lt;strong&gt;topic&lt;/strong&gt; 的并发消费能力，步骤如下：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;假如 &lt;strong&gt;topic1&lt;/strong&gt; ,具有如下 &lt;strong&gt;partitions: P0,P1,P2,P3&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加入 &lt;strong&gt;group&lt;/strong&gt; 中,有如下 &lt;strong&gt;consumer: C1,C2&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;首先根据 &lt;strong&gt;partition&lt;/strong&gt; 索引号对 &lt;strong&gt;partitions&lt;/strong&gt; 排序:  &lt;strong&gt;P0,P1,P2,P3&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据 &lt;strong&gt;consumer.id&lt;/strong&gt; 排序:  &lt;strong&gt;C0,C1&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算倍数:  &lt;strong&gt;M = [P0,P1,P2,P3].size / [C0,C1].size&lt;/strong&gt; ,本例值 &lt;strong&gt;M=2&lt;/strong&gt; (向上取整)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;然后依次分配 &lt;strong&gt;partitions&lt;/strong&gt; :  &lt;strong&gt;C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104025.png&#34; alt=&#34;2017 04 01 104025&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka文件存储机制&#34;&gt;5. kafka文件存储机制&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka文件存储基本结构&#34;&gt;5.1. Kafka文件存储基本结构&lt;/h3&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;Kafka&lt;/strong&gt; 文件存储中，同一个 &lt;strong&gt;topic&lt;/strong&gt; 下有多个不同 &lt;strong&gt;partition&lt;/strong&gt; ，每个 &lt;strong&gt;partition&lt;/strong&gt; 为一个目录， &lt;strong&gt;partiton&lt;/strong&gt; 命名规则为 &lt;strong&gt;topic&lt;/strong&gt; 名称+有序序号，第一个 &lt;strong&gt;partiton&lt;/strong&gt; 序号从0开始，序号最大值为 &lt;strong&gt;partitions&lt;/strong&gt; 数量减1。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个 &lt;strong&gt;partion&lt;/strong&gt; (目录)相当于一个巨型文件被平均分配到多个大小相等 &lt;strong&gt;segment&lt;/strong&gt; (段)数据文件中。但每个段 &lt;strong&gt;segment file&lt;/strong&gt; 消息数量不一定相等，这种特性方便 &lt;strong&gt;old&lt;/strong&gt;   &lt;strong&gt;segment&lt;/strong&gt;   &lt;strong&gt;file&lt;/strong&gt; 快速被删除。默认保留7天的数据。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104135.png&#34; alt=&#34;2017 04 01 104135&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个 &lt;strong&gt;partiton&lt;/strong&gt; 只需要支持顺序读写就行了， &lt;strong&gt;segment&lt;/strong&gt; 文件生命周期由服务端配置参数决定。（什么时候创建，什么时候删除）&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104143.png&#34; alt=&#34;2017 04 01 104143&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;数据有序的讨论？&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;一个 &lt;strong&gt;partition&lt;/strong&gt; 的数据是否是有序的？&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;间隔性有序，不连续
针对一个 &lt;strong&gt;topic&lt;/strong&gt; 里面的数据，只能做到 &lt;strong&gt;partition&lt;/strong&gt; 内部有序，不能做到全局有序。
特别加入消费者的场景后，如何保证消费者消费的数据全局有序的？伪命题。
只有一种情况下才能保证全局有序？就是只有一个 &lt;strong&gt;partition&lt;/strong&gt; 。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka_partition_segment&#34;&gt;5.2. Kafka Partition Segment&lt;/h3&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Segment file&lt;/strong&gt; 组成：由2大部分组成，分别为 &lt;strong&gt;index&lt;/strong&gt;   &lt;strong&gt;file&lt;/strong&gt; 和 &lt;strong&gt;data&lt;/strong&gt;   &lt;strong&gt;file&lt;/strong&gt; ，此2个文件一一对应，成对出现，后缀 &lt;strong&gt;.index&lt;/strong&gt; 和 &lt;strong&gt;.log&lt;/strong&gt; 分别表示为 &lt;strong&gt;segment&lt;/strong&gt; 索引文件、数据文件。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104152.png&#34; alt=&#34;2017 04 01 104152&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Segment&lt;/strong&gt; 文件命名规则： &lt;strong&gt;partion&lt;/strong&gt; 全局的第一个 &lt;strong&gt;segment&lt;/strong&gt; 从0开始，后续每个 &lt;strong&gt;segment&lt;/strong&gt; 文件名为上一个 &lt;strong&gt;segment&lt;/strong&gt; 文件最后一条消息的 &lt;strong&gt;offset&lt;/strong&gt; 值。数值最大为64位 &lt;strong&gt;long&lt;/strong&gt; 大小，19位数字字符长度，没有数字用0填充。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中 &lt;strong&gt;message&lt;/strong&gt; 的物理偏移地址。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104159.png&#34; alt=&#34;2017 04 01 104159&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;3，497：当前log文件中的第几条信息，存放在磁盘上的那个地方&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;上述图中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。
其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;segment data file由许多message组成， qq物理结构如下：
关键字
解释说明&lt;/p&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;8 byte offset &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;4 byte message size &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;message大小&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;4 byte CRC32 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;用crc32校验message&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;1 byte “magic&#34; &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示本次发布Kafka服务程序协议版本号&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;1 byte “attributes&#34; &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示为独立版本、或标识压缩类型、或编码类型。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;4 byte key length &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示key的长度,当key为-1时，K byte key字段不填&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;K byte key &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;可选&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;value bytes payload &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示实际消息数据。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka_查找message&#34;&gt;5.3. Kafka 查找message&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;读取offset=368776的message，需要通过下面2个步骤查找。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104213.png&#34; alt=&#34;2017 04 01 104213&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_查找segment_file&#34;&gt;5.3.1. 查找segment file&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0
　　00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1
　　00000000000000737337.index的起始偏移量为737338=737337 + 1
　　其他后续文件依次类推。
以起始偏移量命名并排序这些文件，只要根据offset **二分查找**文件列表，就可以快速定位到具体文件。当offset=368776时定位到00000000000000368769.index和对应log文件。
5.3.2、通过segment file查找message
当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址
　　然后再通过00000000000000368769.log顺序查找直到offset=368776为止。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka自定义partition&#34;&gt;6. Kafka自定义Partition&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;见代码&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>