<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>工作笔记</title>
    <link>http://dishui.oschina.io/note-hugo/index.xml</link>
    <description>Recent content on 工作笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 22 Mar 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://dishui.oschina.io/note-hugo/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>hadoop-docker</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/docker/hadoop-docker/</link>
      <pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/docker/hadoop-docker/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Dockerfiles&#34; class=&#34;bare&#34;&gt;https://github.com/HariSekhon/Dockerfiles&lt;/a&gt;
&lt;a href=&#34;https://hub.docker.com/r/harisekhon&#34; class=&#34;bare&#34;&gt;https://hub.docker.com/r/harisekhon&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hbase</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hbase/</link>
      <pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hbase/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;create &#39;user&#39;,&#39;info1&#39;,&#39;info2&#39;

describe &#39;user&#39;

exists &#39;user&#39;

put &#39;user&#39;,&#39;1234&#39;,&#39;info1:name&#39;,&#39;zhangsan&#39;
put &#39;user&#39;,&#39;1234&#39;,&#39;info2:name&#39;,&#39;zhangsan&#39;

scan &#39;user&#39;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;table class=&#34;tableblock frame-all grid-all spread&#34;&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 50%;&#34;&gt;
&lt;col style=&#34;width: 50%;&#34;&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;名称&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;命令表达式&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;创建表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;create &#39;表名&#39;, &#39;列族名1&#39;,&#39;列族名2&#39;,&#39;列族名N&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;查看所有表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;list&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;描述表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;describe &#39;表名&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;判断表存在&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;exists &#39;表名&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;判断是否禁用启用表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;is_enabled &#39;表名&#39; is_disabled ‘表名’&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;添加记录&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;put  ‘表名’, ‘rowKey’, ‘列族 : 列‘  ,  &#39;值&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;查看记录rowkey下的所有数据&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;get  &#39;表名&#39; , &#39;rowKey&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;查看表中的记录总数&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;count  &#39;表名&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;获取某个列族&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;get &#39;表名&#39;,&#39;rowkey&#39;,&#39;列族&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;获取某个列族的某个列&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;get &#39;表名&#39;,&#39;rowkey&#39;,&#39;列族：列’&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;删除记录&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;delete  ‘表名’ ,‘行名’ , ‘列族：列&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;删除整行&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;deleteall &#39;表名&#39;,&#39;rowkey&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;删除一张表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;先要屏蔽该表，才能对该表进行删除
第一步 disable ‘表名’，第二步  drop &#39;表名&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;清空表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;truncate &#39;表名&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;查看所有记录&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;scan &#34;表名&#34;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;查看某个表某个列中所有数据&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;scan &#34;表名&#34; , {COLUMNS&amp;#8658;&#39;列族名:列名&#39;}&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;更新记录&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;就是重写一遍，进行覆盖，hbase没有修改，都是追加&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>flume-sqoop</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/flume-sqoop/</link>
      <pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/flume-sqoop/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;flume-sqoop&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_flume介绍&#34;&gt;1. Flume介绍&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_概述&#34;&gt;1.1. 概述&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_运行机制&#34;&gt;1.2. 运行机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_flume采集系统结构图&#34;&gt;1.3. Flume采集系统结构图&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_flume实战案例&#34;&gt;2. Flume实战案例&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_flume的安装部署&#34;&gt;2.1. Flume的安装部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_采集案例&#34;&gt;2.2. 采集案例&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_采集目录到hdfs&#34;&gt;2.2.1. 采集目录到HDFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_采集文件到hdfs&#34;&gt;2.2.2. 采集文件到HDFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_更多source和sink组件&#34;&gt;2.2.3. 更多source和sink组件&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_工作流调度器azkaban&#34;&gt;3. 工作流调度器azkaban&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_为什么需要工作流调度系统&#34;&gt;3.1. 为什么需要工作流调度系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_工作流调度实现方式&#34;&gt;3.2. 工作流调度实现方式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_常见工作流调度系统&#34;&gt;3.3. 常见工作流调度系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_azkaban与oozie对比&#34;&gt;3.4. Azkaban与Oozie对比&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_azkaban介绍&#34;&gt;3.4.1. Azkaban介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_azkaban安装部署&#34;&gt;3.4.2. Azkaban安装部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_azkaban实战&#34;&gt;3.4.3. Azkaban实战&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_command类型多job工作流flow&#34;&gt;Command类型多job工作流flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hdfs操作任务&#34;&gt;HDFS操作任务&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mapreduce任务&#34;&gt;MAPREDUCE任务&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive脚本任务&#34;&gt;HIVE脚本任务&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sqoop数据迁移&#34;&gt;4. sqoop数据迁移&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_sqoop安装&#34;&gt;4.1. sqoop安装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sqoop的数据导入&#34;&gt;4.2. Sqoop的数据导入&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_示例&#34;&gt;4.2.1. 示例&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_导入表表数据到hdfs&#34;&gt;导入表表数据到HDFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_导入关系表到hive&#34;&gt;导入关系表到HIVE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_导入到hdfs指定目录&#34;&gt;导入到HDFS指定目录&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_导入表数据子集&#34;&gt;导入表数据子集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_增量导入&#34;&gt;增量导入&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sqoop的数据导出&#34;&gt;4.3. Sqoop的数据导出&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sqoop作业&#34;&gt;4.4. Sqoop作业&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;preamble&#34;&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在一个完整的大数据处理系统中，除了hdfs+mapreduce+hive组成分析系统的核心之外，还需要数据采集、结果数据导出、任务调度等不可或缺的辅助系统，而这些辅助工具在hadoop生态体系中都有便捷的开源框架，如图所示：&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image001.png&#34; alt=&#34;image001&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_flume介绍&#34;&gt;1. Flume介绍&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_概述&#34;&gt;1.1. 概述&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、hbase、hive、kafka等众多外部存储系统中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一般的采集需求，通过对flume的简单配置即可实现&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_运行机制&#34;&gt;1.2. 运行机制&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Flume&lt;/strong&gt; 分布式系统中最核心的角色是 &lt;strong&gt;agent&lt;/strong&gt;，&lt;strong&gt;flume&lt;/strong&gt; 采集系统就是由一个个*agent*所连接起来形成&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每一个 &lt;strong&gt;agent&lt;/strong&gt; 相当于一个数据传递员 ，内部有三个组件：&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;：采集源，用于跟数据源对接，以获取数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sink&lt;/strong&gt;：下沉地，采集数据的传送目的，用于往下一级 &lt;strong&gt;agent&lt;/strong&gt; 传递数据或者往最终存储系统传递数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Channel&lt;/strong&gt;：&lt;strong&gt;angent&lt;/strong&gt; 内部的数据传输通道，用于从 &lt;strong&gt;source&lt;/strong&gt; 将数据传递到 &lt;strong&gt;sink&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image003.png&#34; alt=&#34;image003&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_flume采集系统结构图&#34;&gt;1.3. Flume采集系统结构图&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;简单结构&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;单个agent采集数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image005.png&#34; alt=&#34;image005&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;复杂结构&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;多个agent之间串联&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image007.png&#34; alt=&#34;image007&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_flume实战案例&#34;&gt;2. Flume实战案例&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_flume的安装部署&#34;&gt;2.1. Flume的安装部署&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Flume的安装非常简单，只需要解压即可，当然，前提是已有hadoop环境&lt;br&gt;
上传安装包到数据源所在节点上&lt;br&gt;
然后解压  &lt;code&gt;tar -zxvf apache-flume-1.6.0-bin.tar.gz&lt;/code&gt;&lt;br&gt;
然后进入 &lt;strong&gt;flume&lt;/strong&gt; 的目录，修改 &lt;strong&gt;conf&lt;/strong&gt; 下的 &lt;strong&gt;flume-env.sh&lt;/strong&gt;，在里面配置 &lt;strong&gt;JAVA_HOME&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据数据采集的需求配置采集方案，描述在配置文件中(文件名可任意自定义)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;指定采集方案配置文件，在相应的节点上启动 &lt;strong&gt;flume agent&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;例子&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;先在flume的conf目录下新建一个文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;netcat-logger.conf&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 定义这个agent中各组件的名字
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 描述和配置source组件：r1
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 描述和配置sink组件：k1
a1.sinks.k1.type = logger

# 描述和配置channel组件，此处使用是内存缓存的方式
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 描述和配置source  channel   sink之间的连接关系
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动agent去采集数据&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/flume-ng agent -c conf \ &lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
-f conf/netcat-logger.conf \ &lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
-n a1 \ &lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
-Dflume.root.logger=INFO,console&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;colist arabic&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;-&lt;strong&gt;c&lt;/strong&gt; &lt;strong&gt;conf&lt;/strong&gt;   指定 &lt;strong&gt;flume&lt;/strong&gt; 自身的配置文件所在目录&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;-&lt;strong&gt;f&lt;/strong&gt; &lt;strong&gt;conf&lt;/strong&gt;/&lt;strong&gt;netcat-logger.con&lt;/strong&gt;  指定我们所描述的采集方案&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;-&lt;strong&gt;n&lt;/strong&gt; &lt;strong&gt;a1&lt;/strong&gt;  指定我们这个*agent*的名字&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;测试
先要往agent采集监听的端口上发送数据，让agent有数据可采
随便在一个能跟agent节点联网的机器上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;telnet anget-hostname  port   （telnet localhost 44444）&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image009.png&#34; alt=&#34;image009&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_采集案例&#34;&gt;2.2. 采集案例&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Q &amp;amp; A &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;a href=&#34;http://www.voidcn.com/blog/lmh94604/article/p-6042484.html&#34; class=&#34;bare&#34;&gt;http://www.voidcn.com/blog/lmh94604/article/p-6042484.html&lt;/a&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_采集目录到hdfs&#34;&gt;2.2.1. 采集目录到HDFS&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;采集需求：某服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到HDFS中去&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;根据需求，首先定义以下3大要素&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;采集源&lt;/strong&gt;，即 &lt;strong&gt;source&lt;/strong&gt; ——监控文件目录 :  &lt;strong&gt;spooldir&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;下沉目标&lt;/strong&gt;，即 &lt;strong&gt;sink&lt;/strong&gt;——&lt;strong&gt;HDFS&lt;/strong&gt; 文件系统  :  &lt;strong&gt;hdfs&lt;/strong&gt; &lt;strong&gt;sink&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;source&lt;/strong&gt; 和 &lt;strong&gt;sink&lt;/strong&gt; 之间的传递通道——&lt;strong&gt;channel&lt;/strong&gt;，可用 &lt;strong&gt;file&lt;/strong&gt; &lt;strong&gt;channel&lt;/strong&gt; 也可以用内存 &lt;strong&gt;channel&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;配置文件编写&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#定义三大组件的名称
agent1.sources = source1
agent1.sinks = sink1
agent1.channels = channel1

# 配置source组件
agent1.sources.source1.type = spooldir
agent1.sources.source1.spoolDir = /home/hadoop/logs/
agent1.sources.source1.fileHeader = false

#配置拦截器
agent1.sources.source1.interceptors = i1
agent1.sources.source1.interceptors.i1.type = host
agent1.sources.source1.interceptors.i1.hostHeader = hostname

# 配置sink组件
agent1.sinks.sink1.type = hdfs
agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M
agent1.sinks.sink1.hdfs.filePrefix = access_log
agent1.sinks.sink1.hdfs.maxOpenFiles = 5000
agent1.sinks.sink1.hdfs.batchSize= 100
agent1.sinks.sink1.hdfs.fileType = DataStream
agent1.sinks.sink1.hdfs.writeFormat =Text
agent1.sinks.sink1.hdfs.rollSize = 102400
agent1.sinks.sink1.hdfs.rollCount = 1000000
agent1.sinks.sink1.hdfs.rollInterval = 60
#agent1.sinks.sink1.hdfs.round = true
#agent1.sinks.sink1.hdfs.roundValue = 10
#agent1.sinks.sink1.hdfs.roundUnit = minute
agent1.sinks.sink1.hdfs.useLocalTimeStamp = true
# Use a channel which buffers events in memory
agent1.channels.channel1.type = memory
agent1.channels.channel1.keep-alive = 120 &lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
agent1.channels.channel1.capacity = 500000 &lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
agent1.channels.channel1.transactionCapacity = 600 &lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;

# Bind the source and sink to the channel
agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;colist arabic&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;capacity&lt;/strong&gt;：默认该通道中最大的可以存储的 &lt;strong&gt;event&lt;/strong&gt; 数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;trasactionCapacity&lt;/strong&gt;：每次最大可以从 &lt;strong&gt;source&lt;/strong&gt; 中拿到或者送到 &lt;strong&gt;sink&lt;/strong&gt; 中的 &lt;strong&gt;event&lt;/strong&gt; 数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;keep-alive&lt;/strong&gt;：&lt;strong&gt;event&lt;/strong&gt; 添加到通道中或者移出的允许时间&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_采集文件到hdfs&#34;&gt;2.2.2. 采集文件到HDFS&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;采集需求：&lt;br&gt;
比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;根据需求，首先定义以下3大要素&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;采集源，即 &lt;strong&gt;source&lt;/strong&gt; ——监控文件内容更新 : &lt;strong&gt;exec&lt;/strong&gt;  &#39;&lt;strong&gt;tail&lt;/strong&gt; -&lt;strong&gt;F&lt;/strong&gt; &lt;strong&gt;file&lt;/strong&gt;&#39;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;下沉目标，即 &lt;strong&gt;sink&lt;/strong&gt;——&lt;strong&gt;HDFS&lt;/strong&gt; 文件系统: &lt;strong&gt;hdfs&lt;/strong&gt; &lt;strong&gt;sink&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt; 和 &lt;strong&gt;sink&lt;/strong&gt; 之间的传递通道—— &lt;strong&gt;channel&lt;/strong&gt;，可用 &lt;strong&gt;file&lt;/strong&gt; &lt;strong&gt;channel&lt;/strong&gt; 也可以用 内存 &lt;strong&gt;channel&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;配置文件编写&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;agent1.sources = source1
agent1.sinks = sink1
agent1.channels = channel1

# Describe/configure tail -F source1
agent1.sources.source1.type = exec
agent1.sources.source1.command = tail -F /home/hadoop/logs/access_log
agent1.sources.source1.channels = channel1

#configure host for source
agent1.sources.source1.interceptors = i1
agent1.sources.source1.interceptors.i1.type = host
agent1.sources.source1.interceptors.i1.hostHeader = hostname

# Describe sink1
agent1.sinks.sink1.type = hdfs
#a1.sinks.k1.channel = c1
agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M
agent1.sinks.sink1.hdfs.filePrefix = access_log
agent1.sinks.sink1.hdfs.maxOpenFiles = 5000
agent1.sinks.sink1.hdfs.batchSize= 100
agent1.sinks.sink1.hdfs.fileType = DataStream
agent1.sinks.sink1.hdfs.writeFormat =Text
agent1.sinks.sink1.hdfs.rollSize = 102400
agent1.sinks.sink1.hdfs.rollCount = 1000000
agent1.sinks.sink1.hdfs.rollInterval = 60
agent1.sinks.sink1.hdfs.round = true
agent1.sinks.sink1.hdfs.roundValue = 10
agent1.sinks.sink1.hdfs.roundUnit = minute
agent1.sinks.sink1.hdfs.useLocalTimeStamp = true

# Use a channel which buffers events in memory
agent1.channels.channel1.type = memory
agent1.channels.channel1.keep-alive = 120
agent1.channels.channel1.capacity = 500000
agent1.channels.channel1.transactionCapacity = 600

# Bind the source and sink to the channel
agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_更多source和sink组件&#34;&gt;2.2.3. 更多source和sink组件&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Flume支持众多的source和sink类型，详细手册可参考官方文档
&lt;a href=&#34;http://flume.apache.org/FlumeUserGuide.html&#34;&gt;FlumeUserGuide&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_工作流调度器azkaban&#34;&gt;3. 工作流调度器azkaban&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_为什么需要工作流调度系统&#34;&gt;3.1. 为什么需要工作流调度系统&lt;/h3&gt;
&lt;div class=&#34;ulist circle&#34;&gt;
&lt;ul class=&#34;circle&#34;&gt;
&lt;li&gt;
&lt;p&gt;一个完整的数据分析系统通常都是由大量任务单元组成：
&lt;strong&gt;shell&lt;/strong&gt; 脚本程序，&lt;strong&gt;java&lt;/strong&gt; 程序，&lt;strong&gt;mapreduce&lt;/strong&gt; 程序、&lt;strong&gt;hive&lt;/strong&gt; 脚本等&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;各任务单元之间存在时间先后及前后依赖关系&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行；&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;例如，我们可能有这样一个需求，某个业务系统每天产生20G原始数据，我们每天都要对其进行处理，处理步骤如下所示：&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;通过 &lt;strong&gt;Hadoop&lt;/strong&gt; 先将原始数据同步到 &lt;strong&gt;HDFS&lt;/strong&gt; 上；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;借助 &lt;strong&gt;MapReduce&lt;/strong&gt; 计算框架对原始数据进行转换，生成的数据以分区表的形式存储到多张 &lt;strong&gt;Hive&lt;/strong&gt; 表中；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需要对 &lt;strong&gt;Hive&lt;/strong&gt; 中多个表的数据进行 &lt;strong&gt;JOIN&lt;/strong&gt; 处理，得到一个明细数据 &lt;strong&gt;Hive&lt;/strong&gt; 大表；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将明细数据进行复杂的统计分析，得到结果报表信息；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_工作流调度实现方式&#34;&gt;3.2. 工作流调度实现方式&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;简单的任务调度&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;直接使用linux的crontab来定义；&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;复杂的任务调度&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;开发调度平台&lt;br&gt;
或使用现成的开源调度系统，比如ooize、azkaban等&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_常见工作流调度系统&#34;&gt;3.3. 常见工作流调度系统&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在hadoop领域，常见的工作流调度器有Oozie, Azkaban,Cascading,Hamake等&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_azkaban与oozie对比&#34;&gt;3.4. Azkaban与Oozie对比&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;对市面上最流行的两种调度器，给出以下详细对比，以供技术选型参考。总体来说，ooize相比azkaban是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不在意某些功能的缺失，轻量级调度器azkaban是很不错的候选对象。
详情如下：&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;功能&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;两者均可以调度mapreduce,pig,java,脚本工作流任务&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;两者均可以定时执行工作流任务&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;工作流定义&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban使用Properties文件定义工作流&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie使用XML文件定义工作流&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;工作流传参&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban支持直接传参，例如${input}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie支持参数和EL表达式，例如${fs:dirSize(myInputDir)}&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;定时执行&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban的定时执行任务是基于时间的&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie的定时执行任务基于时间和输入数据&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;资源管理&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban有较严格的权限控制，如用户对工作流进行读/写/执行等操作&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie暂无严格的权限控制&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;工作流执行&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban有两种运行模式，分别是solo server mode(executor server和web server部署在同一台节点)和multi server mode(executor server和web server可以部署在不同节点)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie作为工作流服务器运行，支持多用户和多工作流&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;工作流管理&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban支持浏览器以及ajax方式操作工作流&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie支持命令行、HTTP REST、Java API、浏览器操作工作流&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_azkaban介绍&#34;&gt;3.4.1. Azkaban介绍&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;它有如下功能特点：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;Web用户界面&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方便上传工作流&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方便设置任务之间的关系&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调度工作流&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;认证/授权(权限的工作)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;能够杀死并重新启动工作流&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模块化和可插拔的插件机制&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;项目工作区&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;工作流和任务的日志记录和审计&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_azkaban安装部署&#34;&gt;3.4.2. Azkaban安装部署&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;准备工作&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;Azkaban Web服务器
    azkaban-web-server-2.5.0.tar.gz
Azkaban执行服务器
    azkaban-executor-server-2.5.0.tar.gz&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;MySQL&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;目前azkaban只支持 mysql,需安装mysql服务器,本文档中默认已安装好mysql服务器,并建立了 root用户,密码 root.

下载地址:http://azkaban.github.io/downloads.html&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;安装&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;将安装文件上传到集群,最好上传到安装 hive、sqoop的机器上,方便命令的执行
在当前用户目录下新建 azkabantools目录,用于存放源安装文件.新建azkaban目录,用于存放azkaban运行程序

azkaban web服务器安装
解压azkaban-web-server-2.5.0.tar.gz
命令:
    tar –zxvf azkaban-web-server-2.5.0.tar.gz
将解压后的 azkaban-web-server-2.5.0 移动到 azkaban目录中,并重新命名 webserver
命令:
    mv azkaban-web-server-2.5.0 ../azkaban
    cd ../azkaban
    mv azkaban-web-server-2.5.0  server&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;azkaban 执行服器安装&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;解压 azkaban-executor-server-2.5.0.tar.gz
命令:
    tar –zxvf azkaban-executor-server-2.5.0.tar.gz
将解压后的 azkaban-executor-server-2.5.0 移动到 azkaban目录中,并重新命名 executor
命令:
    mv azkaban-executor-server-2.5.0  ../azkaban
    cd ../azkaban
    mv azkaban-executor-server-2.5.0  executor&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;azkaban脚本导入&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;解压:
    azkaban-sql-script-2.5.0.tar.gz
命令:
    tar –zxvf azkaban-sql-script-2.5.0.tar.gz
将解压后的mysql 脚本,导入到mysql中:
进入mysql
mysql&amp;gt; create database azkaban;
mysql&amp;gt; use azkaban;
Database changed
mysql&amp;gt; source /home/hadoop/azkaban-2.5.0/create-all-sql-2.5.0.sql;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;创建SSL配置&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;参考地址:
    http://docs.codehaus.org/display/JETTY/How+to+configure+SSL
命令:
    keytool -keystore keystore -alias jetty -genkey -keyalg RSA
运行此命令后,会提示输入当前生成 keystor的密码及相应信息,输入的密码请劳记,信息如下:

输入keystore密码：
再次输入新密码:
您的名字与姓氏是什么？
  [Unknown]：
您的组织单位名称是什么？
  [Unknown]：
您的组织名称是什么？
  [Unknown]：
您所在的城市或区域名称是什么？
  [Unknown]：
您所在的州或省份名称是什么？
  [Unknown]：
该单位的两字母国家代码是什么
  [Unknown]：  CN
CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？
  [否]：  y

输入&amp;lt;jetty&amp;gt;的主密码
        （如果和 keystore 密码相同，按回车）：
再次输入新密码:
完成上述工作后,将在当前目录生成 keystore 证书文件,将keystore 考贝到 azkaban web服务器根目录中.
如: cp keystore azkaban/server&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;配置文件&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;注：先配置好服务器节点上的时区
1、先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可
2、拷贝该时区文件，覆盖系统本地时区配置
cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;azkaban web服务器配置&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;进入azkaban web服务器安装目录 conf目录&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;azkaban.properties&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#Azkaban Personalization Settings
azkaban.name=Test                           #服务器UI名称,用于服务器上方显示的名字
azkaban.label=My Local Azkaban                               #描述
azkaban.color=#FF3601                                                 #UI颜色
azkaban.default.servlet.path=/index                         #
web.resource.dir=web/                                                 #默认根web目录
default.timezone.id=Asia/Shanghai                           #默认时区,已改为亚洲/上海 默认为美国

#Azkaban UserManager class
user.manager.class=azkaban.user.XmlUserManager   #用户权限管理默认类
user.manager.xml.file=conf/azkaban-users.xml              #用户配置,具体配置参加下文

#Loader for projects
executor.global.properties=conf/global.properties    # global配置文件所在位置
azkaban.project.dir=projects                                                #

database.type=mysql                                                              #数据库类型
mysql.port=3306                                                                       #端口号
mysql.host=localhost                                                      #数据库连接IP
mysql.database=azkaban                                                       #数据库实例名
mysql.user=root                                                                 #数据库用户名
mysql.password=root                                                          #数据库密码
mysql.numconnections=100                                                  #最大连接数

# Velocity dev mode
velocity.dev.mode=false
# Jetty服务器属性.
jetty.maxThreads=25                                                               #最大线程数
jetty.ssl.port=8443                                                                   #Jetty SSL端口
jetty.port=8081                                                                         #Jetty端口
jetty.keystore=keystore                                                          #SSL文件名
jetty.password=123456                                                             #SSL文件密码
jetty.keypassword=123456                                                      #Jetty主密码 与 keystore文件相同
jetty.truststore=keystore                                                                #SSL文件名
jetty.trustpassword=123456                                                   # SSL文件密码

# 执行服务器属性
executor.port=12321                                                               #执行服务器端口

# 邮件设置
mail.sender=xxxxxxxx@163.com                                       #发送邮箱
mail.host=smtp.163.com                                                       #发送邮箱smtp地址
mail.user=xxxxxxxx                                       #发送邮件时显示的名称
mail.password=**********                                                 #邮箱密码
job.failure.email=xxxxxxxx@163.com                              #任务失败时发送邮件的地址
job.success.email=xxxxxxxx@163.com                            #任务成功时发送邮件的地址
lockdown.create.projects=false                                           #
cache.directory=cache                                                            #缓存目录&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;azkaban 执行服务器executor配置 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;进入执行服务器安装目录conf,修改azkaban.properties&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;azkaban.properties&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#Azkaban
default.timezone.id=Asia/Shanghai                                              #时区

# Azkaban JobTypes 插件配置
azkaban.jobtype.plugin.dir=plugins/jobtypes                   #jobtype 插件所在位置

#Loader for projects
executor.global.properties=conf/global.properties
azkaban.project.dir=projects

#数据库设置
database.type=mysql                                                                       #数据库类型(目前只支持mysql)
mysql.port=3306                                                                                #数据库端口号
mysql.host=192.168.20.200                                                           #数据库IP地址
mysql.database=azkaban                                                                #数据库实例名
mysql.user=root                                                                       #数据库用户名
mysql.password=root #数据库密码
mysql.numconnections=100                                                           #最大连接数

# 执行服务器配置
executor.maxThreads=50                                                                #最大线程数
executor.port=12321                                                               #端口号(如修改,请与web服务中一致)
executor.flow.threads=30                                                                #线程数&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;用户配置&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;进入azkaban web服务器conf目录,修改azkaban-users.xml
vi azkaban-users.xml 增加 管理员用户&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;azkaban-users.xml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;azkaban-users&amp;gt;
        &amp;lt;user username=&#34;azkaban&#34; password=&#34;azkaban&#34; roles=&#34;admin&#34; groups=&#34;azkaban&#34; /&amp;gt;
        &amp;lt;user username=&#34;metrics&#34; password=&#34;metrics&#34; roles=&#34;metrics&#34;/&amp;gt;
        &amp;lt;user username=&#34;admin&#34; password=&#34;admin&#34; roles=&#34;admin,metrics&#34; /&amp;gt;
        &amp;lt;role name=&#34;admin&#34; permissions=&#34;ADMIN&#34; /&amp;gt;
        &amp;lt;role name=&#34;metrics&#34; permissions=&#34;METRICS&#34;/&amp;gt;
&amp;lt;/azkaban-users&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;启动&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;web服务器
    在azkaban web服务器目录下执行启动命令
        bin/azkaban-web-start.sh
    注:在web服务器根目录运行
或者启动到后台
    nohup  bin/azkaban-web-start.sh  1&amp;gt;/tmp/azstd.out  2&amp;gt;/tmp/azerr.out &amp;amp;
执行服务器
    在执行服务器目录下执行启动命令
        bin/azkaban-executor-start.sh
    注:只能要执行服务器根目录运行

启动完成后,在浏览器(建议使用谷歌浏览器)中输入
https://服务器IP地址:8443 ,即可访问azkaban服务了.
在登录中输入刚才新的户用名及密码,点击 login.&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_azkaban实战&#34;&gt;3.4.3. Azkaban实战&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaba内置的任务类型支持command、java&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Command类型单一job示例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建job描述文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vi command.job
#command.job
type=command
command=echo &#39;hello&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将job资源文件打包成zip文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;zip command.job&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过azkaban的web管理平台创建project并上传job压缩包
首先创建project&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image011.png&#34; alt=&#34;image011&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;上传zip包&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image013.png&#34; alt=&#34;image013&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动执行该job
image::http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image015.png[]
---&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_command类型多job工作流flow&#34;&gt;Command类型多job工作流flow&lt;/h5&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建有依赖关系的多个job描述
第一个job：foo.job&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# foo.job
type=command
command=echo foo&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;第二个job：bar.job依赖foo.job&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# bar.job
type=command
dependencies=foo
command=echo bar&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将所有job资源文件打到一个zip包中&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image017.png&#34; alt=&#34;image017&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在azkaban的web管理界面创建工程并上传zip包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动工作流flow&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_hdfs操作任务&#34;&gt;HDFS操作任务&lt;/h5&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建job描述文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# fs.job
type=command
command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop fs -mkdir /azaz&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将job资源文件打包成zip文件&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image019.png&#34; alt=&#34;image019&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过azkaban的web管理平台创建project并上传job压缩包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动执行该job&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_mapreduce任务&#34;&gt;MAPREDUCE任务&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Mr任务依然可以使用command的job类型来执行&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建job描述文件，及mr程序jar包（示例中直接使用hadoop自带的example jar）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# mrwc.job
type=command
command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop  jar hadoop-mapreduce-examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将所有job资源文件打到一个zip包中&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/flume/image021.png&#34; alt=&#34;image021&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在azkaban的web管理界面创建工程并上传zip包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动job&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_hive脚本任务&#34;&gt;HIVE脚本任务&lt;/h5&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建job描述文件和hive脚本
Hive脚本： test.sql&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;use default;
drop table aztest;
create table aztest(id int,name string) row format delimited fields terminated by &#39;,&#39;;
load data inpath &#39;/aztest/hiveinput&#39; into table aztest;
create table azres as select * from aztest;
insert overwrite directory &#39;/aztest/hiveoutput&#39; select count(1) from aztest;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Job描述文件：hivef.job&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# hivef.job
type=command
command=/home/hadoop/apps/hive/bin/hive -f &#39;test.sql&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将所有job资源文件打到一个zip包中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在azkaban的web管理界面创建工程并上传zip包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动job&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_sqoop数据迁移&#34;&gt;4. sqoop数据迁移&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。
导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统；
导出数据：从Hadoop的文件系统中导出数据到关系数据库&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sqoop安装&#34;&gt;4.1. sqoop安装&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;安装sqoop的前提是已经具备java和hadoop的环境
. 下载并解压&lt;br&gt;
最新版下载地址http://ftp.wayne.edu/apache/sqoop/1.4.6/&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;修改配置文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ cd $SQOOP_HOME/conf
$ mv sqoop-env-template.sh sqoop-env.sh
#打开sqoop-env.sh并编辑下面几行：
export HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.6.1/
export HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.6.1/
export HIVE_HOME=/home/hadoop/apps/hive-1.2.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加入mysql的jdbc驱动包&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cp  ~/app/hive/lib/mysql-connector-java-5.1.28.jar   $SQOOP_HOME/lib/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;验证启动&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ cd $SQOOP_HOME/bin
$ sqoop-version
# 预期的输出：
15/12/17 14:52:32 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6
Sqoop 1.4.6 git commit id 5b34accaca7de251fc91161733f906af2eddbe83
Compiled by abe on Fri Aug 1 11:19:26 PDT 2015&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;到这里，整个Sqoop安装工作完成。&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sqoop的数据导入&#34;&gt;4.2. Sqoop的数据导入&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;“导入工具”导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据）&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;下面的语法用于将数据导入HDFS。&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ sqoop import (generic-args) (import-args)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_示例&#34;&gt;4.2.1. 示例&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;表数据 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在mysql中有一个库userdb中三个表：emp, emp_add和emp_contact&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;表emp&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;id      name    deg     salary  dept
1201    gopal   manager 50,000  TP
1202    manisha Proof reader    50,000  TP
1203    khalil  php dev 30,000  AC
1204    prasanth    php dev 30,000  AC
1205    kranthi admin   20,000  TP&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;表emp_add:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;id      hno     street  city
1201    288A    vgiri   jublee
1202    108I    aoc sec-bad
1203    144Z    pgutta  hyd
1204    78B old city    sec-bad
1205    720X    hitec   sec-bad&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;表emp_conn:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;id      phno    email
1201    2356742 gopal@tp.com
1202    1661663 manisha@tp.com
1203    8887776 khalil@ac.com
1204    9988774 prasanth@ac.com
1205    1231231 kranthi@tp.com&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_导入表表数据到hdfs&#34;&gt;导入表表数据到HDFS&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;下面的命令用于从MySQL数据库服务器中的emp表导入HDFS。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$bin/sqoop import \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--table emp   \
--m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;为了验证在HDFS导入的数据，请使用以下命令查看导入的数据&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-00000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;emp表的数据和字段之间用逗号(,)表示&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1201, gopal,    manager, 50000, TP
1202, manisha,  preader, 50000, TP
1203, kalil,    php dev, 30000, AC
1204, prasanth, php dev, 30000, AC
1205, kranthi,  admin,   20000, TP&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_导入关系表到hive&#34;&gt;导入关系表到HIVE&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --hive-import --m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_导入到hdfs指定目录&#34;&gt;导入到HDFS指定目录&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。
以下是指定目标目录选项的Sqoop导入命令的语法。::&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;--target-dir &amp;lt;new or exist directory in HDFS&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令是用来导入emp_add表数据到&#39;/queryresult&amp;#8217;目录。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop import \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--target-dir /queryresult \
--table emp --m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令是用来验证 /queryresult 目录中 emp_add表导入的数据形式。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$HADOOP_HOME/bin/hadoop fs -cat /queryresult/part-m-*&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;它会用逗号（，）分隔emp_add表的数据和字段。 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1201, 288A, vgiri,   jublee
1202, 108I, aoc,     sec-bad
1203, 144Z, pgutta,  hyd
1204, 78B,  oldcity, sec-bad
1205, 720C, hitech,  sec-bad&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_导入表数据子集&#34;&gt;导入表数据子集&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;我们可以导入表的使用Sqoop导入工具，&#34;where&#34;子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。
where子句的语法如下。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;--where &amp;lt;condition&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令用来导入emp_add表数据的子集。子集查询检索员工ID和地址，居住城市为：Secunderabad &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop import \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--where &#34;city =&#39;sec-bad&#39;&#34; \
--target-dir /wherequery \
--table emp_add --m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;按需导入&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop import \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--target-dir /wherequery2 \
--query &#39;select id,name,deg from emp WHERE  id&amp;gt;1207 and $CONDITIONS&#39; \
--split-by id \
--fields-terminated-by &#39;\t&#39; \
--m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令用来验证数据从emp_add表导入/wherequery目录&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$HADOOP_HOME/bin/hadoop fs -cat /wherequery/part-m-*&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;它用逗号（，）分隔 emp_add表数据和字段。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1202, 108I, aoc, sec-bad
1204, 78B, oldcity, sec-bad
1205, 720C, hitech, sec-bad&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_增量导入&#34;&gt;增量导入&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;增量导入是仅导入新添加的表中的行的技术。
它需要添加‘incremental’, ‘check-column’, 和 ‘last-value’选项来执行增量导入。
下面的语法用于Sqoop导入命令增量选项。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;--incremental &amp;lt;mode&amp;gt;
--check-column &amp;lt;column name&amp;gt;
--last value &amp;lt;last check column value&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;假设新添加的数据转换成emp表如下：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1206, satish p, grp des, 20000, GR&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令用于在EMP表执行增量导入。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop import \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--table emp --m 1 \
--incremental append \
--check-column id \
--last-value 1208&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;以下命令用于从emp表导入HDFS emp/ 目录的数据验证。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-*&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;它用逗号（，）分隔 emp_add表数据和字段。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;1201, gopal,    manager, 50000, TP
1202, manisha,  preader, 50000, TP
1203, kalil,    php dev, 30000, AC
1204, prasanth, php dev, 30000, AC
1205, kranthi,  admin,   20000, TP
1206, satish p, grp des, 20000, GR&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令是从表emp 用来查看修改或新添加的行&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ $HADOOP_HOME/bin/hadoop fs -cat /emp/part-m-*1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;这表示新添加的行用逗号（，）分隔emp表的字段。
1206, satish p, grp des, 20000, GR&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sqoop的数据导出&#34;&gt;4.3. Sqoop的数据导出&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;将数据从HDFS导出到RDBMS数据库
导出前，目标表必须存在于目标数据库中。
   默认操作是从将文件中的数据使用INSERT语句插入到表中
   更新模式下，是生成UPDATE语句更新表数据
语法
以下是export命令语法。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ sqoop export (generic-args) (export-args)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;示例
数据是在HDFS 中“EMP/”目录的emp_data文件中。所述emp_data如下：
1201, gopal,     manager, 50000, TP
1202, manisha,   preader, 50000, TP
1203, kalil,     php dev, 30000, AC
1204, prasanth,  php dev, 30000, AC
1205, kranthi,   admin,   20000, TP
1206, satish p,  grp des, 20000, GR&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;首先需要手动创建mysql中的目标表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ mysql
mysql&amp;gt; USE db;
mysql&amp;gt; CREATE TABLE employee (
   id INT NOT NULL PRIMARY KEY,
   name VARCHAR(20),
   deg VARCHAR(20),
   salary INT,
   dept VARCHAR(10));&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;然后执行导出命令&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop export \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--table employee \
--export-dir /user/hadoop/emp/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;验证表mysql命令行。&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mysql&amp;gt;select * from employee;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果给定的数据存储成功，那么可以找到数据在如下的employee表。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;+------+--------------+-------------+-------------------+--------+
| Id   | Name         | Designation | Salary            | Dept   |
+------+--------------+-------------+-------------------+--------+
| 1201 | gopal        | manager     | 50000             | TP     |
| 1202 | manisha      | preader     | 50000             | TP     |
| 1203 | kalil        | php dev     | 30000             | AC     |
| 1204 | prasanth     | php dev     | 30000             | AC     |
| 1205 | kranthi      | admin       | 20000             | TP     |
| 1206 | satish p     | grp des     | 20000             | GR     |
+------+--------------+-------------+-------------------+--------+&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sqoop作业&#34;&gt;4.4. Sqoop作业&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注：Sqoop作业——将事先定义好的数据导入导出任务按照指定流程运行
语法
以下是创建Sqoop作业的语法。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ sqoop job (generic-args) (job-args)
   [-- [subtool-name] (subtool-args)]

$ sqoop-job (generic-args) (job-args)
   [-- [subtool-name] (subtool-args)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;创建作业(--create) &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在这里，我们创建一个名为myjob，这可以从RDBMS表的数据导入到HDFS作业。&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop job --create myimportjob -- import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;该命令创建了一个从db库的employee表导入到HDFS文件的作业。&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;验证作业 (--list)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;‘--list’ 参数是用来验证保存的作业。下面的命令用来验证保存Sqoop作业的列表。
$ sqoop job --list
它显示了保存作业列表。
Available jobs:
myimportjob
检查作业(--show)
‘--show’ 参数用于检查或验证特定的工作，及其详细信息。以下命令和样本输出用来验证一个名为myjob的作业。
$ sqoop job --show myjob
它显示了工具和它们的选择，这是使用在myjob中作业情况。
Job: myjob
 Tool: import Options:
 ----------------------------
 direct.import = true
 codegen.input.delimiters.record = 0
 hdfs.append.dir = false
 db.table = employee
 ...
 incremental.last.value = 1206
 ...&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;执行作业 (--exec)
‘--exec’ 选项用于执行保存的作业。下面的命令用于执行保存的作业称为myjob。
$ sqoop job --exec myjob
它会显示下面的输出。
10/08/19 13:08:45 INFO tool.CodeGenTool: Beginning code generation
&amp;#8230;&amp;#8203;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Sqoop的原理
概述
Sqoop的原理其实就是将导入导出命令转化为mapreduce程序来执行，sqoop在接收到命令后，都要生成mapreduce程序&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;使用sqoop的代码生成工具可以方便查看到sqoop所生成的java代码，并可在此基础之上进行深入定制开发&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;代码定制
以下是Sqoop代码生成命令的语法：
$ sqoop-codegen (generic-args) (codegen-args)
$ sqoop-codegen (generic-args) (codegen-args)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;示例：以USERDB数据库中的表emp来生成Java代码为例。
下面的命令用来生成导入
$ sqoop-codegen \
--import
--connect jdbc:mysql://localhost/userdb \
--username root \
--table emp&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果命令成功执行，那么它就会产生如下的输出。
14/12/23 02:34:40 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5
14/12/23 02:34:41 INFO tool.CodeGenTool: Beginning code generation
……………….
14/12/23 02:34:42 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop
Note: /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/emp.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
14/12/23 02:34:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/emp.jar&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;验证: 查看输出目录下的文件
$ cd /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/
$ ls
emp.class
emp.jar
emp.java&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果想做深入定制导出，则可修改上述代码文件&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hive详解</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive%E8%AF%A6%E8%A7%A3/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hive详解&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hive简介&#34;&gt;1. Hive简介&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_什么是hive&#34;&gt;1.1. 什么是Hive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_为什么使用hive&#34;&gt;1.2. 为什么使用Hive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive的特点&#34;&gt;1.3. Hive的特点&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#__strong_hive_strong_架构&#34;&gt;2. &lt;strong&gt;Hive&lt;/strong&gt; 架构&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_基本组成&#34;&gt;2.1. 基本组成&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_各组件的基本功能&#34;&gt;2.2. 各组件的基本功能&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive与hadoop的关系&#34;&gt;3. Hive与Hadoop的关系&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive与传统数据库对比&#34;&gt;4. Hive与传统数据库对比&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_对比&#34;&gt;4.1. 对比&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive的数据存储&#34;&gt;5. Hive的数据存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#__strong_hive_strong_的安装部署&#34;&gt;6. &lt;strong&gt;HIVE&lt;/strong&gt; 的安装部署&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_使用方式&#34;&gt;6.1. 使用方式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive基本操作&#34;&gt;7. Hive基本操作&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_ddl操作&#34;&gt;7.1. DDL操作&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_创建表&#34;&gt;7.1.1. 创建表&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_具体实例&#34;&gt;具体实例&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_修改表&#34;&gt;7.1.2. 修改表&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_增加_删除分区&#34;&gt;增加/删除分区&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_重命名表&#34;&gt;重命名表&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_增加_更新列&#34;&gt;增加/更新列&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_显示命令&#34;&gt;显示命令&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_dml操作&#34;&gt;7.1.3. DML操作&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_load&#34;&gt;Load&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_insert&#34;&gt;Insert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_select&#34;&gt;SELECT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive_join&#34;&gt;7.1.4. Hive Join&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive简介&#34;&gt;1. Hive简介&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_什么是hive&#34;&gt;1.1. 什么是Hive&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_为什么使用hive&#34;&gt;1.2. 为什么使用Hive&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;直接使用hadoop所面临的问题 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;人员学习成本太高&lt;br&gt;
项目周期要求太短&lt;br&gt;
MapReduce实现复杂查询逻辑开发难度太大&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;为什么要使用Hive &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;避免了去写MapReduce，减少开发人员的学习成本。+
扩展功能很方便。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_hive的特点&#34;&gt;1.3. Hive的特点&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;可扩展 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Hive可以自由的扩展集群的规模，一般情况下不需要重启服务。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;延展性 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;容错 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;良好的容错性，节点出现问题SQL仍可完成执行&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;__strong_hive_strong_架构&#34;&gt;2. &lt;strong&gt;Hive&lt;/strong&gt; 架构&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/2017-03-17_155621.png&#34; alt=&#34;2017 03 17 155621&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Jobtracker &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;hadoop1.x中的组件，它的功能相当于： Resourcemanager+AppMaster&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;TaskTracker &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;相当于：  Nodemanager  +  yarnchild&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_基本组成&#34;&gt;2.1. 基本组成&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;用户接口&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;包括 CLI、JDBC/ODBC、WebGUI。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;元数据存储&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;通常是存储在关系数据库如 mysql,derby中。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;解释器、编译器、优化器、执行器&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_各组件的基本功能&#34;&gt;2.2. 各组件的基本功能&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;用户接口主要由三个&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;CLI、JDBC/ODBC和WebGUI。其中，CLI为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;元数据存储 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;解释器、编译器、优化器 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive与hadoop的关系&#34;&gt;3. Hive与Hadoop的关系&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Hive利用HDFS存储数据，利用MapReduce查询数据&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/2017-03-17_160404.png&#34; alt=&#34;2017 03 17 160404&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive与传统数据库对比&#34;&gt;4. Hive与传统数据库对比&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/2017-03-17_160454.png&#34; alt=&#34;2017 03 17 160454&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_对比&#34;&gt;4.1. 对比&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;查询语言。由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据存储位置。Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据格式。Hive 中没有定义专门的数据格式，数据格式可以由用户指定，用户定义数据格式需要指定三个属性：列分隔符（通常为空格、”\t”、”\x001″）、行分隔符（”\n”）以及读取文件数据的方法（Hive 中默认有三个文件格式 TextFile，SequenceFile 以及 RCFile）。由于在加载数据的过程中，不需要从用户数据格式到 Hive 定义的数据格式的转换，因此，Hive 在加载的过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的 HDFS 目录中。而在数据库中，不同的数据库有不同的存储引擎，定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据更新。由于 Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive 中不支持对数据的改写和添加，所有的数据都是在加载的时候中确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO &amp;#8230;&amp;#8203;  VALUES 添加数据，使用 UPDATE &amp;#8230;&amp;#8203; SET 修改数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;索引。之前已经说过，Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 Key&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive的数据存储&#34;&gt;5. Hive的数据存储&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hive&lt;/strong&gt; 中所有的数据都存储在 &lt;strong&gt;HDFS&lt;/strong&gt;  中，没有专门的数据存储格式（可支持*Text* ，&lt;strong&gt;SequenceFile&lt;/strong&gt; ，&lt;strong&gt;ParquetFile&lt;/strong&gt; ，&lt;strong&gt;RCFILE&lt;/strong&gt; 等）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;只需要在创建表的时候告诉 &lt;strong&gt;Hive&lt;/strong&gt;  数据中的列分隔符和行分隔符，&lt;strong&gt;Hive&lt;/strong&gt;  就可以解析数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hive&lt;/strong&gt;  中包含以下数据模型：&lt;strong&gt;DB&lt;/strong&gt; 、&lt;strong&gt;Table&lt;/strong&gt; ，&lt;strong&gt;External&lt;/strong&gt;  &lt;strong&gt;Table&lt;/strong&gt; ，&lt;strong&gt;Partition&lt;/strong&gt; ，&lt;strong&gt;Bucket&lt;/strong&gt; 。&lt;/p&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;db&lt;/strong&gt; ：在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现为${&lt;strong&gt;hive&lt;/strong&gt; .&lt;strong&gt;metastore&lt;/strong&gt; .&lt;strong&gt;warehouse&lt;/strong&gt; .&lt;strong&gt;dir&lt;/strong&gt; }目录下一个文件夹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;table&lt;/strong&gt; ：在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现所属*db* 目录下一个文件夹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;external table&lt;/strong&gt; ：外部表, 与*table* 类似，不过其数据存放位置可以在任意指定路径
普通表: 删除表后, &lt;strong&gt;hdfs&lt;/strong&gt; 上的文件都删了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;External&lt;/strong&gt; 外部表删除后, &lt;strong&gt;hdfs&lt;/strong&gt; 上的文件没有删除, 只是把文件删除了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;partition&lt;/strong&gt; ：在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现为 &lt;strong&gt;table&lt;/strong&gt; 目录下的子目录&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;bucket&lt;/strong&gt; ：桶, 在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现为同一个表目录下根据 &lt;strong&gt;hash&lt;/strong&gt; 散列之后的多个文件, 会根据不同的文件把数据放到不同的文件中&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;__strong_hive_strong_的安装部署&#34;&gt;6. &lt;strong&gt;HIVE&lt;/strong&gt; 的安装部署&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive&#34;&gt;hive 安装&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_使用方式&#34;&gt;6.1. 使用方式&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Hive交互shell&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hive&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hive thrift服务&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;启动方式，（假如是在hadoop01上）&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;启动为前台 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hiveserver2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;启动为后台 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;nohup bin/hiveserver2 1&amp;gt;/var/log/hiveserver.log 2&amp;gt;/var/log/hiveserver.err &amp;amp;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;启动成功后，可以在别的节点上用beeline去连接&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;方式（1）&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;code&gt;hive/bin/beeline&lt;/code&gt;  回车，进入beeline的命令界面&lt;br&gt;
输入命令连接 &lt;code&gt;hiveserver2&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;beeline&amp;gt; !connect jdbc:hive2//mini1:10000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;（hadoop01是hiveserver2所启动的那台主机名，端口默认是10000）&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;方式（2） &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;或者启动就连接：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/beeline -u jdbc:hive2://mini1:10000 -n hadoop&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;接下来就可以做正常sql查询了&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive基本操作&#34;&gt;7. Hive基本操作&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_ddl操作&#34;&gt;7.1. DDL操作&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_创建表&#34;&gt;7.1.1. 创建表&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;建表语法&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name
   [(col_name data_type [COMMENT col_comment], ...)]
   [COMMENT table_comment]
   [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
   [CLUSTERED BY (col_name, col_name, ...)
   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
   [ROW FORMAT row_format]
   [STORED AS file_format]
   [LOCATION hdfs_path]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CREATE&lt;/strong&gt; &lt;strong&gt;TABLE&lt;/strong&gt; 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 &lt;strong&gt;IF&lt;/strong&gt; &lt;strong&gt;NOT&lt;/strong&gt; &lt;strong&gt;EXISTS&lt;/strong&gt; 选项来忽略这个异常。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;EXTERNAL&lt;/strong&gt; 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（&lt;strong&gt;LOCATION&lt;/strong&gt;），&lt;strong&gt;Hive&lt;/strong&gt; 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LIKE&lt;/strong&gt; 允许用户复制现有的表结构，但是不复制数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ROW&lt;/strong&gt; &lt;strong&gt;FORMAT&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;用户在建表的时候可以自定义 &lt;strong&gt;SerDe&lt;/strong&gt; 或者使用自带的 &lt;strong&gt;SerDe&lt;/strong&gt;。如果没有指定 &lt;strong&gt;ROW&lt;/strong&gt; &lt;strong&gt;FORMAT&lt;/strong&gt; 或者 &lt;strong&gt;ROW&lt;/strong&gt; &lt;strong&gt;FORMAT&lt;/strong&gt; &lt;strong&gt;DELIMITED&lt;/strong&gt;，将会使用自带的 &lt;strong&gt;SerDe&lt;/strong&gt;。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 &lt;strong&gt;SerDe&lt;/strong&gt;，&lt;strong&gt;Hive*通过 *SerDe&lt;/strong&gt; 确定表的具体的列的数据。&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;STORED&lt;/strong&gt; &lt;strong&gt;AS&lt;/strong&gt;
&lt;strong&gt;SEQUENCEFILE&lt;/strong&gt;|&lt;strong&gt;TEXTFILE&lt;/strong&gt;|&lt;strong&gt;RCFILE&lt;/strong&gt;
如果文件数据是纯文本，可以使用 &lt;strong&gt;STORED&lt;/strong&gt; &lt;strong&gt;AS&lt;/strong&gt; &lt;strong&gt;TEXTFILE&lt;/strong&gt;。如果数据需要压缩，使用 &lt;strong&gt;STORED&lt;/strong&gt; &lt;strong&gt;AS&lt;/strong&gt; &lt;strong&gt;SEQUENCEFILE&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLUSTERED&lt;/strong&gt; &lt;strong&gt;BY&lt;/strong&gt;
对于每一个表（&lt;strong&gt;table&lt;/strong&gt;）或者分区， &lt;strong&gt;Hive&lt;/strong&gt; 可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。&lt;strong&gt;Hive&lt;/strong&gt; 也是 针对某一列进行桶的组织。&lt;strong&gt;Hive&lt;/strong&gt; 采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。
把表（或者分区）组织成桶（&lt;strong&gt;Bucket&lt;/strong&gt;）有两个理由：&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;获得更高的查询处理效率。桶为表加上了额外的结构，&lt;strong&gt;Hive&lt;/strong&gt; 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 &lt;strong&gt;Map&lt;/strong&gt; 端连接 （&lt;strong&gt;Map&lt;/strong&gt;-&lt;strong&gt;side&lt;/strong&gt; &lt;strong&gt;join&lt;/strong&gt;）高效的实现。比如 &lt;strong&gt;JOIN&lt;/strong&gt; 操作。对于 &lt;strong&gt;JOIN&lt;/strong&gt; 操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行 &lt;strong&gt;JOIN&lt;/strong&gt; 操作就可以，可以大大较少 &lt;strong&gt;JOIN&lt;/strong&gt; 的数据量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使取样（&lt;strong&gt;sampling&lt;/strong&gt;）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_具体实例&#34;&gt;具体实例&lt;/h5&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建内部表mytable&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image010.png&#34; alt=&#34;image010&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建外部表pageview&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image012.png&#34; alt=&#34;image012&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建分区表invites&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;create table student_p(Sno int,Sname string,Sex string,Sage int,Sdept string) partitioned by(part string) row format delimited fields terminated by &#39;,&#39;stored as textfile;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image014.png&#34; alt=&#34;image014&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建带桶的表student&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image016.png&#34; alt=&#34;image016&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_修改表&#34;&gt;7.1.2. 修改表&lt;/h4&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_增加_删除分区&#34;&gt;增加/删除分区&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION &#39;location1&#39; ] partition_spec [ LOCATION &#39;location2&#39; ] ...
partition_spec:
: PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)

ALTER TABLE table_name DROP partition_spec, partition_spec,...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;alter table student_p add partition(part=&#39;a&#39;) partition(part=&#39;b&#39;);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image018.png&#34; alt=&#34;image018&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image020.png&#34; alt=&#34;image020&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_重命名表&#34;&gt;重命名表&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name RENAME TO new_table_name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image022.png&#34; alt=&#34;image022&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_增加_更新列&#34;&gt;增加/更新列&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image024.png&#34; alt=&#34;image024&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_显示命令&#34;&gt;显示命令&lt;/h5&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;show tables
show databases
show partitions
show functions
desc extended t_name;
desc formatted table_name;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_dml操作&#34;&gt;7.1.3. DML操作&lt;/h4&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_load&#34;&gt;Load&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO
TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;说明&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;filepath&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;相对路径，例如：&lt;strong&gt;project/data1&lt;/strong&gt;&lt;br&gt;
绝对路径，例如：&lt;strong&gt;/user/hive/project/data1&lt;/strong&gt;&lt;br&gt;
包含模式的完整 URI，列如：+
&lt;strong&gt;hdfs://namenode:9000/user/hive/project/data1&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LOCAL关键字&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。&lt;br&gt;
如果没有指定 LOCAL 关键字，则根据inpath中的uri 查找文件&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OVERWRITE 关键字&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。&lt;br&gt;
如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;加载相对路径数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image026.png&#34; alt=&#34;image026&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加载绝对路径数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image028.png&#34; alt=&#34;image028&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加载包含模式数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image030.png&#34; alt=&#34;image030&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OVERWRITE关键字使用&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image032.png&#34; alt=&#34;image032&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_insert&#34;&gt;Insert&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement

Multiple inserts:
FROM from_statement
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1
[INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...

Dynamic partition inserts:
INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;基本模式插入&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image034.png&#34; alt=&#34;image034&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多插入模式&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image036.png&#34; alt=&#34;image036&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自动分区模式&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image038.png&#34; alt=&#34;image038&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect5&#34;&gt;
&lt;h6 id=&#34;_导出表数据&#34;&gt;导出表数据&lt;/h6&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...


multiple inserts:
FROM from_statement
INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1
[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;导出文件到本地&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image040.png&#34; alt=&#34;image040&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：
数据写入到文件系统时进行文本序列化，且每列用^A来区分，\n为换行符。用more命令查看时不容易看出分割符，可以使用:sed -e &#39;s/\x01/|/g&#39; filename 来查看。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;导出数据到HDFS&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image042.png&#34; alt=&#34;image042&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_select&#34;&gt;SELECT&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list [HAVING condition]]
[CLUSTER BY col_list
  | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]
]
[LIMIT number]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注：
. order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。
. sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&amp;gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。
. distribute by根据distribute by指定的内容将数据分到同一个reducer。
. Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;获取年龄大的3个学生&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image044.png&#34; alt=&#34;image044&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查询学生信息按年龄，降序排序&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image046.png&#34; alt=&#34;image046&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image048.png&#34; alt=&#34;image048&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image050.png&#34; alt=&#34;image050&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image052.png&#34; alt=&#34;image052&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;按学生名称汇总学生年龄&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/hadoop/image0.png&#34; alt=&#34;image0&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_hive_join&#34;&gt;7.1.4. Hive Join&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;join_table:
  table_reference JOIN table_factor [join_condition]
  | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition
  | table_reference LEFT SEMI JOIN table_reference join_condition&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Hive 支持等值连接（equality joins）、外连接（outer joins）和（left/right joins）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。
另外，Hive 支持多于 2 个表的连接。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;写 join 查询时，需要注意几个关键点：
1. 只支持等值join
例如：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;  SELECT a.* FROM a JOIN b ON (a.id = b.id)
  SELECT a.* FROM a JOIN b
    ON (a.id = b.id AND a.department = b.department)
是正确的，然而:
  SELECT a.* FROM a JOIN b ON (a.id&amp;gt;b.id)
是错误的。&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;可以 join 多于 2 个表。
例如
  SELECT a.val, b.val, c.val FROM a JOIN b
    ON (a.key = b.key1) JOIN c ON (c.key = b.key2)
如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，例如：
  SELECT a.val, b.val, c.val FROM a JOIN b
    ON (a.key = b.key1) JOIN c
    ON (c.key = b.key1)
被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key。
SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1)
  JOIN c ON (c.key = b.key2)
而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 join。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;3．join 时，每次 map/reduce 任务的逻辑：
    reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如：
SELECT a.val, b.val, c.val FROM a
    JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)
所有表都使用同一个 join key（使用 1 次 map/reduce 任务计算）。Reduce 端会缓存 a 表和 b 表的记录，然后每次取得一个 c 表的记录就计算一次 join 结果，类似的还有：
  SELECT a.val, b.val, c.val FROM a
    JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)
这里用了 2 次 map/reduce 任务。第一次缓存 a 表，用 b 表序列化；第二次缓存第一次 map/reduce 任务的结果，然后用 c 表序列化。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;4．LEFT，RIGHT 和 FULL OUTER 关键字用于处理 join 中空记录的情况
例如：
  SELECT a.val, b.val FROM
a LEFT OUTER JOIN b ON (a.key=b.key)
对应所有 a 表中的记录都有一条记录输出。输出的结果应该是 a.val, b.val，当 a.key=b.key 时，而当 b.key 中找不到等值的 a.key 记录时也会输出:
a.val, NULL
所以 a 表中的所有记录都被保留了；
“a RIGHT OUTER JOIN b”会保留所有 b 表的记录。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Join 发生在 WHERE 子句之前。如果你想限制 join 的输出，应该在 WHERE 子句中写过滤条件——或是在 join 子句中写。这里面一个容易混淆的问题是表分区的情况：
  SELECT a.val, b.val FROM a
  LEFT OUTER JOIN b ON (a.key=b.key)
  WHERE a.ds=&#39;2009-07-07&#39; AND b.ds=&#39;2009-07-07&#39;
会 join a 表到 b 表（OUTER JOIN），列出 a.val 和 b.val 的记录。WHERE 从句中可以使用其他列作为过滤条件。但是，如前所述，如果 b 表中找不到对应 a 表的记录，b 表的所有列都会列出 NULL，包括 ds 列。也就是说，join 会过滤 b 表中不能找到匹配 a 表 join key 的所有记录。这样的话，LEFT OUTER 就使得查询结果与 WHERE 子句无关了。解决的办法是在 OUTER JOIN 时使用以下语法：
  SELECT a.val, b.val FROM a LEFT OUTER JOIN b
  ON (a.key=b.key AND
      b.ds=&#39;2009-07-07&#39; AND
      a.ds=&#39;2009-07-07&#39;)
这一查询的结果是预先在 join 阶段过滤过的，所以不会存在上述问题。这一逻辑也可以应用于 RIGHT 和 FULL 类型的 join 中。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hive</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hive/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hive&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hive只在一个节点上安装即可&#34;&gt;1. Hive只在一个节点上安装即可&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive只在一个节点上安装即可&#34;&gt;1. Hive只在一个节点上安装即可&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;上传tar包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf hive-1.2.1.tar.gz -C /hadoop/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装mysql数据库（切换到root用户）（装在哪里没有限制，只有能联通hadoop集群的节点）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;mysql.yml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;version: &#39;2&#39;
services:
  mysql:
    image: dishui.io:5000/mysql:5.5.52
    container_name: mysql
    environment:
      - &#34;MYSQL_ROOT_PASSWORD=111111&#34;
    ports:
      - &#34;3306:3306&#34;
    networks:
      - hadoop
networks:
  hadoop:
    external: true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置 &lt;strong&gt;hive&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;配置 &lt;strong&gt;HIVE_HOME&lt;/strong&gt; 环境变量  &lt;strong&gt;vi conf/hive-env.sh&lt;/strong&gt; 配置其中的 &lt;strong&gt;$hadoop_home&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置元数据库信息   &lt;strong&gt;vi hive-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;jdbc:mysql://mysql:3306/hive?createDatabaseIfNotExist=true&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;JDBC connect string for a JDBC metastore&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;Driver class name for a JDBC metastore&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;root&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;username to use against metastore database&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;111111&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;password to use against metastore database&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装 &lt;strong&gt;hive&lt;/strong&gt; 和 &lt;strong&gt;mysql&lt;/strong&gt; 完成后，将 &lt;strong&gt;mysql&lt;/strong&gt; 的连接 &lt;strong&gt;jar&lt;/strong&gt; 包拷贝到 &lt;strong&gt;$HIVE_HOME/lib&lt;/strong&gt; 目录下&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果出现没有权限的问题，在 &lt;strong&gt;mysql&lt;/strong&gt; 授权(在安装 &lt;strong&gt;mysql&lt;/strong&gt; 的机器上执行)&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mysql -uroot -p&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;执行下面的语句  &lt;strong&gt;.&lt;/strong&gt;:所有库下的所有表   %：任何IP地址或主机都可以连接&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;111111&#39; WITH GRANT OPTION;
FLUSH PRIVILEGES;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Jline&lt;/strong&gt; 包版本不一致的问题，需要拷贝 &lt;strong&gt;hive&lt;/strong&gt; 的 &lt;strong&gt;lib&lt;/strong&gt; 目录中 &lt;strong&gt;jline.2.12.jar&lt;/strong&gt; 的 &lt;strong&gt;jar&lt;/strong&gt; 包替换掉 &lt;strong&gt;hadoop&lt;/strong&gt; 中的
&lt;strong&gt;/home/hadoop/app/hadoop-2.6.4/share/hadoop/yarn/lib/jline-0.9.94.jar&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动 &lt;strong&gt;hive&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hive&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;建表(默认是内部表)&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;create table trade_detail(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39;;
# 建分区表
create table td_part(id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by &#39;\t&#39;;
# 建外部表
create external table td_ext(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39; location &#39;/td_ext&#39;;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建分区表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 普通表和分区表区别：有大量数据增加的需要建分区表
create table book (id bigint, name string) partitioned by (pubdate string) row format delimited fields terminated by &#39;\t&#39;;

# 分区表加载数据
load data local inpath &#39;./book.txt&#39; overwrite into table book partition (pubdate=&#39;2010-08-22&#39;);

load data local inpath &#39;/root/data.am&#39; into table beauty partition (nation=&#34;USA&#34;);

select nation, avg(size) from beauties group by nation order by avg(size);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hadoop-shell</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop-shell/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop-shell/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hadoop-shell&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hdfs&#34;&gt;1. hdfs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hdfs&#34;&gt;1. hdfs&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;列表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -ls /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -put /test.ee /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看文件内容&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -cat /test.ee&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;下载文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -get /test.ee /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建目录&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -mkdir -p /wordcount/input&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hadoop2.4.1集群搭建</title>
      <link>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop2/</link>
      <pubDate>Sun, 12 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop2/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hadoop&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#__strong_centos_strong_安装&#34;&gt;1. &lt;strong&gt;CentOS&lt;/strong&gt; 安装&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_准备linux环境&#34;&gt;1.1. 准备Linux环境&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_安装_strong_jdk_strong&#34;&gt;1.2. 安装 &lt;strong&gt;JDK&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_安装_strong_hadoop2_4_1_strong&#34;&gt;1.3. 安装 &lt;strong&gt;hadoop2.4.1&lt;/strong&gt;&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_将_strong_hadoop_strong_添加到环境变量&#34;&gt;1.3.1. 将 &lt;strong&gt;hadoop&lt;/strong&gt; 添加到环境变量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_格式化_strong_namenode_strong_是对_strong_namenode_strong_进行初始化&#34;&gt;1.3.2. 格式化 &lt;strong&gt;namenode&lt;/strong&gt;（是对 &lt;strong&gt;namenode&lt;/strong&gt; 进行初始化）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_启动_strong_hadoop_strong&#34;&gt;1.3.3. 启动 &lt;strong&gt;hadoop&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_验证是否启动成功&#34;&gt;1.3.4. 验证是否启动成功&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_配置ssh免登陆&#34;&gt;1.4. 配置ssh免登陆&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_docker_方式&#34;&gt;2. docker 方式&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_compose&#34;&gt;2.1. compose&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_example&#34;&gt;3. Example&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_wordcount&#34;&gt;3.1. wordcount&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hdfs_文件上传流程&#34;&gt;4. hdfs 文件上传流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_参考&#34;&gt;5. 参考&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;__strong_centos_strong_安装&#34;&gt;1. &lt;strong&gt;CentOS&lt;/strong&gt; 安装&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_准备linux环境&#34;&gt;1.1. 准备Linux环境&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;先将虚拟机的网络模式选为NAT&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改主机名&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vi /etc/sysconfig/network

NETWORKING=yes
HOSTNAME=mini1    ###&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改IP&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/sysconfig/network-scripts/ifcfg-eth0

DEVICE=&#34;eth0&#34;
BOOTPROTO=&#34;static&#34;               ###
HWADDR=&#34;00:0C:29:3C:BF:E7&#34;
IPV6INIT=&#34;yes&#34;
NM_CONTROLLED=&#34;yes&#34;
ONBOOT=&#34;yes&#34;
TYPE=&#34;Ethernet&#34;
UUID=&#34;ce22eeca-ecde-4536-8cc2-ef0dc36d4a8c&#34;
IPADDR=&#34;192.168.1.101&#34;           ###
NETMASK=&#34;255.255.255.0&#34;          ###
GATEWAY=&#34;192.168.1.1&#34;            ###&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改主机名和IP的映射关系&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/hosts

192.168.1.101   itcast&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;关闭防火墙&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#查看防火墙状态
service iptables status
#关闭防火墙
service iptables stop
#查看防火墙开机启动状态
chkconfig iptables --list
#关闭防火墙开机启动
chkconfig iptables off&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改sudo&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su root
vim /etc/sudoers
给hadoop用户添加执行的权限&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装_strong_jdk_strong&#34;&gt;1.2. 安装 &lt;strong&gt;JDK&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;上传 &lt;strong&gt;jdk-7u_65-i585.tar.gz&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压 &lt;strong&gt;jdk&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#创建文件夹
mkdir /home/hadoop/app
#解压
tar -zxvf jdk-7u55-linux-i586.tar.gz -C /home/hadoop/app&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将 &lt;strong&gt;java&lt;/strong&gt; 添加到环境变量中&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/profile
#在文件最后添加
export JAVA_HOME=/home/hadoop/app/jdk-7u_65-i585
export PATH=$PATH:$JAVA_HOME/bin

#刷新配置
source /etc/profile&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装_strong_hadoop2_4_1_strong&#34;&gt;1.3. 安装 &lt;strong&gt;hadoop2.4.1&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&#34;admonitionblock tip&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-tip&#34; title=&#34;Tip&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
先上传 hadoop 的安装包到服务器上去 /home/hadoop/&lt;br&gt;
注意：hadoop2.x 的配置文件 $HADOOP_HOME/etc/hadoop&lt;br&gt;
伪分布式需要修改5个配置文件
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;配置 hadoop&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hadoop-env.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim hadoop-env.sh
#第27行
export JAVA_HOME=/usr/java/jdk1.7.0_65&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;core-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://weekend-1206-01:9000&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;!-- 指定hadoop运行时产生文件的存储目录 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/home/hadoop/hadoop-2.4.1/tmp&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;hdfs-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定HDFS副本的数量 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.secondary.http.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;192.168.1.152:50090&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;mapred-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mv mapred-site.xml.template mapred-site.xml
vim mapred-site.xml
&amp;lt;!-- 指定mr运行在yarn上 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;yarn-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定YARN的老大（ResourceManager）的地址 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.hostname&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;weekend-1206-01&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;!-- reducer获取数据的方式 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_将_strong_hadoop_strong_添加到环境变量&#34;&gt;1.3.1. 将 &lt;strong&gt;hadoop&lt;/strong&gt; 添加到环境变量&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/proflie

export JAVA_HOME=/usr/java/jdk1.7.0_65
export HADOOP_HOME=/itcast/hadoop-2.4.1
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

source /etc/profile&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_格式化_strong_namenode_strong_是对_strong_namenode_strong_进行初始化&#34;&gt;1.3.2. 格式化 &lt;strong&gt;namenode&lt;/strong&gt;（是对 &lt;strong&gt;namenode&lt;/strong&gt; 进行初始化）&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hdfs namenode -format
或
hadoop namenode -format&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_启动_strong_hadoop_strong&#34;&gt;1.3.3. 启动 &lt;strong&gt;hadoop&lt;/strong&gt;&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 先启动HDFS
sbin/start-dfs.sh

# 再启动YARN
sbin/start-yarn.sh

# 启动 namenode
sbin/hadoop-daemon.sh start namenode

# 启动 dataNode
sbin/hadoop-daemon.sh start datanode&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_验证是否启动成功&#34;&gt;1.3.4. 验证是否启动成功&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# jps
27408 NameNode
28218 Jps
27643 SecondaryNameNode
28066 NodeManager
27803 ResourceManager
27512 DataNode&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;http://192.168.1.101:50070 （HDFS管理界面）
http://192.168.1.101:8088 （MR管理界面）&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_配置ssh免登陆&#34;&gt;1.4. 配置ssh免登陆&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#生成ssh免登陆密钥
#进入到我的home目录
cd ~/.ssh

ssh-keygen -t rsa （四个回车）
# 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）
# 将公钥拷贝到要免密登陆的目标机器上
ssh-copy-id localhost
# ssh免登陆：
# 生成key:
ssh-keygen
# 复制从A复制到B上:
ssh-copy-id B
# 验证：
ssh localhost/exit，ps -e|grep ssh
ssh A  #在B中执行&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_docker_方式&#34;&gt;2. docker 方式&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_compose&#34;&gt;2.1. compose&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hadoop.yml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;version: &#39;2&#39;
services:
  mini1:
    image: dishui.io:5000/sequenceiq/hadoop-docker:2.4.1
    container_name: mini1
    environment:
      - &#34;HOSTNAME=mini1&#34;
    ports:
      - &#34;50070:50070&#34;
      - &#34;8088:8088&#34;
    volumes:
      - ./core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml
      - ./yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop.sh:/etc/profile.d/hadoop.sh
    command: /etc/bootstrap.sh -d
    networks:
      - hadoop
  mini2:
    image: dishui.io:5000/sequenceiq/hadoop-docker:2.4.1
    container_name: mini2
    environment:
      - &#34;HOSTNAME=mini1&#34;
    volumes:
      - ./core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml
      - ./yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop.sh:/etc/profile.d/hadoop.sh
    command: /usr/sbin/sshd -d
    networks:
      - hadoop

networks:
  hadoop:
    driver: bridge&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;core-site.xml&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;hdfs://mini1:9000&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;/hadoop-2.4.1/tmp&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ps: &lt;code&gt;cd $HADOOP_PREFIX&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;hadoop.sh&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;export PATH=$HADOOP_PREFIX/bin:HADOOP_PREFIX/sbin:$PATH&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_example&#34;&gt;3. Example&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_wordcount&#34;&gt;3.1. wordcount&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;input&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;a.txt&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;a
b
abc
ef
efg
abc
ef
h
aakk
ef
h&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传到 &lt;strong&gt;HDFS&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 目录不存在,创建目录 ( `hadoop fs -mkdir -p /wordcount/input` )
hadoop fs -put /a.txt /wordcount/input&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hdfs_文件上传流程&#34;&gt;4. hdfs 文件上传流程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/post/bigdata/hadoop/hadoop-upload.svg&#34; alt=&#34;hadoop upload&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_参考&#34;&gt;5. 参考&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kiwenlau/hadoop-cluster-docker&#34;&gt;hadoop-cluster-docker&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>多元函数</title>
      <link>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B0/</link>
      <pubDate>Thu, 09 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B0/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;多元函数&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_多元函数&#34;&gt;1. 多元函数&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_二元函数&#34;&gt;1.1. 二元函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_二元函数图形&#34;&gt;1.2. 二元函数图形&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_复杂的二元函数&#34;&gt;1.3. 复杂的二元函数&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_多元函数的极限和连续性&#34;&gt;2. 多元函数的极限和连续性&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_一元函数的极限&#34;&gt;2.1. 一元函数的极限&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_二元函数的极限&#34;&gt;2.2. 二元函数的极限&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_多元函数&#34;&gt;1. 多元函数&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_二元函数&#34;&gt;1.1. 二元函数&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_084154.png&#34; alt=&#34;2017 03 09 084154&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_084307.png&#34; alt=&#34;2017 03 09 084307&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_084436.png&#34; alt=&#34;2017 03 09 084436&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_085405.png&#34; alt=&#34;2017 03 09 085405&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_085445.png&#34; alt=&#34;2017 03 09 085445&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_二元函数图形&#34;&gt;1.2. 二元函数图形&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_085648.png&#34; alt=&#34;2017 03 09 085648&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_085710.png&#34; alt=&#34;2017 03 09 085710&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_085736.png&#34; alt=&#34;2017 03 09 085736&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_085810.png&#34; alt=&#34;2017 03 09 085810&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_085829.png&#34; alt=&#34;2017 03 09 085829&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_复杂的二元函数&#34;&gt;1.3. 复杂的二元函数&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_085844.png&#34; alt=&#34;2017 03 09 085844&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_085922.png&#34; alt=&#34;2017 03 09 085922&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_085943.png&#34; alt=&#34;2017 03 09 085943&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_090004.png&#34; alt=&#34;2017 03 09 090004&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_090044.png&#34; alt=&#34;2017 03 09 090044&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_090320.png&#34; alt=&#34;2017 03 09 090320&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_多元函数的极限和连续性&#34;&gt;2. 多元函数的极限和连续性&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_一元函数的极限&#34;&gt;2.1. 一元函数的极限&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_090540.png&#34; alt=&#34;2017 03 09 090540&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_二元函数的极限&#34;&gt;2.2. 二元函数的极限&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_090750.png&#34; alt=&#34;2017 03 09 090750&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_090911.png&#34; alt=&#34;2017 03 09 090911&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_090935.png&#34; alt=&#34;2017 03 09 090935&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-09_091037.png&#34; alt=&#34;2017 03 09 091037&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hugo</title>
      <link>http://dishui.oschina.io/note-hugo/post/hugo/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/hugo/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hugo&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hugo&#34;&gt;1. hugo&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_设置发布目录&#34;&gt;1.1. 设置发布目录&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_部署&#34;&gt;1.2. 部署&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_本地部署&#34;&gt;1.2.1. 本地部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_码云部署&#34;&gt;1.2.2. 码云部署&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_生成_strong_lunr_strong_索引&#34;&gt;2. 生成 &lt;strong&gt;lunr&lt;/strong&gt; 索引&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_参考&#34;&gt;3. 参考&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hugo&#34;&gt;1. hugo&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_设置发布目录&#34;&gt;1.1. 设置发布目录&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 创建 gh-pages 分支
git checkout --orphan gh-pages
#
git worktree add public gh-pages&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_部署&#34;&gt;1.2. 部署&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_本地部署&#34;&gt;1.2.1. 本地部署&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hugo-local.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#!/bin/sh

cd /e/note-hugo/

# 替换域名
echo &#34;替换域名&#34;
git checkout -- themes/mainroad/static/js/app.js content/post/base.adoc config.toml
sed -i &#39;s/dishui.oschina.io\/note-hugo/localhost:1313/g&#39; themes/mainroad/static/js/app.js content/post/base.adoc config.toml
echo &#34;启动 hugo server&#34;
hugo server&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_码云部署&#34;&gt;1.2.2. 码云部署&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hugo.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#!/bin/sh

cd /e/note-hugo/

# 还原
echo &#34;恢复域名&#34;
git checkout -- themes/mainroad/static/js/app.js content/post/base.adoc config.toml

echo &#34;Generating site&#34;
hugo

echo &#34;Updating gh-pages branch&#34;
cd public &amp;amp;&amp;amp; git add --all &amp;amp;&amp;amp; git commit -m &#34;$(date +%Y-%m-%d)&#34; &amp;amp;&amp;amp; git push

echo &#34;Commit note-hugo&#34;
cd .. &amp;amp;&amp;amp; git add --all &amp;amp;&amp;amp; git commit -m &#34;$(date +%Y-%m-%d)&#34; &amp;amp;&amp;amp; git push&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_生成_strong_lunr_strong_索引&#34;&gt;2. 生成 &lt;strong&gt;lunr&lt;/strong&gt; 索引&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;启动 &lt;strong&gt;nodejieba&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;启动 &lt;strong&gt;docker-volume-netshare&lt;/strong&gt; ( &lt;a href=&#34;http://dishui.oschina.io/note-hugo/post/docker/docker-base/#_mount_aws_efs_nfs_or_cifs_samba_volumes_in_docker&#34;&gt;docker-volume-netshare&lt;/a&gt; )&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动 &lt;strong&gt;nodejieba&lt;/strong&gt; 容器&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;docker run -it --name=lunr --volume-driver=cifs -v 192.168.137.2/note-hugo:/hugo dishui.io:5000/nodejieba:1.1 /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;构建索引&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /hugo/content &amp;amp;&amp;amp; \
nodejs hugo-lunr.js &amp;amp;&amp;amp; \ # 生成原始数据
nodejs index_builder.js # 生成索引&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_参考&#34;&gt;3. 参考&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://gohugo.io/tutorials/github-pages-blog/&#34;&gt;Deployment via gh-pages branch&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>欧拉方程</title>
      <link>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E6%AC%A7%E6%8B%89%E6%96%B9%E7%A8%8B/</link>
      <pubDate>Mon, 06 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E6%AC%A7%E6%8B%89%E6%96%B9%E7%A8%8B/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;欧拉方程&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_欧拉方程&#34;&gt;1. 欧拉方程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_欧拉方程&#34;&gt;1. 欧拉方程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-06_090630.png&#34; alt=&#34;2017 03 06 090630&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-06_090702.png&#34; alt=&#34;2017 03 06 090702&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-06_090730.png&#34; alt=&#34;2017 03 06 090730&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-06_090747.png&#34; alt=&#34;2017 03 06 090747&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-06_090812.png&#34; alt=&#34;2017 03 06 090812&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-06_090838.png&#34; alt=&#34;2017 03 06 090838&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-06_090854.png&#34; alt=&#34;2017 03 06 090854&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-06_090907.png&#34; alt=&#34;2017 03 06 090907&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-06_090926.png&#34; alt=&#34;2017 03 06 090926&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-06_090955.png&#34; alt=&#34;2017 03 06 090955&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>log</title>
      <link>http://dishui.oschina.io/note-hugo/post/linux/log/</link>
      <pubDate>Tue, 28 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/linux/log/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Log&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_elk&#34;&gt;1. ELK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_rsyslog&#34;&gt;2. Rsyslog&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_supervisord发送log日志到rsyslog&#34;&gt;2.1. Supervisord发送log日志到Rsyslog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_logrotate_日志分割&#34;&gt;3. Logrotate 日志分割&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_dockerfile&#34;&gt;3.1. Dockerfile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_脚本&#34;&gt;3.2. 脚本&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_配置文件&#34;&gt;3.3. 配置文件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_定时器&#34;&gt;3.4. 定时器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_elk&#34;&gt;1. ELK&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;下载&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;参考 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/gaoyingju/article/details/23750563&#34;&gt;开源分布式搜索平台ELK(Elasticsearch+Logstash+Kibana)入门学习资源索引&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.linuxea.com/1204.html&#34;&gt;日志实时收集分析-ELK Stack&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_rsyslog&#34;&gt;2. Rsyslog&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_supervisord发送log日志到rsyslog&#34;&gt;2.1. Supervisord发送log日志到Rsyslog&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;supervisor conf &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;[eventlistener:logging]
command = supervisor_logging
events = PROCESS_LOG

[program:wuliu]
command=/apache-tomcat-7.0.62/bin/catalina.sh run
user=root
autostart=true
stdout_events_enabled = true
stderr_events_enabled = true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;rsyslog server &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ModLoad imudp
$UDPServerRun 514 #开启UDP

$ModLoad imtcp
$InputTCPServerRun 514 #开启TCP

#$template TraditionalFormat,&#34;%programname% %syslogtag% %msg%\n&#34;
#$template TraditionalFormat,&#34;%programname% %hostname% %msg%\n&#34;
$template TraditionalFormat,&#34;%fromhost-ip% %msg%\n&#34;


$ActionFileDefaultTemplate TraditionalFormat # 默认日志格式模板与client端发送的日志格式模板保持一致

$template Remote, &#34;~/log/%fromhost-ip%.log&#34;

:fromhost-ip, !isequal, &#34;127.0.0.1&#34; ?Remote

#:syslogtag, isequal, &#34;wuliu-log&#34; ?Remote
#:syslogtag, isequal, &#34;wuliu-error-log&#34; ?Remote-error&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;参考 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/hanyifeng/p/5463338.html&#34;&gt;Linux 之 rsyslog 系统日志转发&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.biglog.cn/tomcat-syslog/&#34;&gt;Tomcat日志配置远程Syslog采集&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_logrotate_日志分割&#34;&gt;3. Logrotate 日志分割&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;参考 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://blog.sina.com.cn/s/blog_5f54f0be0101h6y8.html&#34;&gt;Logrotate 日志分割&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_dockerfile&#34;&gt;3.1. Dockerfile&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;FROM blacklabelops/centos

RUN yum install -y \
    vi rsyslog &amp;amp;&amp;amp; \
    yum install crontabs -y &amp;amp;&amp;amp; \
    yum clean all &amp;amp;&amp;amp; rm -rf /var/cache/yum/* &amp;amp;&amp;amp; \
    echo &#34;30 22 * * * . /usr/sbin/logrotate /etc/logrotate.conf&#34; &amp;gt;&amp;gt; /var/spool/cron/root &amp;amp;&amp;amp; \
    mkdir -p ~/log

COPY ./docker-entrypoint.sh /docker-entrypoint.sh
COPY ./rsyslog.conf /etc/rsyslog.d/listen.conf
COPY ./wuliu /etc/logrotate.d/wuliu

EXPOSE 514
ENTRYPOINT [&#34;/docker-entrypoint.sh&#34;]
CMD [&#34;rsyslogd&#34;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_脚本&#34;&gt;3.2. 脚本&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#!/bin/bash -x
#
# A helper script for ENTRYPOINT.

set -e

if [ &#34;$1&#34; = &#39;rsyslogd&#39; ]; then
  crond
  rsyslogd -n
fi

exec &#34;$@&#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_配置文件&#34;&gt;3.3. 配置文件&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;vi /etc/logrotate.d/wuliu&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;~/log/wuliu.log {
  daily
  rotate 5
  sharedscripts
  postrotate
    /bin/kill -HUP `cat /var/run/syslogd.pid 2&amp;gt; /dev/null` 2&amp;gt; /dev/null || true
    /bin/kill -HUP `cat /var/run/rsyslogd.pid 2&amp;gt; /dev/null` 2&amp;gt; /dev/null || true
  endscript
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_定时器&#34;&gt;3.4. 定时器&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 每天晚上 10点30 执行一次
30 22 * * * . /usr/sbin/logrotate /etc/logrotate.conf&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>一阶线性微分方程</title>
      <link>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E4%B8%80%E9%98%B6%E7%BA%BF%E6%80%A7%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</link>
      <pubDate>Sat, 25 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E4%B8%80%E9%98%B6%E7%BA%BF%E6%80%A7%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;一阶线性微分方程&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_一阶线性微分方程&#34;&gt;1. 一阶线性微分方程&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_常数变易法&#34;&gt;1.1. 常数变易法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_一阶线性方程的解的结构定理&#34;&gt;1.2. 一阶线性方程的解的结构定理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_例子&#34;&gt;1.3. 例子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_一阶微分方程总结&#34;&gt;1.4. 一阶微分方程总结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_一阶线性微分方程&#34;&gt;1. 一阶线性微分方程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102207.png&#34; alt=&#34;2017 02 25 102207&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102339.png&#34; alt=&#34;2017 02 25 102339&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102423.png&#34; alt=&#34;2017 02 25 102423&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102520.png&#34; alt=&#34;2017 02 25 102520&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102548.png&#34; alt=&#34;2017 02 25 102548&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102652.png&#34; alt=&#34;2017 02 25 102652&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102724.png&#34; alt=&#34;2017 02 25 102724&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102820.png&#34; alt=&#34;2017 02 25 102820&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102844.png&#34; alt=&#34;2017 02 25 102844&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102900.png&#34; alt=&#34;2017 02 25 102900&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_常数变易法&#34;&gt;1.1. 常数变易法&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_102950.png&#34; alt=&#34;2017 02 25 102950&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_103054.png&#34; alt=&#34;2017 02 25 103054&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_一阶线性方程的解的结构定理&#34;&gt;1.2. 一阶线性方程的解的结构定理&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_103151.png&#34; alt=&#34;2017 02 25 103151&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_例子&#34;&gt;1.3. 例子&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_104309.png&#34; alt=&#34;2017 02 25 104309&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_104504.png&#34; alt=&#34;2017 02 25 104504&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_104537.png&#34; alt=&#34;2017 02 25 104537&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_110352.png&#34; alt=&#34;2017 02 25 110352&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_110431.png&#34; alt=&#34;2017 02 25 110431&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_110454.png&#34; alt=&#34;2017 02 25 110454&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_110556.png&#34; alt=&#34;2017 02 25 110556&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_110718.png&#34; alt=&#34;2017 02 25 110718&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_一阶微分方程总结&#34;&gt;1.4. 一阶微分方程总结&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-26_093702.png&#34; alt=&#34;2017 02 26 093702&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-26_093730.png&#34; alt=&#34;2017 02 26 093730&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>伯努利方程</title>
      <link>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E4%BC%AF%E5%8A%AA%E5%88%A9%E6%96%B9%E7%A8%8B/</link>
      <pubDate>Sat, 25 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E4%BC%AF%E5%8A%AA%E5%88%A9%E6%96%B9%E7%A8%8B/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;伯努利方程&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_伯努利方程&#34;&gt;1. 伯努利方程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_伯努利方程&#34;&gt;1. 伯努利方程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-02-25_175041.png&#34; alt=&#34;2017 02 25 175041&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>向量及其线性运算</title>
      <link>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E5%90%91%E9%87%8F%E5%8F%8A%E5%85%B6%E7%BA%BF%E6%80%A7%E8%BF%90%E7%AE%97/</link>
      <pubDate>Sat, 25 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E5%90%91%E9%87%8F%E5%8F%8A%E5%85%B6%E7%BA%BF%E6%80%A7%E8%BF%90%E7%AE%97/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;向量及其线性运算&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_向量及其线性运算&#34;&gt;1. 向量及其线性运算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_模_方向角_方向余弦&#34;&gt;2. 模 方向角 方向余弦&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_向量的投影&#34;&gt;3. 向量的投影&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_夹角公式&#34;&gt;4. 夹角公式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_向量及其线性运算&#34;&gt;1. 向量及其线性运算&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_090305.png&#34; alt=&#34;2017 03 07 090305&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_091205.png&#34; alt=&#34;2017 03 07 091205&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_091508.png&#34; alt=&#34;2017 03 07 091508&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_091539.png&#34; alt=&#34;2017 03 07 091539&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_091635.png&#34; alt=&#34;2017 03 07 091635&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_092511.png&#34; alt=&#34;2017 03 07 092511&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_模_方向角_方向余弦&#34;&gt;2. 模 方向角 方向余弦&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093123.png&#34; alt=&#34;2017 03 07 093123&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093235.png&#34; alt=&#34;2017 03 07 093235&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093335.png&#34; alt=&#34;2017 03 07 093335&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093447.png&#34; alt=&#34;2017 03 07 093447&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093542.png&#34; alt=&#34;2017 03 07 093542&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093618.png&#34; alt=&#34;2017 03 07 093618&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093637.png&#34; alt=&#34;2017 03 07 093637&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093711.png&#34; alt=&#34;2017 03 07 093711&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093725.png&#34; alt=&#34;2017 03 07 093725&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093741.png&#34; alt=&#34;2017 03 07 093741&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093802.png&#34; alt=&#34;2017 03 07 093802&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093820.png&#34; alt=&#34;2017 03 07 093820&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093837.png&#34; alt=&#34;2017 03 07 093837&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_093856.png&#34; alt=&#34;2017 03 07 093856&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_向量的投影&#34;&gt;3. 向量的投影&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_124618.png&#34; alt=&#34;2017 03 07 124618&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_124717.png&#34; alt=&#34;2017 03 07 124717&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_124745.png&#34; alt=&#34;2017 03 07 124745&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_124826.png&#34; alt=&#34;2017 03 07 124826&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_夹角公式&#34;&gt;4. 夹角公式&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_125417.png&#34; alt=&#34;2017 03 07 125417&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_125453.png&#34; alt=&#34;2017 03 07 125453&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_130933.png&#34; alt=&#34;2017 03 07 130933&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;http://dishui.oschina.io/note-hugo/src/img/gaoshu/2017-03-07_131227.png&#34; alt=&#34;2017 03 07 131227&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>高阶线性微分方程</title>
      <link>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E9%AB%98%E9%98%B6%E7%BA%BF%E6%80%A7%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</link>
      <pubDate>Sat, 25 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://dishui.oschina.io/note-hugo/post/mathematics/xuxiaozhan/%E9%AB%98%E9%98%B6%E7%BA%BF%E6%80%A7%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</guid>
      <description>&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_高阶线性微分方程&#34;&gt;高阶线性微分方程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_齐次线性方程的解的叠加原理&#34;&gt;齐次线性方程的解的叠加原理&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;image::{img}/img/gaoshu/2017-03-02_090230.png[]&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;image::{img}/img/gaoshu/2017-03-02_090328.png[]&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;image::{img}/img/gaoshu/2017-03-02_090346.png[]&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;image::{img}/img/gaoshu/2017-03-02_090409.png[]&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;image::{img}/img/gaoshu/2017-03-02_090619.png[]&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>