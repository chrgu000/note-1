<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>工作笔记</title>
    <link>/index.xml</link>
    <description>Recent content on 工作笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 01 Apr 2017 09:31:48 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Kafka负载均衡-自定义Partition-文件存储机制</title>
      <link>/post/bigdata/storm/Kafka%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-%E8%87%AA%E5%AE%9A%E4%B9%89Partition-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/</link>
      <pubDate>Sat, 01 Apr 2017 09:31:48 +0000</pubDate>
      
      <guid>/post/bigdata/storm/Kafka%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-%E8%87%AA%E5%AE%9A%E4%B9%89Partition-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;kafka&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka整体结构图&#34;&gt;1. Kafka整体结构图&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_consumer与topic关系&#34;&gt;2. Consumer与topic关系&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka消息的分发&#34;&gt;3. Kafka消息的分发&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_consumer的负载均衡&#34;&gt;4. Consumer的负载均衡&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka文件存储机制&#34;&gt;5. kafka文件存储机制&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka文件存储基本结构&#34;&gt;5.1. Kafka文件存储基本结构&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka_partition_segment&#34;&gt;5.2. Kafka Partition Segment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka_查找message&#34;&gt;5.3. Kafka 查找message&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_查找segment_file&#34;&gt;5.3.1. 查找segment file&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka自定义partition&#34;&gt;6. Kafka自定义Partition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka整体结构图&#34;&gt;1. Kafka整体结构图&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Kafka名词解释和工作方式&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Producer &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;消息生产者，就是向kafka broker发消息的客户端。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Consumer &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;消息消费者，向kafka broker取消息的客户端&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Topic &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;咋们可以理解为一个队列。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Consumer Group （CG）&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;这是 &lt;strong&gt;kafka&lt;/strong&gt; 用来实现一个 &lt;strong&gt;topic&lt;/strong&gt; 消息的广播（发给所有的 &lt;strong&gt;consumer&lt;/strong&gt; ）和单播（发给任意一个 &lt;strong&gt;consumer&lt;/strong&gt; ）的手段。一个 &lt;strong&gt;topic&lt;/strong&gt; 可以有多个 &lt;strong&gt;CG&lt;/strong&gt; 。&lt;strong&gt;topic&lt;/strong&gt; 的消息会复制（不是真的复制，是概念上的）到所有的 &lt;strong&gt;CG&lt;/strong&gt; ，但每个 &lt;strong&gt;partion&lt;/strong&gt; 只会把消息发给该 &lt;strong&gt;CG&lt;/strong&gt; 中的一个 &lt;strong&gt;consumer&lt;/strong&gt; 。如果需要实现广播，只要每个 &lt;strong&gt;consumer&lt;/strong&gt; 有一个独立的 &lt;strong&gt;CG&lt;/strong&gt; 就可以了。要实现单播只要所有的 &lt;strong&gt;consumer&lt;/strong&gt; 在同一个 &lt;strong&gt;CG&lt;/strong&gt; 。用 &lt;strong&gt;CG&lt;/strong&gt; 还可以将 &lt;strong&gt;consumer&lt;/strong&gt; 进行自由的分组而不需要多次发送消息到不同的 &lt;strong&gt;topic&lt;/strong&gt; 。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Broker &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;一台 &lt;strong&gt;kafka&lt;/strong&gt; 服务器就是一个 &lt;strong&gt;broker&lt;/strong&gt; 。一个集群由多个 &lt;strong&gt;broker&lt;/strong&gt; 组成。一个 &lt;strong&gt;broker&lt;/strong&gt; 可以容纳多个 &lt;strong&gt;topic&lt;/strong&gt; 。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Partition&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;为了实现扩展性，一个非常大的 &lt;strong&gt;topic&lt;/strong&gt; 可以分布到多个 &lt;strong&gt;broker&lt;/strong&gt; （即服务器）上，一个 &lt;strong&gt;topic&lt;/strong&gt; 可以分为多个 &lt;strong&gt;partition&lt;/strong&gt; ，每个 &lt;strong&gt;partition&lt;/strong&gt; 是一个有序的队列。 &lt;strong&gt;partition&lt;/strong&gt; 中的每条消息都会被分配一个有序的 &lt;strong&gt;id&lt;/strong&gt; （ &lt;strong&gt;offset&lt;/strong&gt; ）。 &lt;strong&gt;kafka&lt;/strong&gt; 只保证按一个 &lt;strong&gt;partition&lt;/strong&gt; 中的顺序将消息发给 &lt;strong&gt;consumer&lt;/strong&gt; ，不保证一个 &lt;strong&gt;topic&lt;/strong&gt; 的整体（多个 &lt;strong&gt;partition&lt;/strong&gt; 间）的顺序。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Offset&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;strong&gt;kafka&lt;/strong&gt; 的存储文件都是按照 &lt;strong&gt;offset.kafka&lt;/strong&gt; 来命名，用 &lt;strong&gt;offset&lt;/strong&gt; 做名字的好处是方便查找。例如你想找位于2049的位置，只要找到 &lt;strong&gt;2048.kafka&lt;/strong&gt; 的文件即可。当然 &lt;strong&gt;the first offset&lt;/strong&gt; 就是 &lt;strong&gt;00000000000.kafka&lt;/strong&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_consumer与topic关系&#34;&gt;2. Consumer与topic关系&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;本质上 &lt;strong&gt;kafka&lt;/strong&gt; 只支持 &lt;strong&gt;Topic&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;每个 &lt;strong&gt;group&lt;/strong&gt; 中可以有多个 &lt;strong&gt;consumer&lt;/strong&gt; ，每个 &lt;strong&gt;consumer&lt;/strong&gt; 属于一个 &lt;strong&gt;consumer&lt;/strong&gt;   &lt;strong&gt;group&lt;/strong&gt; ；
　　通常情况下，一个 &lt;strong&gt;group&lt;/strong&gt; 中会包含多个 &lt;strong&gt;consumer&lt;/strong&gt; ，这样不仅可以提高 &lt;strong&gt;topic&lt;/strong&gt; 中消息的并发消费能力，而且还能提高&#34;故障容错&#34;性，如果 &lt;strong&gt;group&lt;/strong&gt; 中的某个 &lt;strong&gt;consumer&lt;/strong&gt; 失效那么其消费的 &lt;strong&gt;partitions&lt;/strong&gt; 将会有其他 &lt;strong&gt;consumer&lt;/strong&gt; 自动接管。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于 &lt;strong&gt;Topic&lt;/strong&gt; 中的一条特定的消息，只会被订阅此 &lt;strong&gt;Topic&lt;/strong&gt; 的每个 &lt;strong&gt;group&lt;/strong&gt; 中的其中一个 &lt;strong&gt;consumer&lt;/strong&gt; 消费，此消息不会发送给一个 &lt;strong&gt;group&lt;/strong&gt; 的多个 &lt;strong&gt;consumer&lt;/strong&gt; ；
　　那么一个 &lt;strong&gt;group&lt;/strong&gt; 中所有的 &lt;strong&gt;consumer&lt;/strong&gt; 将会交错的消费整个 &lt;strong&gt;Topic&lt;/strong&gt; ，每个 &lt;strong&gt;group&lt;/strong&gt; 中 &lt;strong&gt;consumer&lt;/strong&gt; 消息消费互相独立，我们可以认为一个 &lt;strong&gt;group&lt;/strong&gt; 是一个&#34;订阅&#34;者。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;kafka&lt;/strong&gt; 中,一个 &lt;strong&gt;partition&lt;/strong&gt; 中的消息只会被 &lt;strong&gt;group&lt;/strong&gt; 中的一个 &lt;strong&gt;consumer&lt;/strong&gt; 消费(同一时刻)；
一个 &lt;strong&gt;Topic&lt;/strong&gt; 中的每个 &lt;strong&gt;partions&lt;/strong&gt; ，只会被一个&#34;订阅者&#34;中的一个 &lt;strong&gt;consumer&lt;/strong&gt; 消费，不过一个 &lt;strong&gt;consumer&lt;/strong&gt; 可以同时消费多个 &lt;strong&gt;partitions&lt;/strong&gt; 中的消息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;kafka&lt;/strong&gt; 的设计原理决定,对于一个 &lt;strong&gt;topic&lt;/strong&gt; ，同一个 &lt;strong&gt;group&lt;/strong&gt; 中不能有多于 &lt;strong&gt;partitions&lt;/strong&gt; 个数的 &lt;strong&gt;consumer&lt;/strong&gt; 同时消费，否则将意味着某些 &lt;strong&gt;consumer&lt;/strong&gt; 将无法得到消息。
　　 &lt;strong&gt;kafka&lt;/strong&gt; 只能保证一个 &lt;strong&gt;partition&lt;/strong&gt; 中的消息被某个 &lt;strong&gt;consumer&lt;/strong&gt; 消费时是顺序的；事实上，从 &lt;strong&gt;Topic&lt;/strong&gt; 角度来说,当有多个 &lt;strong&gt;partitions&lt;/strong&gt; 时,消息仍不是全局有序的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka消息的分发&#34;&gt;3. Kafka消息的分发&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;Producer&lt;/strong&gt; 客户端负责消息的分发&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;kafka&lt;/strong&gt; 集群中的任何一个 &lt;strong&gt;broker&lt;/strong&gt; 都可以向 &lt;strong&gt;producer&lt;/strong&gt; 提供 &lt;strong&gt;metadata&lt;/strong&gt; 信息,这些 &lt;strong&gt;metadata&lt;/strong&gt; 中包含&#34;集群中存活的 &lt;strong&gt;servers&lt;/strong&gt; 列表 &lt;strong&gt;&#34;/&#34;partitions leader&lt;/strong&gt; 列表&#34;等信息；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当 &lt;strong&gt;producer&lt;/strong&gt; 获取到 &lt;strong&gt;metadata&lt;/strong&gt; 信息之后,  &lt;strong&gt;producer&lt;/strong&gt; 将会和 &lt;strong&gt;Topic&lt;/strong&gt; 下所有 &lt;strong&gt;partition&lt;/strong&gt;   &lt;strong&gt;leader&lt;/strong&gt; 保持 &lt;strong&gt;socket&lt;/strong&gt; 连接；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;消息由 &lt;strong&gt;producer&lt;/strong&gt; 直接通过 &lt;strong&gt;socket&lt;/strong&gt; 发送到 &lt;strong&gt;broker&lt;/strong&gt; ，中间不会经过任何&#34;路由层&#34;，事实上，消息被路由到哪个 &lt;strong&gt;partition&lt;/strong&gt; 上由 &lt;strong&gt;producer&lt;/strong&gt; 客户端决定；
　　比如可以采用&#34; &lt;strong&gt;random&lt;/strong&gt; &#34;&#34; &lt;strong&gt;key&lt;/strong&gt; - &lt;strong&gt;hash&lt;/strong&gt; &#34;&#34;轮询&#34;等,如果一个 &lt;strong&gt;topic&lt;/strong&gt; 中有多个 &lt;strong&gt;partitions&lt;/strong&gt; ,那么在 &lt;strong&gt;producer&lt;/strong&gt; 端实现&#34;消息均衡分发&#34;是必要的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;producer&lt;/strong&gt; 端的配置文件中,开发者可以指定 &lt;strong&gt;partition&lt;/strong&gt; 路由的方式。&lt;/p&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　Producer消息发送的应答机制
　　设置发送数据是否需要服务端的反馈,有三个值0,1,-1
　　0: producer不会等待broker发送ack
　　1: 当leader接收到消息之后发送ack
　　-1: 当所有的follower都同步消息成功后发送ack
    request.required.acks=0&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_consumer的负载均衡&#34;&gt;4. Consumer的负载均衡&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;当一个 &lt;strong&gt;group&lt;/strong&gt; 中,有 &lt;strong&gt;consumer&lt;/strong&gt; 加入或者离开时,会触发 &lt;strong&gt;partitions&lt;/strong&gt; 均衡.均衡的最终目的,是提升 &lt;strong&gt;topic&lt;/strong&gt; 的并发消费能力，步骤如下：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;假如 &lt;strong&gt;topic1&lt;/strong&gt; ,具有如下 &lt;strong&gt;partitions: P0,P1,P2,P3&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加入 &lt;strong&gt;group&lt;/strong&gt; 中,有如下 &lt;strong&gt;consumer: C1,C2&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;首先根据 &lt;strong&gt;partition&lt;/strong&gt; 索引号对 &lt;strong&gt;partitions&lt;/strong&gt; 排序:  &lt;strong&gt;P0,P1,P2,P3&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据 &lt;strong&gt;consumer.id&lt;/strong&gt; 排序:  &lt;strong&gt;C0,C1&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算倍数:  &lt;strong&gt;M = [P0,P1,P2,P3].size / [C0,C1].size&lt;/strong&gt; ,本例值 &lt;strong&gt;M=2&lt;/strong&gt; (向上取整)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;然后依次分配 &lt;strong&gt;partitions&lt;/strong&gt; :  &lt;strong&gt;C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104025.png&#34; alt=&#34;2017 04 01 104025&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka文件存储机制&#34;&gt;5. kafka文件存储机制&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka文件存储基本结构&#34;&gt;5.1. Kafka文件存储基本结构&lt;/h3&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;在 &lt;strong&gt;Kafka&lt;/strong&gt; 文件存储中，同一个 &lt;strong&gt;topic&lt;/strong&gt; 下有多个不同 &lt;strong&gt;partition&lt;/strong&gt; ，每个 &lt;strong&gt;partition&lt;/strong&gt; 为一个目录， &lt;strong&gt;partiton&lt;/strong&gt; 命名规则为 &lt;strong&gt;topic&lt;/strong&gt; 名称+有序序号，第一个 &lt;strong&gt;partiton&lt;/strong&gt; 序号从0开始，序号最大值为 &lt;strong&gt;partitions&lt;/strong&gt; 数量减1。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个 &lt;strong&gt;partion&lt;/strong&gt; (目录)相当于一个巨型文件被平均分配到多个大小相等 &lt;strong&gt;segment&lt;/strong&gt; (段)数据文件中。但每个段 &lt;strong&gt;segment file&lt;/strong&gt; 消息数量不一定相等，这种特性方便 &lt;strong&gt;old&lt;/strong&gt;   &lt;strong&gt;segment&lt;/strong&gt;   &lt;strong&gt;file&lt;/strong&gt; 快速被删除。默认保留7天的数据。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104135.png&#34; alt=&#34;2017 04 01 104135&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个 &lt;strong&gt;partiton&lt;/strong&gt; 只需要支持顺序读写就行了， &lt;strong&gt;segment&lt;/strong&gt; 文件生命周期由服务端配置参数决定。（什么时候创建，什么时候删除）&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104143.png&#34; alt=&#34;2017 04 01 104143&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;数据有序的讨论？&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;一个 &lt;strong&gt;partition&lt;/strong&gt; 的数据是否是有序的？&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;间隔性有序，不连续
针对一个 &lt;strong&gt;topic&lt;/strong&gt; 里面的数据，只能做到 &lt;strong&gt;partition&lt;/strong&gt; 内部有序，不能做到全局有序。
特别加入消费者的场景后，如何保证消费者消费的数据全局有序的？伪命题。
只有一种情况下才能保证全局有序？就是只有一个 &lt;strong&gt;partition&lt;/strong&gt; 。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka_partition_segment&#34;&gt;5.2. Kafka Partition Segment&lt;/h3&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Segment file&lt;/strong&gt; 组成：由2大部分组成，分别为 &lt;strong&gt;index&lt;/strong&gt;   &lt;strong&gt;file&lt;/strong&gt; 和 &lt;strong&gt;data&lt;/strong&gt;   &lt;strong&gt;file&lt;/strong&gt; ，此2个文件一一对应，成对出现，后缀 &lt;strong&gt;.index&lt;/strong&gt; 和 &lt;strong&gt;.log&lt;/strong&gt; 分别表示为 &lt;strong&gt;segment&lt;/strong&gt; 索引文件、数据文件。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104152.png&#34; alt=&#34;2017 04 01 104152&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Segment&lt;/strong&gt; 文件命名规则： &lt;strong&gt;partion&lt;/strong&gt; 全局的第一个 &lt;strong&gt;segment&lt;/strong&gt; 从0开始，后续每个 &lt;strong&gt;segment&lt;/strong&gt; 文件名为上一个 &lt;strong&gt;segment&lt;/strong&gt; 文件最后一条消息的 &lt;strong&gt;offset&lt;/strong&gt; 值。数值最大为64位 &lt;strong&gt;long&lt;/strong&gt; 大小，19位数字字符长度，没有数字用0填充。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中 &lt;strong&gt;message&lt;/strong&gt; 的物理偏移地址。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104159.png&#34; alt=&#34;2017 04 01 104159&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;3，497：当前log文件中的第几条信息，存放在磁盘上的那个地方&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;上述图中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。
其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;segment data file由许多message组成， qq物理结构如下：
关键字
解释说明&lt;/p&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;8 byte offset &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;4 byte message size &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;message大小&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;4 byte CRC32 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;用crc32校验message&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;1 byte “magic&#34; &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示本次发布Kafka服务程序协议版本号&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;1 byte “attributes&#34; &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示为独立版本、或标识压缩类型、或编码类型。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;4 byte key length &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示key的长度,当key为-1时，K byte key字段不填&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;K byte key &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;可选&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;value bytes payload &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;表示实际消息数据。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_kafka_查找message&#34;&gt;5.3. Kafka 查找message&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;读取offset=368776的message，需要通过下面2个步骤查找。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-04-01_104213.png&#34; alt=&#34;2017 04 01 104213&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_查找segment_file&#34;&gt;5.3.1. 查找segment file&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0
　　00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1
　　00000000000000737337.index的起始偏移量为737338=737337 + 1
　　其他后续文件依次类推。
以起始偏移量命名并排序这些文件，只要根据offset **二分查找**文件列表，就可以快速定位到具体文件。当offset=368776时定位到00000000000000368769.index和对应log文件。
5.3.2、通过segment file查找message
当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址
　　然后再通过00000000000000368769.log顺序查找直到offset=368776为止。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka自定义partition&#34;&gt;6. Kafka自定义Partition&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;见代码&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Kafka集群安装部署、Kafka生产者、Kafka消费者</title>
      <link>/post/bigdata/storm/kafka/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/storm/kafka/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;kafka&lt;/div&gt;
&lt;ul class=&#34;sectlevel0&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka是什么&#34;&gt;Kafka是什么&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_jms是什么&#34;&gt;JMS是什么&lt;/a&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_jms的基础&#34;&gt;1. JMS的基础&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_jms消息传输模型&#34;&gt;2. JMS消息传输模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_jms核心组件&#34;&gt;3. JMS核心组件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_常见的类jms消息服务器&#34;&gt;4. 常见的类JMS消息服务器&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_jms消息服务器_activemq&#34;&gt;4.1. JMS消息服务器 ActiveMQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_分布式消息中间件_metamorphosis&#34;&gt;4.2. 分布式消息中间件 Metamorphosis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_分布式消息中间件_rocketmq&#34;&gt;4.3. 分布式消息中间件 RocketMQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_其他mq&#34;&gt;4.4. 其他MQ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_为什么需要消息队列_重要&#34;&gt;为什么需要消息队列（重要）&lt;/a&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_用户注册的一般流程&#34;&gt;1. 用户注册的一般流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_用户注册的并行执行&#34;&gt;2. 用户注册的并行执行&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_用户注册的最终一致&#34;&gt;3. 用户注册的最终一致&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka核心组件&#34;&gt;Kafka核心组件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka集群部署&#34;&gt;Kafka集群部署&lt;/a&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_集群部署的基本流程&#34;&gt;1. 集群部署的基本流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_集群部署的基础环境准备&#34;&gt;2. 集群部署的基础环境准备&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka集群部署_2&#34;&gt;3. Kafka集群部署&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_下载安装包&#34;&gt;3.1. 下载安装包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_解压安装包&#34;&gt;3.2. 解压安装包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_修改配置文件&#34;&gt;3.3. 修改配置文件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_分发安装包&#34;&gt;3.4. 分发安装包&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_再次修改配置文件_重要&#34;&gt;3.5. 再次修改配置文件（重要）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_启动集群&#34;&gt;3.6. 启动集群&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka生产者java_api&#34;&gt;Kafka生产者Java API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_kafka消费者java_api&#34;&gt;Kafka消费者Java API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_kafka是什么&#34; class=&#34;sect0&#34;&gt;Kafka是什么&lt;/h1&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。
　　
　　KAFKA + STORM +REDIS
[disc]　
* Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。
* Kafka最初是由LinkedIn开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。
* Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。
* Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。
* 无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_jms是什么&#34; class=&#34;sect0&#34;&gt;JMS是什么&lt;/h1&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_jms的基础&#34;&gt;1. JMS的基础&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;    JMS是什么：JMS是Java提供的一套技术规范
　　JMS干什么用：用来异构系统 集成通信，缓解系统瓶颈，提高系统的伸缩性增强系统用户体验，使得系统模块化和组件化变得可行并更加灵活
　　通过什么方式：生产消费者模式（生产者、服务器、消费者）&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152744.png&#34; alt=&#34;2017 03 28 152744&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　jdk，kafka，activemq……&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_jms消息传输模型&#34;&gt;2. JMS消息传输模型&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）
点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;发布/订阅模式（一对多，数据生产后，推送给所有订阅者）
发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即时当前订阅者不可用，处于离线状态。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152816.png&#34; alt=&#34;2017 03 28 152816&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;queue.put（object）  数据生产
queue.take(object)    数据消费&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_jms核心组件&#34;&gt;3. JMS核心组件&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;Destination：消息发送的目的地，也就是前面说的Queue和Topic。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Message [m1]：从字面上就可以看出是被发送的消息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Producer： 消息的生产者，要发送一个消息，必须通过这个生产者来发送。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MessageConsumer： 与生产者相对应，这是消息的消费者或接收者，通过它来接收一个消息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152836.png&#34; alt=&#34;2017 03 28 152836&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;通过与ConnectionFactory可以获得一个connection
通过connection可以获得一个session会话。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_常见的类jms消息服务器&#34;&gt;4. 常见的类JMS消息服务器&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_jms消息服务器_activemq&#34;&gt;4.1. JMS消息服务器 ActiveMQ&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　ActiveMQ 是Apache出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持JMS1.1和J2EE 1.4规范的。
　　主要特点：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;多种语言和协议编写客户端。语言: Java, C, C++, C#, Ruby, Perl, Python, PHP。应用协议: OpenWire,Stomp REST,WS Notification,XMPP,AMQP&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;完全支持JMS1.1和J2EE 1.4规范 (持久化,XA消息,事务)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对Spring的支持,ActiveMQ可以很容易内嵌到使用Spring的系统里面去,而且也支持Spring2.0的特性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过了常见J2EE服务器(如 Geronimo,JBoss 4, GlassFish,WebLogic)的测试,其中通过JCA 1.5 resource adaptors的配置,可以让ActiveMQ可以自动的部署到任何兼容J2EE 1.4 商业服务器上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持多种传送协议:in-VM,TCP,SSL,NIO,UDP,JGroups,JXTA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持通过JDBC和journal提供高速的消息持久化&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从设计上保证了高性能的集群,客户端-服务器,点对点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持Ajax&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持与Axis的整合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以很容易得调用内嵌JMS provider,进行测试&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_分布式消息中间件_metamorphosis&#34;&gt;4.2. 分布式消息中间件 Metamorphosis&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　Metamorphosis (MetaQ) 是一个高性能、高可用、可扩展的分布式消息中间件，类似于LinkedIn的Kafka，具有消息存储顺序写、吞吐量大和支持本地和XA事务等特性，适用于大吞吐量、顺序消息、广播和日志数据传输等场景，在淘宝和支付宝有着广泛的应用，现已开源。
　　主要特点：&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;生产者、服务器和消费者都可分布&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;消息存储顺序写&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性能极高,吞吐量大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持消息顺序&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持本地和XA事务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;客户端pull，随机读,利用sendfile系统调用，zero-copy ,批量拉数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持消费端事务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持消息广播模式&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持异步发送消息&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持http协议&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持消息重试和recover&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据迁移、扩容对用户透明&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;消费状态保存在客户端&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持同步和异步复制两种HA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持group commit&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_分布式消息中间件_rocketmq&#34;&gt;4.3. 分布式消息中间件 RocketMQ&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点：
* 能够保证严格的消息顺序
* 提供丰富的消息拉取模式
* 高效的订阅者水平扩展能力
* 实时的消息订阅机制
* 亿级消息堆积能力
* Metaq3.0 版本改名，产品名称改为RocketMQ&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_其他mq&#34;&gt;4.4. 其他MQ&lt;/h3&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;.NET消息中间件 DotNetMQ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于HBase的消息队列 HQueue&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go 的 MQ 框架 KiteQ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AMQP消息服务器 RabbitMQ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MemcacheQ 是一个基于 MemcacheDB 的消息队列服务器。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_为什么需要消息队列_重要&#34; class=&#34;sect0&#34;&gt;为什么需要消息队列（重要）&lt;/h1&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;消息系统的核心作用就是三点：解耦，异步和并行
以用户注册的案列来说明消息系统的作用&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_用户注册的一般流程&#34;&gt;1. 用户注册的一般流程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152848.png&#34; alt=&#34;2017 03 28 152848&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;问题：随着后端流程越来越多，每步流程都需要额外的耗费很多时间，从而会导致用户更长的等待延迟。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_用户注册的并行执行&#34;&gt;2. 用户注册的并行执行&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152855.png&#34; alt=&#34;2017 03 28 152855&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;问题：系统并行的发起了4个请求，4个请求中，如果某一个环节执行1分钟，其他环节再快，用户也需要等待1分钟。如果其中一个环节异常之后，整个服务挂掉了。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152902.png&#34; alt=&#34;2017 03 28 152902&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_用户注册的最终一致&#34;&gt;3. 用户注册的最终一致&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152910.png&#34; alt=&#34;2017 03 28 152910&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1、 保证主流程的正常执行、执行成功之后，发送MQ消息出去。
2、 需要这个destination的其他系统通过消费数据再执行，最终一致。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152916.png&#34; alt=&#34;2017 03 28 152916&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_kafka核心组件&#34; class=&#34;sect0&#34;&gt;Kafka核心组件&lt;/h1&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;Topic ：消息根据Topic进行归类&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Producer：发送消息者&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consumer：消息接受者&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;broker：每个kafka实例(server)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zookeeper：依赖集群保存meta信息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152924.png&#34; alt=&#34;2017 03 28 152924&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h1 id=&#34;_kafka集群部署&#34; class=&#34;sect0&#34;&gt;Kafka集群部署&lt;/h1&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_集群部署的基本流程&#34;&gt;1. 集群部署的基本流程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　下载安装包、解压安装包、修改配置文件、分发安装包、启动集群&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_集群部署的基础环境准备&#34;&gt;2. 集群部署的基础环境准备&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　安装前的准备工作（zk集群已经部署完毕）&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;关闭防火墙
　　chkconfig iptables off  &amp;amp;&amp;amp; setenforce 0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建用户
　　groupadd realtime &amp;amp;&amp;amp;　useradd realtime　&amp;amp;&amp;amp; usermod -a -G realtime realtime&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建工作目录并赋权
　　mkdir /export
　　mkdir /export/servers
　　chmod 755 -R /export&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;切换到realtime用户下
　　su realtime&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_kafka集群部署_2&#34;&gt;3. Kafka集群部署&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_下载安装包&#34;&gt;3.1. 下载安装包&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　http://kafka.apache.org/downloads.html
　　在linux中使用wget命令下载安装包
    wget http://mirrors.hust.edu.cn/apache/kafka/0.8.2.2/kafka_2.11-0.8.2.2.tgz&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_解压安装包&#34;&gt;3.2. 解压安装包&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　tar -zxvf /export/software/kafka_2.11-0.8.2.2.tgz -C /export/servers/
　　cd /export/servers/
　　ln -s kafka_2.11-0.8.2.2 kafka&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_修改配置文件&#34;&gt;3.3. 修改配置文件&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;cp   /export/servers/kafka/config/server.properties
　　/export/servers/kafka/config/server.properties.bak
　　vi /export/servers/kafka/config/server.properties
　　输入以下内容：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152939.png&#34; alt=&#34;2017 03 28 152939&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_分发安装包&#34;&gt;3.4. 分发安装包&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　scp -r /export/servers/kafka_2.11-0.8.2.2 kafka02:/export/servers
　　然后分别在各机器上创建软连
　　cd /export/servers/
　　ln -s kafka_2.11-0.8.2.2 kafka&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_再次修改配置文件_重要&#34;&gt;3.5. 再次修改配置文件（重要）&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　依次修改各服务器上配置文件的的broker.id，分别是0,1,2不得重复。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_启动集群&#34;&gt;3.6. 启动集群&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　依次在各节点上启动kafka
　　bin/kafka-server-start.sh config/server.properties
　　
== Kafka常用操作命令&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;查看当前服务器中的所有topic
　　bin/kafka-topics.sh --list --zookeeper zk01:2181&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建topic
　　./kafka-topics.sh --create --zookeeper mini1:2181 --replication-factor 1 --partitions 3 --topic first&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;删除topic
　　sh bin/kafka-topics.sh --delete --zookeeper zk01:2181 --topic test
需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过shell命令发送消息
　　kafka-console-producer.sh --broker-list kafka01:9092 --topic itheima&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过shell消费消息
　　sh bin/kafka-console-consumer.sh --zookeeper zk01:2181 --from-beginning --topic test1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看消费位置
　　sh kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zk01:2181 --group testGroup&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看某个Topic的详情
　　sh kafka-topics.sh --topic test --describe --zookeeper zk01:2181&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;_kafka生产者java_api&#34; class=&#34;sect0&#34;&gt;Kafka生产者Java API&lt;/h1&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_152953.png&#34; alt=&#34;2017 03 28 152953&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h1 id=&#34;_kafka消费者java_api&#34; class=&#34;sect0&#34;&gt;Kafka消费者Java API&lt;/h1&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_153002.png&#34; alt=&#34;2017 03 28 153002&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　
　　StreamMessage：Java 数据流消息，用标准流操作来顺序的填充和读取。
　　MapMessage：一个Map类型的消息；名称为 string 类型，而值为 Java 的基本类型。
　　TextMessage：普通字符串消息，包含一个String。
　　ObjectMessage：对象消息，包含一个可序列化的Java 对象
　　BytesMessage：二进制数组消息，包含一个byte[]。
　　XMLMessage:  一个XML类型的消息。
最常用的是TextMessage和ObjectMessage。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Storm目录树、任务提交、消息容错</title>
      <link>/post/bigdata/storm/Storm%E7%9B%AE%E5%BD%95%E6%A0%91%E3%80%81%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E3%80%81%E6%B6%88%E6%81%AF%E5%AE%B9%E9%94%99/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/storm/Storm%E7%9B%AE%E5%BD%95%E6%A0%91%E3%80%81%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E3%80%81%E6%B6%88%E6%81%AF%E5%AE%B9%E9%94%99/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;Storm&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_storm程序的并发机制&#34;&gt;1. Storm程序的并发机制&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_概念&#34;&gt;1.1. 概念&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_配置并行度&#34;&gt;1.2. 配置并行度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm通信机制&#34;&gt;2. Storm通信机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_worker进程间通信&#34;&gt;3. Worker进程间通信&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_worker进程间通信分析&#34;&gt;4. Worker进程间通信分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_worker进程间技术_netty_zeromq&#34;&gt;5. Worker进程间技术(Netty、ZeroMQ)&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_netty&#34;&gt;5.1. Netty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_zeromq&#34;&gt;5.2. ZeroMQ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_worker_内部通信技术_disruptor&#34;&gt;6. Worker 内部通信技术(Disruptor)&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_disruptor的来历&#34;&gt;6.1. Disruptor的来历&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_disruptor是什么&#34;&gt;6.2. Disruptor是什么&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_disruptor主要特点&#34;&gt;6.3. Disruptor主要特点&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm组件本地目录树&#34;&gt;7. Storm组件本地目录树&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm_zookeeper目录树&#34;&gt;8. Storm zookeeper目录树&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm_任务提交的过程&#34;&gt;9. Storm 任务提交的过程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm_消息容错机制&#34;&gt;10. Storm 消息容错机制&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_总体介绍&#34;&gt;10.1. 总体介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_基本实现&#34;&gt;10.2. 基本实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_可靠性配置&#34;&gt;10.3. 可靠性配置&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm程序的并发机制&#34;&gt;1. Storm程序的并发机制&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_概念&#34;&gt;1.1. 概念&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Workers (JVMs)&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在一个物理节点上可以运行一个或多个独立的JVM 进程。一个Topology可以包含一个或多个worker(并行的跑在不同的物理机上), 所以worker process就是执行一个topology的子集, 并且worker只能对应于一个topology&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Executors (threads)&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在一个worker JVM进程中运行着多个Java线程。一个executor线程可以执行一个或多个tasks。但一般默认每个executor只执行一个task。一个worker可以包含一个或多个executor, 每个component (spout或bolt)至少对应于一个executor, 所以可以说executor执行一个compenent的子集, 同时一个executor只能对应于一个component。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Tasks(bolt/spout instances&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Task就是具体的处理逻辑对象，每一个Spout和Bolt会被当作很多task在整个集群里面执行。每一个task对应到一个线程，而stream grouping则是定义怎么从一堆task发射tuple到另外一堆task。你可以调用TopologyBuilder.setSpout和TopBuilder.setBolt来设置并行度 — 也就是有多少个task。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_配置并行度&#34;&gt;1.2. 配置并行度&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;对于并发度的配置, 在storm里面可以在多个地方进行配置, 优先级为：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;　　defaults.yaml &amp;lt; storm.yaml &amp;lt; topology-specific configuration
　　&amp;lt; internal component-specific configuration &amp;lt; external component-specific configuration&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;worker processes的数目, 可以通过配置文件和代码中配置, worker就是执行进程, 所以考虑并发的效果, 数目至少应该大亍machines的数目&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;executor的数目, component的并发线程数，只能在代码中配置(通过setBolt和setSpout的参数), 例如, setBolt(&#34;green-bolt&#34;, new GreenBolt(), 2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;tasks的数目, 可以不配置, 默认和executor1:1, 也可以通过setNumTasks()配置&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;Topology的worker数通过config设置，即执行该topology的worker（java）进程数。它可以通过 storm rebalance 命令任意调整。
Config conf = newConfig();
conf.setNumWorkers(2);//用2个worker
topologyBuilder.setSpout(&#34;blue-spout&#34;,newBlueSpout(),2);//设置2个并发度
topologyBuilder.setBolt(&#34;green-bolt&#34;,newGreenBolt(),2).setNumTasks(4).shuffleGrouping(&#34;blue-spout&#34;);//设置2个并发度，4个任务
topologyBuilder.setBolt(&#34;yellow-bolt&#34;,newYellowBolt(),6).shuffleGrouping(&#34;green-bolt&#34;);//设置6个并发度
StormSubmitter.submitTopology(&#34;mytopology&#34;, conf, topologyBuilder.createTopology());&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163004.png&#34; alt=&#34;2017 03 28 163004&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;3个组件的并发度加起来是10，就是说拓扑一共有10个executor，一共有2个worker，每个worker产生10 / 2 = 5条线程。
绿色的bolt配置成2个executor和4个task。为此每个executor为这个bolt运行2个task。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;动态的改变并行度
　　Storm支持在不 restart topology 的情况下, 动态的改变(增减) worker processes 的数目和 executors 的数目, 称为rebalancing. 通过Storm web UI，或者通过storm rebalance命令实现：
storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm通信机制&#34;&gt;2. Storm通信机制&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;Worker间的通信经常需要通过网络跨节点进行，Storm使用ZeroMQ或Netty(0.9以后默认使用)作为进程间通信的消息框架。
Worker进程内部通信：不同worker的thread通信使用LMAX Disruptor来完成。
不同topologey之间的通信，Storm不负责，需要自己想办法实现，例如使用kafka等；&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_worker进程间通信&#34;&gt;3. Worker进程间通信&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　worker进程间消息传递机制，消息的接收和处理的大概流程见下图&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163014.png&#34; alt=&#34;2017 03 28 163014&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;对于worker进程来说，为了管理流入和传出的消息，每个worker进程有一个独立的接收线程[m1](对配置的TCP端口supervisor.slots.ports进行监听);
对应Worker接收线程，每个worker存在一个独立的发送线程[m2]，它负责从worker的transfer-queue[m3]中读取消息，并通过网络发送给其他worker&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个executor有自己的incoming-queue[m4]和outgoing-queue[m5]。
Worker接收线程将收到的消息通过task编号传递给对应的executor(一个或多个)的incoming-queues;
每个executor有单独的线程分别来处理spout/bolt的业务逻辑，业务逻辑输出的中间数据会存放在outgoing-queue中，当executor的outgoing-queue中的tuple达到一定的阀值，executor的发送线程将批量获取outgoing-queue中的tuple,并发送到transfer-queue中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个worker进程控制一个或多个executor线程，用户可在代码中进行配置。其实就是我们在代码中设置的并发度个数。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_worker进程间通信分析&#34;&gt;4. Worker进程间通信分析&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163023.png&#34; alt=&#34;2017 03 28 163023&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1、 Worker接受线程通过网络接受数据，并根据Tuple中包含的taskId，匹配到对应的executor；然后根据executor找到对应的incoming-queue，将数据存发送到incoming-queue队列中。
2、 业务逻辑执行现成消费incoming-queue的数据，通过调用Bolt的execute(xxxx)方法，将Tuple作为参数传输给用户自定义的方法
3、 业务逻辑执行完毕之后，将计算的中间数据发送给outgoing-queue队列，当outgoing-queue中的tuple达到一定的阀值，executor的发送线程将批量获取outgoing-queue中的tuple,并发送到Worker的transfer-queue中
4、 Worker发送线程消费transfer-queue中数据，计算Tuple的目的地，连接不同的node+port将数据通过网络传输的方式传送给另一个的Worker。
5、 另一个worker执行以上步骤1的操作。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_worker进程间技术_netty_zeromq&#34;&gt;5. Worker进程间技术(Netty、ZeroMQ)&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_netty&#34;&gt;5.1. Netty&lt;/h3&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;　　Netty是一个NIO client-server(客户端服务器)框架，使用Netty可以快速开发网络应用，例如服务器和客户端协议。Netty提供了一种新的方式来使开发网络应用程序，这种新的方式使得它很容易使用和有很强的扩展性。Netty的内部实现时很复杂的，但是Netty提供了简单易用的api从网络处理代码中解耦业务逻辑。Netty是完全基于NIO实现的，所以整个Netty都是异步的。
　　书籍：Netty权威指南&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_zeromq&#34;&gt;5.2. ZeroMQ&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ZeroMQ是一种基于消息队列的多线程网络库，其对套接字类型、连接处理、帧、甚至路由的底层细节进行抽象，提供跨越多种传输协议的套接字。ZeroMQ是网络通信中新的一层，介于应用层和传输层之间（按照TCP/IP划分），其是一个可伸缩层，可并行运行，分散在分布式系统间。
ZeroMQ定位为：一个简单好用的传输层，像框架一样的一个socket library，他使得Socket编程更加简单、简洁和性能更高。是一个消息处理队列库，可在多个线程、内核和主机盒之间弹性伸缩。ZMQ的明确目标是“成为标准网络协议栈的一部分，之后进入Linux内核”。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_worker_内部通信技术_disruptor&#34;&gt;6. Worker 内部通信技术(Disruptor)&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_disruptor的来历&#34;&gt;6.1. Disruptor的来历&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;一个公司的业务与技术的关系，一般可以分为三个阶段。第一个阶段就是跟着业务跑。第二个阶段是经历了几年的时间，才达到的驱动业务阶段。第三个阶段，技术引领业务的发展乃至企业的发展。所以我们在学习Disruptor这个技术时，不得不提LMAX这个机构，因为Disruptor这门技术就是由LMAX公司开发并开源的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LMAX是在英国注册并受到FSA监管（监管号码为509778）的外汇黄金交易所。LMAX也是欧洲第一家也是唯一一家采用多边交易设施Multilateral Trading Facility（MTF）拥有交易所牌照和经纪商牌照的欧洲顶级金融公司&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LAMX拥有最迅捷的交易平台，顶级技术支持。LMAX交易所使用“（MTF）分裂器Disruptor”技术，可以在极短时间内（一般在3百万秒之一内）处理订单，在一个线程里每秒处理6百万订单。所有订单均为撮合成交形式，无一例外。多边交易设施（MTF）曾经用来设计伦敦证券交易 所（london Stock Exchange）、德国证券及衍生工具交易所（Deutsche Borse）和欧洲证券交易所（Euronext）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2011年LMAX凭借该技术获得了金融行业技术评选大赛的最佳交易系统奖和甲骨文“公爵杯”创新编程框架奖。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_disruptor是什么&#34;&gt;6.2. Disruptor是什么&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1、 简单理解：Disruptor是一个Queue。Disruptor是实现了“队列”的功能，而且是一个有界队列。而队列的应用场景自然就是“生产者-消费者”模型。
2、 在JDK中Queue有很多实现类，包括不限于ArrayBlockingQueue、LinkBlockingQueue，这两个底层的数据结构分别是数组和链表。数组查询快，链表增删快，能够适应大多数应用场景。
3、 但是ArrayBlockingQueue、LinkBlockingQueue都是线程安全的。涉及到线程安全，就会有synchronized、lock等关键字，这就意味着CPU会打架。
4、 Disruptor一种线程之间信息无锁的交换方式（使用CAS（Compare And Swap/Set）操作）。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_disruptor主要特点&#34;&gt;6.3. Disruptor主要特点&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1、 没有竞争=没有锁=非常快。
2、 所有访问者都记录自己的序号的实现方式，允许多个生产者与多个消费者共享相同的数据结构。
3、 在每个对象中都能跟踪序列号（ring buffer，claim Strategy，生产者和消费者），加上神奇的cache line padding，就意味着没有为伪共享和非预期的竞争。
2.4.2、 Disruptor 核心技术点
　　Disruptor可以看成一个事件监听或消息机制，在队列中一边生产者放入消息，另外一边消费者并行取出处理.
　　底层是单个数据结构：一个ring buffer。
　　每个生产者和消费者都有一个次序计算器，以显示当前缓冲工作方式。
　　每个生产者消费者能够操作自己的次序计数器的能够读取对方的计数器，生产者能够读取消费者的计算器确保其在没有锁的情况下是可写的。
　　
　　核心组件
* Ring Buffer 环形的缓冲区，负责对通过 Disruptor 进行交换的数据（事件）进行存储和更新。
* Sequence 通过顺序递增的序号来编号管理通过其进行交换的数据（事件），对数据(事件)的处理过程总是沿着序号逐个递增处理。
* RingBuffer底层是个数组，次序计算器是一个64bit long 整数型，平滑增长。
　　&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163035.png&#34; alt=&#34;2017 03 28 163035&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;1、 接受数据并写入到脚标31的位置，之后会沿着序号一直写入，但是不会绕过消费者所在的脚标。
2、 Joumaler和replicator同时读到24的位置，他们可以批量读取数据到30
3、消费逻辑线程读到了14的位置，但是没法继续读下去，因为他的sequence暂停在15的位置上，需要等到他的sequence给他序号。如果sequence能正常工作，就能读取到30的数据。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm组件本地目录树&#34;&gt;7. Storm组件本地目录树&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163042.png&#34; alt=&#34;2017 03 28 163042&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm_zookeeper目录树&#34;&gt;8. Storm zookeeper目录树&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163050.png&#34; alt=&#34;2017 03 28 163050&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm_任务提交的过程&#34;&gt;9. Storm 任务提交的过程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163058.png&#34; alt=&#34;2017 03 28 163058&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;TopologyMetricsRunnable.TaskStartEvent[oldAssignment=&amp;lt;null&amp;gt;,newAssignment=Assignment[masterCodeDir=C:\Users\MAOXIA~1\AppData\Local\Temp\\e73862a8-f7e7-41f3-883d-af494618bc9f\nimbus\stormdist\double11-1-1458909887,nodeHost={61ce10a7-1e78-4c47-9fb3-c21f43a331ba=192.168.1.106},taskStartTimeSecs={1=1458909910, 2=1458909910, 3=1458909910, 4=1458909910, 5=1458909910, 6=1458909910, 7=1458909910, 8=1458909910},workers=[ResourceWorkerSlot[hostname=192.168.1.106,memSize=0,cpu=0,tasks=[1, 2, 3, 4, 5, 6, 7, 8],jvm=&amp;lt;null&amp;gt;,nodeId=61ce10a7-1e78-4c47-9fb3-c21f43a331ba,port=6900]],timeStamp=1458909910633,type=Assign],task2Component=&amp;lt;null&amp;gt;,clusterName=&amp;lt;null&amp;gt;,topologyId=double11-1-1458909887,timestamp=0]&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163734.png&#34; alt=&#34;2017 03 28 163734&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm_消息容错机制&#34;&gt;10. Storm 消息容错机制&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_总体介绍&#34;&gt;10.1. 总体介绍&lt;/h3&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在storm中，可靠的信息处理机制是从spout开始的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一个提供了可靠的处理机制的spout需要记录他发射出去的tuple，当下游bolt处理tuple或者子tuple失败时spout能够重新发射。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Storm通过调用Spout的nextTuple()发送一个tuple。为实现可靠的消息处理，首先要给每个发出的tuple带上唯一的ID，并且将ID作为参数传递给SoputOutputCollector的emit()方法：collector.emit(new Values(&#34;value1&#34;,&#34;value2&#34;), msgId); messageid就是用来标示唯一的tupke的，而rootid是随机生成的
给每个tuple指定ID告诉Storm系统，无论处理成功还是失败，spout都要接收tuple树上所有节点返回的通知。如果处理成功，spout的ack()方法将会对编号是msgId的消息应答确认；如果处理失败或者超时，会调用fail()方法。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_基本实现&#34;&gt;10.2. 基本实现&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Storm 系统中有一组叫做&#34;acker&#34;的特殊的任务，它们负责跟踪DAG（有向无环图）中的每个消息。
acker任务保存了spout id到一对值的映射。第一个值就是spout的任务id，通过这个id，acker就知道消息处理完成时该通知哪个spout任务。第二个值是一个64bit的数字，我们称之为&#34;ack val&#34;， 它是树中所有消息的随机id的异或计算结果。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ack val表示了整棵树的的状态，无论这棵树多大，只需要这个固定大小的数字就可以跟踪整棵树。当消息被创建和被应答的时候都会有相同的消息id发送过来做异或。 每当acker发现一棵树的ack val值为0的时候，它就知道这棵树已经被完全处理了&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163743.png&#34; alt=&#34;2017 03 28 163743&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163751.png&#34; alt=&#34;2017 03 28 163751&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163757.png&#34; alt=&#34;2017 03 28 163757&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163804.png&#34; alt=&#34;2017 03 28 163804&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28_163811.png&#34; alt=&#34;2017 03 28 163811&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_可靠性配置&#34;&gt;10.3. 可靠性配置&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;有三种方法可以去掉消息的可靠性：
将参数Config.TOPOLOGY_ACKERS设置为0，通过此方法，当Spout发送一个消息的时候，它的ack方法将立刻被调用；
Spout发送一个消息时，不指定此消息的messageID。当需要关闭特定消息可靠性的时候，可以使用此方法；
最后，如果你不在意某个消息派生出来的子孙消息的可靠性，则此消息派生出来的子消息在发送时不要做锚定，即在emit方法中不指定输入消息。因为这些子孙消息没有被锚定在任何tuple tree中，因此他们的失败不会引起任何spout重新发送消息。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;一个worker进程运行一个专用的接收线程来负责将外部发送过来的消息移动到对应的executor线程的incoming-queue中
transfer-queue的大小由参数topology.transfer.buffer.size来设置。transfer-queue的每个元素实际上代表一个tuple的集合
transfer-queue的大小由参数topology.transfer.buffer.size来设置。
executor的incoming-queue的大小用户可以自定义配置。
executor的outgoing-queue的大小用户可以自定义配置&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>storm集群部署,单词计数,Stream Grouping</title>
      <link>/post/bigdata/storm/storm%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2,%E5%8D%95%E8%AF%8D%E8%AE%A1%E6%95%B0,Stream%20Grouping/</link>
      <pubDate>Tue, 28 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/storm/storm%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2,%E5%8D%95%E8%AF%8D%E8%AE%A1%E6%95%B0,Stream%20Grouping/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;storm&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_集群部署的基本流程&#34;&gt;1. 集群部署的基本流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_集群部署的基础环境准备&#34;&gt;2. 集群部署的基础环境准备&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm集群部署&#34;&gt;3. Storm集群部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm常用操作命令&#34;&gt;4. Storm常用操作命令&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm集群的进程及日志熟悉&#34;&gt;5. Storm集群的进程及日志熟悉&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm源码下载及目录熟悉&#34;&gt;6. Storm源码下载及目录熟悉&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm单词技术案例_重点掌握&#34;&gt;7. Storm单词技术案例（重点掌握）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_集群部署的基本流程&#34;&gt;1. 集群部署的基本流程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;集群部署的流程：下载安装包、解压安装包、修改配置文件、分发安装包、启动集群
注意：
    所有的集群上都需要配置hosts&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vi  /etc/hosts
192.168.239.128 storm01 zk01 hadoop01
192.168.239.129 storm02 zk02 hadoop02
192.168.239.130 storm03 zk03 hadoop03&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_集群部署的基础环境准备&#34;&gt;2. 集群部署的基础环境准备&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;安装前的准备工作（zk集群已经部署完毕）&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;关闭防火墙&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;chkconfig iptables off  &amp;amp;&amp;amp; setenforce 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建用户&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;groupadd realtime &amp;amp;&amp;amp;　useradd realtime　&amp;amp;&amp;amp; usermod -a -G realtime realtime&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建工作目录并赋权&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mkdir /export
mkdir /export/servers
chmod 755 -R /export&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;切换到realtime用户下&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su realtime&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm集群部署&#34;&gt;3. Storm集群部署&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;下载安装包&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;wget http://124.202.164.6/files/1139000006794ECA/apache.fayea.com/storm/apache-storm-0.9.5/apache-storm-0.9.5.tar.gz&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压安装包&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf apache-storm-0.9.5.tar.gz -C /export/servers/
cd /export/servers/
ln -s apache-storm-0.9.5 storm&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改配置文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mv /export/servers/storm/conf/storm.yaml /export/servers/storm/conf/storm.yaml.bak
vi /export/servers/storm/conf/storm.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;输入以下内容：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-01.png&#34; alt=&#34;2017 03 28 01&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分发安装包&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;scp -r /export/servers/apache-storm-0.9.5 storm02:/export/servers
#然后分别在各机器上创建软连接
cd /export/servers/
ln -s apache-storm-0.9.5 storm&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动集群&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#在nimbus.host所属的机器上启动 nimbus服务
cd /export/servers/storm/bin/
nohup ./storm nimbus &amp;amp;
#在nimbus.host所属的机器上启动ui服务
cd /export/servers/storm/bin/
nohup ./storm ui &amp;amp;
#在其它个点击上启动supervisor服务
cd /export/servers/storm/bin/
nohup ./storm supervisor &amp;amp;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看集群
访问nimbus.host:/8080，即可看到storm的ui界面。&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-02.png&#34; alt=&#34;2017 03 28 02&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm常用操作命令&#34;&gt;4. Storm常用操作命令&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;有许多简单且有用的命令可以用来管理拓扑，它们可以提交、杀死、禁用、再平衡拓扑。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;提交任务命令格式&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;storm jar 【jar路径】 【拓扑包名.拓扑类名】 【拓扑名称】&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/storm jar examples/storm-starter/storm-starter-topologies-0.9.6.jar storm.starter.WordCountTopology wordcount&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;杀死任务命令格式&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;storm kill 【拓扑名称】 -w 10（执行kill命令时可以通过-w [等待秒数]指定拓扑停用以后的等待时间）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;storm kill topology-name -w 10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;停用任务命令格式&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;storm deactivte  【拓扑名称】&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;storm deactivte topology-name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;我们能够挂起或停用运行中的拓扑。当停用拓扑时，所有已分发的元组都会得到处理，但是spouts的nextTuple方法不会被调用。销毁一个拓扑，可以使用kill命令。它会以一种安全的方式销毁一个拓扑，首先停用拓扑，在等待拓扑消息的时间段内允许拓扑完成当前的数据流。&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;启用任务命令格式&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;storm activate【拓扑名称】&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;storm activate topology-name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;重新部署任务命令格式&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;storm rebalance  【拓扑名称】&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;storm rebalance topology-name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;再平衡使你重分配集群任务。这是个很强大的命令。比如，你向一个运行中的集群增加了节点。再平衡命令将会停用拓扑，然后在相应超时时间之后重分配工人，并重启拓扑。&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm集群的进程及日志熟悉&#34;&gt;5. Storm集群的进程及日志熟悉&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;部署成功之后，启动storm集群。
依次启动集群的各种角色&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看nimbus的日志信息
在nimbus的服务器上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /export/servers/storm/logs
tail -100f /export/servers/storm/logs/nimbus.log&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看ui运行日志信息
在ui的服务器上，一般和nimbus一个服务器&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /export/servers/storm/logs
tail -100f /export/servers/storm/logs/ui.log&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看supervisor运行日志信息
在supervisor服务上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /export/servers/storm/logs
tail -100f /export/servers/storm/logs/supervisor.log&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看supervisor上worker运行日志信息
在supervisor服务上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /export/servers/storm/logs
tail -100f /export/servers/storm/logs/worker-6702.log&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-03.png&#34; alt=&#34;2017 03 28 03&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;(该worker正在运行wordcount程序)&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm源码下载及目录熟悉&#34;&gt;6. Storm源码下载及目录熟悉&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;在Storm官方网站上寻找源码地址&lt;br&gt;
&lt;a href=&#34;http://storm.apache.org/downloads.html&#34; class=&#34;bare&#34;&gt;http://storm.apache.org/downloads.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;点击文字标签进入github&lt;br&gt;
点击Apache/storm文字标签，进入github
      &lt;a href=&#34;https://github.com/apache/storm&#34; class=&#34;bare&#34;&gt;https://github.com/apache/storm&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;拷贝storm源码地址&lt;br&gt;
在网页右侧，拷贝storm源码地址&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用Subversion客户端下载&lt;br&gt;
&lt;a href=&#34;https://github.com/apache/storm/tags/v0.9.5&#34; class=&#34;bare&#34;&gt;https://github.com/apache/storm/tags/v0.9.5&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Storm源码目录分析（重要）&lt;br&gt;
扩展包中的三个项目，使storm能与hbase、hdfs、kafka交互&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm单词技术案例_重点掌握&#34;&gt;7. Storm单词技术案例（重点掌握）&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;功能说明&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;设计一个topology，来实现对文档里面的单词出现的频率进行统计。
整个topology分为三个部分：
RandomSentenceSpout：
    数据源，在已知的英文句子中，随机发送一条句子出去。
SplitSentenceBolt：
    负责将单行文本记录（句子）切分成单词
WordCountBolt：
    负责对单词的频率进行累加&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;项目主要流程&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-04.png&#34; alt=&#34;2017 03 28 04&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RandomSentenceSpout的实现及生命周期&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-05.png&#34; alt=&#34;2017 03 28 05&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SplitSentenceBolt的实现及生命周期&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-06.png&#34; alt=&#34;2017 03 28 06&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WordCountBolt的实现及生命周期&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-28-07.png&#34; alt=&#34;2017 03 28 07&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stream Grouping详解&lt;br&gt;
Storm里面有7种类型的stream grouping&lt;/p&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Shuffle Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;随机分组， 随机派发stream里面的tuple，保证每个bolt接收到的tuple数目大致相同。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Fields Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;按字段分组，比如按userid来分组，具有同样userid的tuple会被分到相同的Bolts里的一个task，而不同的userid则会被分配到不同的bolts里的task。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;All Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;广播发送，对于每一个tuple，所有的bolts都会收到。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Global Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;全局分组， 这个tuple被分配到storm中的一个bolt的其中一个task。再具体一点就是分配给id值最低的那个task。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Non Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;不分组，这stream grouping个分组的意思是说stream不关心到底谁会收到它的tuple。目前这种分组和Shuffle grouping是一样的效果， 有一点不同的是storm会把这个bolt放到这个bolt的订阅者同一个线程里面去执行。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Direct Grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;直接分组， 这是一种比较特别的分组方法，用这种分组意味着消息的发送者指定由消息接收者的哪个task处理这个消息。只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来获取处理它的消息的task的id （OutputCollector.emit方法也会返回task的id）。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Local or shuffle grouping&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;如果目标bolt有一个或者多个task在同一个工作进程中，tuple将会被随机发生给这些tasks。否则，和普通的Shuffle Grouping行为一致。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>storm</title>
      <link>/post/bigdata/storm/storm/</link>
      <pubDate>Sun, 26 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/storm/storm/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;storm 简介&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_离线计算是什么&#34;&gt;1. 离线计算是什么？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_流式计算是什么&#34;&gt;2. 流式计算是什么&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_离线计算与实时计算的区别&#34;&gt;3. 离线计算与实时计算的区别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm是什么&#34;&gt;4. Storm是什么？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm与hadoop的区别&#34;&gt;5. Storm与Hadoop的区别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm应用场景及行业案例&#34;&gt;6. Storm应用场景及行业案例&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_典型案列&#34;&gt;6.1. 典型案列&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm核心组件_重要&#34;&gt;7. Storm核心组件（重要）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_storm编程模型_重要&#34;&gt;8. Storm编程模型（重要）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_流式计算一般架构图_重要&#34;&gt;9. 流式计算一般架构图（重要）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_离线计算是什么&#34;&gt;1. 离线计算是什么？&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;    离线计算：批量获取数据、批量传输数据、周期性批量计算数据、数据展示
    代表技术：Sqoop批量导入数据、HDFS批量存储数据、MapReduce批量计算数据、Hive批量计算数据、***任务调度
1，hivesql
2、调度平台
3、Hadoop集群运维
4、数据清洗（脚本语言）
5、元数据管理
6、数据稽查
7、数据仓库模型架构&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_流式计算是什么&#34;&gt;2. 流式计算是什么&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;流式计算：数据实时产生、数据实时传输、数据实时计算、实时展示
代表技术：Flume实时获取数据、Kafka/metaq实时数据存储、Storm/JStorm实时数据计算、Redis实时结果缓存、持久化存储(mysql)。
一句话总结：将源源不断产生的数据实时收集并实时计算，尽可能快的得到计算结果&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_离线计算与实时计算的区别&#34;&gt;3. 离线计算与实时计算的区别&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;最大的区别：实时收集、实时计算、实时展示&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm是什么&#34;&gt;4. Storm是什么？&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;Flume实时采集，低延迟
Kafka消息队列，低延迟
Storm实时计算，低延迟
Redis实时存储，低延迟

Storm用来实时处理数据，特点：低延迟、高可用、分布式、可扩展、数据不丢失。提供简单容易理解的接口，便于开发。


海量数据？数据类型很多，产生数据的终端很多，处理数据能力增强&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm与hadoop的区别&#34;&gt;5. Storm与Hadoop的区别&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;   Storm用于实时计算，Hadoop用于离线计算。
   Storm处理的数据保存在内存中，源源不断；Hadoop处理的数据保存在文件系统中，一批一批。
   Storm的数据通过网络传输进来；Hadoop的数据保存在磁盘中。
   Storm与Hadoop的编程模型相似

Job：任务名称
JobTracker：项目经理
TaskTracker：开发组长、产品经理
Child:负责开发的人员
Mapper/Reduce:开发人员中的两种角色，一种是服务器开发、一种是客户端开发

Topology:任务名称
Nimbus:项目经理
Supervisor:开组长、产品经理
Worker:开人员
Spout/Bolt：开人员中的两种角色，一种是服务器开发、一种是客户端开发&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm应用场景及行业案例&#34;&gt;6. Storm应用场景及行业案例&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;        Storm用来实时计算源源不断产生的数据，如同流水线生产。
6.1、运用场景
   日志分析
从海量日志中分析出特定的数据，并将分析的结果存入外部存储器用来辅佐决策。
   管道系统
将一个数据从一个系统传输到另外一个系统，比如将数据库同步到Hadoop
   消息转化器
将接受到的消息按照某种格式进行转化，存储到另外一个系统如消息中间件&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_典型案列&#34;&gt;6.1. 典型案列&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;   一淘-实时分析系统：实时分析用户的属性，并反馈给搜索引擎
最初，用户属性分析是通过每天在云梯上定时运行的MR job来完成的。为了满足实时性的要求，希望能够实时分析用户的行为日志，将最新的用户属性反馈给搜索引擎，能够为用户展现最贴近其当前需求的结果。
   携程-网站性能监控：实时分析系统监控携程网的网站性能
利用HTML5提供的performance标准获得可用的指标，并记录日志。Storm集群实时分析日志和入库。使用DRPC聚合成报表，通过历史数据对比等判断规则，触发预警事件。
   阿里妈妈-用户画像：实时计算用户的兴趣数据
为了更加精准投放广告，阿里妈妈后台计算引擎需要维护每个用户的兴趣点（理想状态是，你对什么感兴趣，就向你投放哪类广告）。用户兴趣主要基于用户的历史行为、用户的实时查询、用户的实时点击、用户的地理信息而得，其中实时查询、实时点击等用户行为都是实时数据。考虑到系统的实时性，阿里妈妈使用Storm维护用户兴趣数据，并在此基础上进行受众定向的广告投放。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm核心组件_重要&#34;&gt;7. Storm核心组件（重要）&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-26-3.png&#34; alt=&#34;2017 03 26 3&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;   Nimbus：负责资源分配和任务调度。
   Supervisor：负责接受nimbus分配的任务，启动和停止属于自己管理的worker进程。---通过配置文件设置当前supervisor上启动多少个worker。
   Worker：运行具体处理组件逻辑的进程。Worker运行的任务类型只有两种，一种是Spout任务，一种是Bolt任务。
   Task：worker中每一个spout/bolt的线程称为一个task. 在storm0.8之后，task不再与物理线程对应，不同spout/bolt的task可能会共享一个物理线程，该线程称为executor。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_storm编程模型_重要&#34;&gt;8. Storm编程模型（重要）&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-26-2.png&#34; alt=&#34;2017 03 26 2&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;   Topology：Storm中运行的一个实时应用程序的名称。（拓扑）
   Spout：在一个topology中获取源数据流的组件。
通常情况下spout会从外部数据源中读取数据，然后转换为topology内部的源数据。
   Bolt：接受数据然后执行处理的组件,用户可以在其中执行自己想要的操作。
   Tuple：一次消息传递的基本单元，理解为一组消息就是一个Tuple。
   Stream：表示数据的流向。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_流式计算一般架构图_重要&#34;&gt;9. 流式计算一般架构图（重要）&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/storm/2017-03-26-1.png&#34; alt=&#34;2017 03 26 1&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;   其中flume用来获取数据。
   Kafka用来临时保存数据。
   Strom用来计算数据。
   Redis是个内存数据库，用来保存数据。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>CentOS7</title>
      <link>/post/linux/CentOS7/</link>
      <pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/linux/CentOS7/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;CentOS7&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_centos7&#34;&gt;1. CentOS7&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_安装_docker&#34;&gt;1.1. 安装 docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_centos7&#34;&gt;1. CentOS7&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装_docker&#34;&gt;1.1. 安装 docker&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hadoop-docker</title>
      <link>/post/bigdata/hadoop/docker/hadoop-docker/</link>
      <pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/hadoop/docker/hadoop-docker/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/HariSekhon/Dockerfiles&#34; class=&#34;bare&#34;&gt;https://github.com/HariSekhon/Dockerfiles&lt;/a&gt;
&lt;a href=&#34;https://hub.docker.com/r/harisekhon&#34; class=&#34;bare&#34;&gt;https://hub.docker.com/r/harisekhon&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hbase</title>
      <link>/post/bigdata/hadoop/hbase/</link>
      <pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/hadoop/hbase/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hbase&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_安装&#34;&gt;1. 安装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hbase_性能优化&#34;&gt;2. HBase 性能优化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_安装&#34;&gt;1. 安装&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;上传
首先确保用户是 &lt;strong&gt;hadoop&lt;/strong&gt; ，用工具将 &lt;strong&gt;hbase&lt;/strong&gt; 安装包 &lt;strong&gt;hbase-0.99.2-bin.tar.gz&lt;/strong&gt; 上传到 &lt;strong&gt;/home/hadoop&lt;/strong&gt; 下，确保 &lt;strong&gt;hbase-0.99.2-bin.tar.gz&lt;/strong&gt; 的用户是 &lt;strong&gt;hadoop&lt;/strong&gt;，如果不是，执行 &lt;strong&gt;chown&lt;/strong&gt; 命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su – hadoop
tar –zxvf hbase-0.99.2-bin.tar.gz&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重命名&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mv hbase-0.99.2hbase&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改环境变量&lt;/p&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;在master机器上执行下面命令：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su – root
vi/etc/profile
添加内容：
export HBASE_HOME=/home/hadoop/hbase
export PATH=$PATH:$HBASE_HOME/bin
执行命令：
source /etc/profile
su – hadoop&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在其他机器上执行上述操作。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改配置文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su – hadoop&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;将配置文件 上传到 &lt;strong&gt;/home/hadoop/hbase/conf&lt;/strong&gt; 文件夹下。
每个文件的解释如下：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hbase-env.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;export JAVA_HOME=/usr/jdk   //jdk安装目录
export HBASE_CLASSPATH=/home/hadoop/hadoop/conf   //hadoop配置文件的位置
export HBASE_MANAGES_ZK=true   #如果使用独立安装的zookeeper这个地方就是false&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hbase-site.xml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hbase.master&amp;lt;/name&amp;gt;       #hbasemaster的主机和端口
        &amp;lt;value&amp;gt;master1:60000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hbase.master.maxclockskew&amp;lt;/name&amp;gt;    #时间同步允许的时间差
        &amp;lt;value&amp;gt;180000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hbase.rootdir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs:// hadoop-cluster1/hbase&amp;lt;/value&amp;gt;#hbase共享目录，持久化hbase数据
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hbase.cluster.distributed&amp;lt;/name&amp;gt;  #是否分布式运行，false即为单机
        &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hbase.zookeeper.quorum&amp;lt;/name&amp;gt;#zookeeper地址
        &amp;lt;value&amp;gt;slave1, slave2,slave3&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hbase.zookeeper.property.dataDir&amp;lt;/name&amp;gt;#zookeeper配置信息快照的位置
        &amp;lt;value&amp;gt;/home/hadoop/hbase/tmp/zookeeper&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Regionservers    //是从机器的域名
slave1
slave2
slave3&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;把hadoop的hdfs-site.xml和core-site.xml 放到hbase/conf下&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cp /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml /home/hadoop/hbase/conf
cp /home/hadoop/hadoop/etc/hadoop/core-site.xml /home/hadoop/hbase/conf&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;发送到其他机器&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su - hadoop
scp –r /home/hadoop/hbase hadoop@slave1:/home/hadoop
scp –r /home/hadoop/hbase hadoop@slave2:/home/hadoop
scp –r /home/hadoop/hbase hadoop@slave3:/home/hadoop&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su – hadoop
start-hbase.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;进程：jps
进入hbase的shell：hbase shell
退出hbase的shell：quit
页面：http://master:60010/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;create &#39;user&#39;,&#39;info1&#39;,&#39;info2&#39;

describe &#39;user&#39;

exists &#39;user&#39;

put &#39;user&#39;,&#39;1234&#39;,&#39;info1:name&#39;,&#39;zhangsan&#39;
put &#39;user&#39;,&#39;1234&#39;,&#39;info2:name&#39;,&#39;zhangsan&#39;

scan &#39;user&#39;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;table class=&#34;tableblock frame-all grid-all spread&#34;&gt;
&lt;colgroup&gt;
&lt;col style=&#34;width: 50%;&#34;&gt;
&lt;col style=&#34;width: 50%;&#34;&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;名称&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;命令表达式&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;创建表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;create &#39;表名&#39;, &#39;列族名1&#39;,&#39;列族名2&#39;,&#39;列族名N&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;查看所有表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;list&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;描述表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;describe &#39;表名&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;判断表存在&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;exists &#39;表名&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;判断是否禁用启用表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;is_enabled &#39;表名&#39; is_disabled ‘表名’&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;添加记录&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;put  ‘表名’, ‘rowKey’, ‘列族 : 列‘  ,  &#39;值&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;查看记录rowkey下的所有数据&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;get  &#39;表名&#39; , &#39;rowKey&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;查看表中的记录总数&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;count  &#39;表名&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;获取某个列族&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;get &#39;表名&#39;,&#39;rowkey&#39;,&#39;列族&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;获取某个列族的某个列&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;get &#39;表名&#39;,&#39;rowkey&#39;,&#39;列族：列’&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;删除记录&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;delete  ‘表名’ ,‘行名’ , ‘列族：列&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;删除整行&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;deleteall &#39;表名&#39;,&#39;rowkey&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;删除一张表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;先要屏蔽该表，才能对该表进行删除
第一步 disable ‘表名’，第二步  drop &#39;表名&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;清空表&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;truncate &#39;表名&#39;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;查看所有记录&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;scan &#34;表名&#34;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;查看某个表某个列中所有数据&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;scan &#34;表名&#34; , {COLUMNS&amp;#8658;&#39;列族名:列名&#39;}&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tableblock halign-right valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;更新记录&lt;/p&gt;&lt;/td&gt;
&lt;td class=&#34;tableblock halign-center valign-top&#34;&gt;&lt;p class=&#34;tableblock&#34;&gt;就是重写一遍，进行覆盖，hbase没有修改，都是追加&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hbase_性能优化&#34;&gt;2. HBase 性能优化&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;修改Linux最大文件数&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;Linux系统最大可打开文件数一般默认的参数值是1024，如果你不进行修改并发量上来的时候会出现“Too Many Open Files”的错误，导致整个HBase不可运行
查看： ulimit -a    结果：open files (-n) 1024
临时修改： ulimit -n 4096
持久修改：
vi /etc/security/limits.conf在文件最后加上：
* soft nofile 65535
* hard nofile 65535
* soft nproc 65535
* hard nproc 65535&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改 JVM 配置&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;修改hbase-env.sh文件中的配置参数
HBASE_HEAPSIZE 4000 #HBase使用的 JVM 堆的大小
HBASE_OPTS &#34;‐server ‐XX:+UseConcMarkSweepGC&#34;JVM #GC 选项
参数解释：
-client，-server
这两个参数用于设置虚拟机使用何种运行模式，client模式启动比较快，但运行时性能和内存管理效率不如server模式，通常用于客户端应用程序。相反，server模式启动比client慢，但可获得更高的运行性能。
‐XX:+UseConcMarkSweepGC：设置为并发收集&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改HBase配置：hbase-site.xml&lt;/p&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;zookeeper.session.timeout&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;默认值：3分钟（180000ms）,可以改成1分钟
说明：RegionServer与Zookeeper间的连接超时时间。当超时时间到后，ReigonServer会被Zookeeper从RS集群清单中移除，HMaster收到移除通知后，会对这台server负责的regions重新balance，让其他存活的RegionServer接管.
调优：
这个timeout决定了RegionServer是否能够及时的failover。设置成1分钟或更低，可以减少因等待超时而被延长的failover时间。
不过需要注意的是，对于一些Online应用，RegionServer从宕机到恢复时间本身就很短的（网络闪断，crash等故障，运维可快速介入），如果调低timeout时间，反而会得不偿失。因为当ReigonServer被正式从RS集群中移除时，HMaster就开始做balance了（让其他RS根据故障机器记录的WAL日志进行恢复）。当故障的RS在人工介入恢复后，这个balance动作是毫无意义的，反而会使负载不均匀，给RS带来更多负担。特别是那些固定分配regions的场景。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hbase.regionserver.handler.count&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;默认值：10
说明：RegionServer的请求处理IO线程数。
调优：
这个参数的调优与内存息息相关。
较少的IO线程，适用于处理单次请求内存消耗较高的Big PUT场景（大容量单次PUT或设置了较大cache的scan，均属于Big PUT）或ReigonServer的内存比较紧张的场景。
较多的IO线程，适用于单次请求内存消耗低，TPS（吞吐量）要求非常高的场景。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hbase.hregion.max.filesize&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;默认值：256M
说明：在当前ReigonServer上单个Reigon的最大存储空间，单个Region超过该值时，这个Region会被自动split成更小的region。
调优：
小region对split和compaction友好，因为拆分region或compact小region里的storefile速度很快，内存占用低。缺点是split和compaction会很频繁。
特别是数量较多的小region不停地split, compaction，会导致集群响应时间波动很大，region数量太多不仅给管理上带来麻烦，甚至会引发一些Hbase的bug。
一般512以下的都算小region。
大region，则不会经常split和compaction，因为做一次compact和split会产生较长时间的停顿，对应用的读写性能冲击非常大。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hfile.block.cache.size&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;默认值：0.2
说明：storefile的读缓存占用内存的大小百分比，0.2表示20%。该值直接影响数据读的性能。
调优：当然是越大越好，如果写比读少很多，开到0.4-0.5也没问题。如果读写较均衡，0.3左右。如果写比读多，果断默认吧。
HBase上Regionserver的内存分为两个部分，一部分作为Memstore，主要用来写；另外一部分作为BlockCache，主要用于读。
写请求会先写入Memstore，Regionserver会给每个region提供一个Memstore，当Memstore满64MB以后，会启动 flush刷新到磁盘。
读请求先到Memstore中查数据，查不到就到BlockCache中查，再查不到就会到磁盘上读，并把读的结果放入BlockCache。由于BlockCache采用的是LRU策略（Least Recently Used 近期最少使用算法），因此BlockCache达到上限(heapsize * hfile.block.cache.size * 0.85)后，会启动淘汰机制，淘汰掉最老的一批数据。
一个Regionserver上有一个BlockCache和N个Memstore，它们的大小之和不能大于等于内存 * 0.8，否则HBase不能启动。默认BlockCache为0.2，而Memstore为0.4。对于注重读响应时间的系统，可以将 BlockCache设大些，比如设置BlockCache=0.4，Memstore=0.39，以加大缓存的命中率。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hbase.hregion.memstore.block.multiplier&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;默认值：2
说明：当一个region里的memstore占用内存大小超过hbase.hregion.memstore.flush.size两倍的大小时，block该region的所有请求，进行flush，释放内存。
虽然我们设置了region所占用的memstores总内存大小，比如64M，但想象一下，在最后63.9M的时候，我Put了一个200M的数据，此时memstore的大小会瞬间暴涨到超过预期的hbase.hregion.memstore.flush.size的几倍。这个参数的作用是当memstore的大小增至超过hbase.hregion.memstore.flush.size 2倍时，block所有请求，遏制风险进一步扩大。
调优： 这个参数的默认值还是比较靠谱的。如果你预估你的正常应用场景（不包括异常）不会出现突发写或写的量可控，那么保持默认值即可。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>flume-sqoop</title>
      <link>/post/bigdata/hadoop/flume-sqoop/</link>
      <pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/hadoop/flume-sqoop/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;flume-sqoop&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_flume介绍&#34;&gt;1. Flume介绍&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_概述&#34;&gt;1.1. 概述&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_运行机制&#34;&gt;1.2. 运行机制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_flume采集系统结构图&#34;&gt;1.3. Flume采集系统结构图&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_flume实战案例&#34;&gt;2. Flume实战案例&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_flume的安装部署&#34;&gt;2.1. Flume的安装部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_采集案例&#34;&gt;2.2. 采集案例&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_采集目录到hdfs&#34;&gt;2.2.1. 采集目录到HDFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_采集文件到hdfs&#34;&gt;2.2.2. 采集文件到HDFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_更多source和sink组件&#34;&gt;2.2.3. 更多source和sink组件&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_工作流调度器azkaban&#34;&gt;3. 工作流调度器azkaban&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_为什么需要工作流调度系统&#34;&gt;3.1. 为什么需要工作流调度系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_工作流调度实现方式&#34;&gt;3.2. 工作流调度实现方式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_常见工作流调度系统&#34;&gt;3.3. 常见工作流调度系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_azkaban与oozie对比&#34;&gt;3.4. Azkaban与Oozie对比&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_azkaban介绍&#34;&gt;3.4.1. Azkaban介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_azkaban安装部署&#34;&gt;3.4.2. Azkaban安装部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_azkaban实战&#34;&gt;3.4.3. Azkaban实战&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_command类型多job工作流flow&#34;&gt;Command类型多job工作流flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hdfs操作任务&#34;&gt;HDFS操作任务&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_mapreduce任务&#34;&gt;MAPREDUCE任务&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive脚本任务&#34;&gt;HIVE脚本任务&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sqoop数据迁移&#34;&gt;4. sqoop数据迁移&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_sqoop安装&#34;&gt;4.1. sqoop安装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sqoop的数据导入&#34;&gt;4.2. Sqoop的数据导入&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_示例&#34;&gt;4.2.1. 示例&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_导入表表数据到hdfs&#34;&gt;导入表表数据到HDFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_导入关系表到hive&#34;&gt;导入关系表到HIVE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_导入到hdfs指定目录&#34;&gt;导入到HDFS指定目录&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_导入表数据子集&#34;&gt;导入表数据子集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_增量导入&#34;&gt;增量导入&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sqoop的数据导出&#34;&gt;4.3. Sqoop的数据导出&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_sqoop作业&#34;&gt;4.4. Sqoop作业&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;preamble&#34;&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在一个完整的大数据处理系统中，除了hdfs+mapreduce+hive组成分析系统的核心之外，还需要数据采集、结果数据导出、任务调度等不可或缺的辅助系统，而这些辅助工具在hadoop生态体系中都有便捷的开源框架，如图所示：&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/flume/image001.png&#34; alt=&#34;image001&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_flume介绍&#34;&gt;1. Flume介绍&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_概述&#34;&gt;1.1. 概述&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、hbase、hive、kafka等众多外部存储系统中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一般的采集需求，通过对flume的简单配置即可实现&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_运行机制&#34;&gt;1.2. 运行机制&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Flume&lt;/strong&gt; 分布式系统中最核心的角色是 &lt;strong&gt;agent&lt;/strong&gt;，&lt;strong&gt;flume&lt;/strong&gt; 采集系统就是由一个个*agent*所连接起来形成&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每一个 &lt;strong&gt;agent&lt;/strong&gt; 相当于一个数据传递员 ，内部有三个组件：&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;：采集源，用于跟数据源对接，以获取数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sink&lt;/strong&gt;：下沉地，采集数据的传送目的，用于往下一级 &lt;strong&gt;agent&lt;/strong&gt; 传递数据或者往最终存储系统传递数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Channel&lt;/strong&gt;：&lt;strong&gt;angent&lt;/strong&gt; 内部的数据传输通道，用于从 &lt;strong&gt;source&lt;/strong&gt; 将数据传递到 &lt;strong&gt;sink&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/flume/image003.png&#34; alt=&#34;image003&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_flume采集系统结构图&#34;&gt;1.3. Flume采集系统结构图&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;简单结构&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;单个agent采集数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/flume/image005.png&#34; alt=&#34;image005&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;复杂结构&lt;/p&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;多个agent之间串联&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/flume/image007.png&#34; alt=&#34;image007&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_flume实战案例&#34;&gt;2. Flume实战案例&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_flume的安装部署&#34;&gt;2.1. Flume的安装部署&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Flume的安装非常简单，只需要解压即可，当然，前提是已有hadoop环境&lt;br&gt;
上传安装包到数据源所在节点上&lt;br&gt;
然后解压  &lt;code&gt;tar -zxvf apache-flume-1.6.0-bin.tar.gz&lt;/code&gt;&lt;br&gt;
然后进入 &lt;strong&gt;flume&lt;/strong&gt; 的目录，修改 &lt;strong&gt;conf&lt;/strong&gt; 下的 &lt;strong&gt;flume-env.sh&lt;/strong&gt;，在里面配置 &lt;strong&gt;JAVA_HOME&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;根据数据采集的需求配置采集方案，描述在配置文件中(文件名可任意自定义)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;指定采集方案配置文件，在相应的节点上启动 &lt;strong&gt;flume agent&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;例子&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;先在flume的conf目录下新建一个文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;netcat-logger.conf&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 定义这个agent中各组件的名字
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 描述和配置source组件：r1
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# 描述和配置sink组件：k1
a1.sinks.k1.type = logger

# 描述和配置channel组件，此处使用是内存缓存的方式
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# 描述和配置source  channel   sink之间的连接关系
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动agent去采集数据&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/flume-ng agent -c conf \ &lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
-f conf/netcat-logger.conf \ &lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
-n a1 \ &lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
-Dflume.root.logger=INFO,console&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;colist arabic&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;-&lt;strong&gt;c&lt;/strong&gt; &lt;strong&gt;conf&lt;/strong&gt;   指定 &lt;strong&gt;flume&lt;/strong&gt; 自身的配置文件所在目录&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;-&lt;strong&gt;f&lt;/strong&gt; &lt;strong&gt;conf&lt;/strong&gt;/&lt;strong&gt;netcat-logger.con&lt;/strong&gt;  指定我们所描述的采集方案&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;-&lt;strong&gt;n&lt;/strong&gt; &lt;strong&gt;a1&lt;/strong&gt;  指定我们这个*agent*的名字&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;测试
先要往agent采集监听的端口上发送数据，让agent有数据可采
随便在一个能跟agent节点联网的机器上&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;telnet anget-hostname  port   （telnet localhost 44444）&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/flume/image009.png&#34; alt=&#34;image009&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_采集案例&#34;&gt;2.2. 采集案例&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Q &amp;amp; A &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;a href=&#34;http://www.voidcn.com/blog/lmh94604/article/p-6042484.html&#34; class=&#34;bare&#34;&gt;http://www.voidcn.com/blog/lmh94604/article/p-6042484.html&lt;/a&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_采集目录到hdfs&#34;&gt;2.2.1. 采集目录到HDFS&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;采集需求：某服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到HDFS中去&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;根据需求，首先定义以下3大要素&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;采集源&lt;/strong&gt;，即 &lt;strong&gt;source&lt;/strong&gt; ——监控文件目录 :  &lt;strong&gt;spooldir&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;下沉目标&lt;/strong&gt;，即 &lt;strong&gt;sink&lt;/strong&gt;——&lt;strong&gt;HDFS&lt;/strong&gt; 文件系统  :  &lt;strong&gt;hdfs&lt;/strong&gt; &lt;strong&gt;sink&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;source&lt;/strong&gt; 和 &lt;strong&gt;sink&lt;/strong&gt; 之间的传递通道——&lt;strong&gt;channel&lt;/strong&gt;，可用 &lt;strong&gt;file&lt;/strong&gt; &lt;strong&gt;channel&lt;/strong&gt; 也可以用内存 &lt;strong&gt;channel&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;配置文件编写&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#定义三大组件的名称
agent1.sources = source1
agent1.sinks = sink1
agent1.channels = channel1

# 配置source组件
agent1.sources.source1.type = spooldir
agent1.sources.source1.spoolDir = /home/hadoop/logs/
agent1.sources.source1.fileHeader = false

#配置拦截器
agent1.sources.source1.interceptors = i1
agent1.sources.source1.interceptors.i1.type = host
agent1.sources.source1.interceptors.i1.hostHeader = hostname

# 配置sink组件
agent1.sinks.sink1.type = hdfs
agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M
agent1.sinks.sink1.hdfs.filePrefix = access_log
agent1.sinks.sink1.hdfs.maxOpenFiles = 5000
agent1.sinks.sink1.hdfs.batchSize= 100
agent1.sinks.sink1.hdfs.fileType = DataStream
agent1.sinks.sink1.hdfs.writeFormat =Text
agent1.sinks.sink1.hdfs.rollSize = 102400
agent1.sinks.sink1.hdfs.rollCount = 1000000
agent1.sinks.sink1.hdfs.rollInterval = 60
#agent1.sinks.sink1.hdfs.round = true
#agent1.sinks.sink1.hdfs.roundValue = 10
#agent1.sinks.sink1.hdfs.roundUnit = minute
agent1.sinks.sink1.hdfs.useLocalTimeStamp = true
# Use a channel which buffers events in memory
agent1.channels.channel1.type = memory
agent1.channels.channel1.keep-alive = 120 &lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
agent1.channels.channel1.capacity = 500000 &lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
agent1.channels.channel1.transactionCapacity = 600 &lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;

# Bind the source and sink to the channel
agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;colist arabic&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;1&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;capacity&lt;/strong&gt;：默认该通道中最大的可以存储的 &lt;strong&gt;event&lt;/strong&gt; 数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;2&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;trasactionCapacity&lt;/strong&gt;：每次最大可以从 &lt;strong&gt;source&lt;/strong&gt; 中拿到或者送到 &lt;strong&gt;sink&lt;/strong&gt; 中的 &lt;strong&gt;event&lt;/strong&gt; 数量&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&#34;conum&#34; data-value=&#34;3&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;keep-alive&lt;/strong&gt;：&lt;strong&gt;event&lt;/strong&gt; 添加到通道中或者移出的允许时间&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_采集文件到hdfs&#34;&gt;2.2.2. 采集文件到HDFS&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;采集需求：&lt;br&gt;
比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;根据需求，首先定义以下3大要素&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;采集源，即 &lt;strong&gt;source&lt;/strong&gt; ——监控文件内容更新 : &lt;strong&gt;exec&lt;/strong&gt;  &#39;&lt;strong&gt;tail&lt;/strong&gt; -&lt;strong&gt;F&lt;/strong&gt; &lt;strong&gt;file&lt;/strong&gt;&#39;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;下沉目标，即 &lt;strong&gt;sink&lt;/strong&gt;——&lt;strong&gt;HDFS&lt;/strong&gt; 文件系统: &lt;strong&gt;hdfs&lt;/strong&gt; &lt;strong&gt;sink&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt; 和 &lt;strong&gt;sink&lt;/strong&gt; 之间的传递通道—— &lt;strong&gt;channel&lt;/strong&gt;，可用 &lt;strong&gt;file&lt;/strong&gt; &lt;strong&gt;channel&lt;/strong&gt; 也可以用 内存 &lt;strong&gt;channel&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;配置文件编写&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;agent1.sources = source1
agent1.sinks = sink1
agent1.channels = channel1

# Describe/configure tail -F source1
agent1.sources.source1.type = exec
agent1.sources.source1.command = tail -F /home/hadoop/logs/access_log
agent1.sources.source1.channels = channel1

#configure host for source
agent1.sources.source1.interceptors = i1
agent1.sources.source1.interceptors.i1.type = host
agent1.sources.source1.interceptors.i1.hostHeader = hostname

# Describe sink1
agent1.sinks.sink1.type = hdfs
#a1.sinks.k1.channel = c1
agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M
agent1.sinks.sink1.hdfs.filePrefix = access_log
agent1.sinks.sink1.hdfs.maxOpenFiles = 5000
agent1.sinks.sink1.hdfs.batchSize= 100
agent1.sinks.sink1.hdfs.fileType = DataStream
agent1.sinks.sink1.hdfs.writeFormat =Text
agent1.sinks.sink1.hdfs.rollSize = 102400
agent1.sinks.sink1.hdfs.rollCount = 1000000
agent1.sinks.sink1.hdfs.rollInterval = 60
agent1.sinks.sink1.hdfs.round = true
agent1.sinks.sink1.hdfs.roundValue = 10
agent1.sinks.sink1.hdfs.roundUnit = minute
agent1.sinks.sink1.hdfs.useLocalTimeStamp = true

# Use a channel which buffers events in memory
agent1.channels.channel1.type = memory
agent1.channels.channel1.keep-alive = 120
agent1.channels.channel1.capacity = 500000
agent1.channels.channel1.transactionCapacity = 600

# Bind the source and sink to the channel
agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_更多source和sink组件&#34;&gt;2.2.3. 更多source和sink组件&lt;/h4&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Flume支持众多的source和sink类型，详细手册可参考官方文档
&lt;a href=&#34;http://flume.apache.org/FlumeUserGuide.html&#34;&gt;FlumeUserGuide&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_工作流调度器azkaban&#34;&gt;3. 工作流调度器azkaban&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_为什么需要工作流调度系统&#34;&gt;3.1. 为什么需要工作流调度系统&lt;/h3&gt;
&lt;div class=&#34;ulist circle&#34;&gt;
&lt;ul class=&#34;circle&#34;&gt;
&lt;li&gt;
&lt;p&gt;一个完整的数据分析系统通常都是由大量任务单元组成：
&lt;strong&gt;shell&lt;/strong&gt; 脚本程序，&lt;strong&gt;java&lt;/strong&gt; 程序，&lt;strong&gt;mapreduce&lt;/strong&gt; 程序、&lt;strong&gt;hive&lt;/strong&gt; 脚本等&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;各任务单元之间存在时间先后及前后依赖关系&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行；&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;例如，我们可能有这样一个需求，某个业务系统每天产生20G原始数据，我们每天都要对其进行处理，处理步骤如下所示：&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;通过 &lt;strong&gt;Hadoop&lt;/strong&gt; 先将原始数据同步到 &lt;strong&gt;HDFS&lt;/strong&gt; 上；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;借助 &lt;strong&gt;MapReduce&lt;/strong&gt; 计算框架对原始数据进行转换，生成的数据以分区表的形式存储到多张 &lt;strong&gt;Hive&lt;/strong&gt; 表中；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需要对 &lt;strong&gt;Hive&lt;/strong&gt; 中多个表的数据进行 &lt;strong&gt;JOIN&lt;/strong&gt; 处理，得到一个明细数据 &lt;strong&gt;Hive&lt;/strong&gt; 大表；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将明细数据进行复杂的统计分析，得到结果报表信息；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_工作流调度实现方式&#34;&gt;3.2. 工作流调度实现方式&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;简单的任务调度&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;直接使用linux的crontab来定义；&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;复杂的任务调度&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;开发调度平台&lt;br&gt;
或使用现成的开源调度系统，比如ooize、azkaban等&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_常见工作流调度系统&#34;&gt;3.3. 常见工作流调度系统&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在hadoop领域，常见的工作流调度器有Oozie, Azkaban,Cascading,Hamake等&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_azkaban与oozie对比&#34;&gt;3.4. Azkaban与Oozie对比&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;对市面上最流行的两种调度器，给出以下详细对比，以供技术选型参考。总体来说，ooize相比azkaban是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不在意某些功能的缺失，轻量级调度器azkaban是很不错的候选对象。
详情如下：&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;功能&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;两者均可以调度mapreduce,pig,java,脚本工作流任务&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;两者均可以定时执行工作流任务&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;工作流定义&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban使用Properties文件定义工作流&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie使用XML文件定义工作流&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;工作流传参&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban支持直接传参，例如${input}&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie支持参数和EL表达式，例如${fs:dirSize(myInputDir)}&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;定时执行&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban的定时执行任务是基于时间的&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie的定时执行任务基于时间和输入数据&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;资源管理&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban有较严格的权限控制，如用户对工作流进行读/写/执行等操作&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie暂无严格的权限控制&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;工作流执行&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban有两种运行模式，分别是solo server mode(executor server和web server部署在同一台节点)和multi server mode(executor server和web server可以部署在不同节点)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie作为工作流服务器运行，支持多用户和多工作流&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;工作流管理&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban支持浏览器以及ajax方式操作工作流&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Oozie支持命令行、HTTP REST、Java API、浏览器操作工作流&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_azkaban介绍&#34;&gt;3.4.1. Azkaban介绍&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;它有如下功能特点：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist disc&#34;&gt;
&lt;ul class=&#34;disc&#34;&gt;
&lt;li&gt;
&lt;p&gt;Web用户界面&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方便上传工作流&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方便设置任务之间的关系&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调度工作流&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;认证/授权(权限的工作)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;能够杀死并重新启动工作流&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模块化和可插拔的插件机制&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;项目工作区&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;工作流和任务的日志记录和审计&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_azkaban安装部署&#34;&gt;3.4.2. Azkaban安装部署&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;准备工作&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;Azkaban Web服务器
    azkaban-web-server-2.5.0.tar.gz
Azkaban执行服务器
    azkaban-executor-server-2.5.0.tar.gz&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;MySQL&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;目前azkaban只支持 mysql,需安装mysql服务器,本文档中默认已安装好mysql服务器,并建立了 root用户,密码 root.

下载地址:http://azkaban.github.io/downloads.html&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;安装&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;将安装文件上传到集群,最好上传到安装 hive、sqoop的机器上,方便命令的执行
在当前用户目录下新建 azkabantools目录,用于存放源安装文件.新建azkaban目录,用于存放azkaban运行程序

azkaban web服务器安装
解压azkaban-web-server-2.5.0.tar.gz
命令:
    tar –zxvf azkaban-web-server-2.5.0.tar.gz
将解压后的 azkaban-web-server-2.5.0 移动到 azkaban目录中,并重新命名 webserver
命令:
    mv azkaban-web-server-2.5.0 ../azkaban
    cd ../azkaban
    mv azkaban-web-server-2.5.0  server&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;azkaban 执行服器安装&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;解压 azkaban-executor-server-2.5.0.tar.gz
命令:
    tar –zxvf azkaban-executor-server-2.5.0.tar.gz
将解压后的 azkaban-executor-server-2.5.0 移动到 azkaban目录中,并重新命名 executor
命令:
    mv azkaban-executor-server-2.5.0  ../azkaban
    cd ../azkaban
    mv azkaban-executor-server-2.5.0  executor&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;azkaban脚本导入&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;解压:
    azkaban-sql-script-2.5.0.tar.gz
命令:
    tar –zxvf azkaban-sql-script-2.5.0.tar.gz
将解压后的mysql 脚本,导入到mysql中:
进入mysql
mysql&amp;gt; create database azkaban;
mysql&amp;gt; use azkaban;
Database changed
mysql&amp;gt; source /home/hadoop/azkaban-2.5.0/create-all-sql-2.5.0.sql;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;创建SSL配置&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;参考地址:
    http://docs.codehaus.org/display/JETTY/How+to+configure+SSL
命令:
    keytool -keystore keystore -alias jetty -genkey -keyalg RSA
运行此命令后,会提示输入当前生成 keystor的密码及相应信息,输入的密码请劳记,信息如下:

输入keystore密码：
再次输入新密码:
您的名字与姓氏是什么？
  [Unknown]：
您的组织单位名称是什么？
  [Unknown]：
您的组织名称是什么？
  [Unknown]：
您所在的城市或区域名称是什么？
  [Unknown]：
您所在的州或省份名称是什么？
  [Unknown]：
该单位的两字母国家代码是什么
  [Unknown]：  CN
CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？
  [否]：  y

输入&amp;lt;jetty&amp;gt;的主密码
        （如果和 keystore 密码相同，按回车）：
再次输入新密码:
完成上述工作后,将在当前目录生成 keystore 证书文件,将keystore 考贝到 azkaban web服务器根目录中.
如: cp keystore azkaban/server&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;配置文件&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;注：先配置好服务器节点上的时区
1、先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可
2、拷贝该时区文件，覆盖系统本地时区配置
cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;azkaban web服务器配置&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;进入azkaban web服务器安装目录 conf目录&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;azkaban.properties&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#Azkaban Personalization Settings
azkaban.name=Test                           #服务器UI名称,用于服务器上方显示的名字
azkaban.label=My Local Azkaban                               #描述
azkaban.color=#FF3601                                                 #UI颜色
azkaban.default.servlet.path=/index                         #
web.resource.dir=web/                                                 #默认根web目录
default.timezone.id=Asia/Shanghai                           #默认时区,已改为亚洲/上海 默认为美国

#Azkaban UserManager class
user.manager.class=azkaban.user.XmlUserManager   #用户权限管理默认类
user.manager.xml.file=conf/azkaban-users.xml              #用户配置,具体配置参加下文

#Loader for projects
executor.global.properties=conf/global.properties    # global配置文件所在位置
azkaban.project.dir=projects                                                #

database.type=mysql                                                              #数据库类型
mysql.port=3306                                                                       #端口号
mysql.host=localhost                                                      #数据库连接IP
mysql.database=azkaban                                                       #数据库实例名
mysql.user=root                                                                 #数据库用户名
mysql.password=root                                                          #数据库密码
mysql.numconnections=100                                                  #最大连接数

# Velocity dev mode
velocity.dev.mode=false
# Jetty服务器属性.
jetty.maxThreads=25                                                               #最大线程数
jetty.ssl.port=8443                                                                   #Jetty SSL端口
jetty.port=8081                                                                         #Jetty端口
jetty.keystore=keystore                                                          #SSL文件名
jetty.password=123456                                                             #SSL文件密码
jetty.keypassword=123456                                                      #Jetty主密码 与 keystore文件相同
jetty.truststore=keystore                                                                #SSL文件名
jetty.trustpassword=123456                                                   # SSL文件密码

# 执行服务器属性
executor.port=12321                                                               #执行服务器端口

# 邮件设置
mail.sender=xxxxxxxx@163.com                                       #发送邮箱
mail.host=smtp.163.com                                                       #发送邮箱smtp地址
mail.user=xxxxxxxx                                       #发送邮件时显示的名称
mail.password=**********                                                 #邮箱密码
job.failure.email=xxxxxxxx@163.com                              #任务失败时发送邮件的地址
job.success.email=xxxxxxxx@163.com                            #任务成功时发送邮件的地址
lockdown.create.projects=false                                           #
cache.directory=cache                                                            #缓存目录&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;azkaban 执行服务器executor配置 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;进入执行服务器安装目录conf,修改azkaban.properties&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;azkaban.properties&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#Azkaban
default.timezone.id=Asia/Shanghai                                              #时区

# Azkaban JobTypes 插件配置
azkaban.jobtype.plugin.dir=plugins/jobtypes                   #jobtype 插件所在位置

#Loader for projects
executor.global.properties=conf/global.properties
azkaban.project.dir=projects

#数据库设置
database.type=mysql                                                                       #数据库类型(目前只支持mysql)
mysql.port=3306                                                                                #数据库端口号
mysql.host=192.168.20.200                                                           #数据库IP地址
mysql.database=azkaban                                                                #数据库实例名
mysql.user=root                                                                       #数据库用户名
mysql.password=root #数据库密码
mysql.numconnections=100                                                           #最大连接数

# 执行服务器配置
executor.maxThreads=50                                                                #最大线程数
executor.port=12321                                                               #端口号(如修改,请与web服务中一致)
executor.flow.threads=30                                                                #线程数&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;用户配置&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;进入azkaban web服务器conf目录,修改azkaban-users.xml
vi azkaban-users.xml 增加 管理员用户&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;azkaban-users.xml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;azkaban-users&amp;gt;
        &amp;lt;user username=&#34;azkaban&#34; password=&#34;azkaban&#34; roles=&#34;admin&#34; groups=&#34;azkaban&#34; /&amp;gt;
        &amp;lt;user username=&#34;metrics&#34; password=&#34;metrics&#34; roles=&#34;metrics&#34;/&amp;gt;
        &amp;lt;user username=&#34;admin&#34; password=&#34;admin&#34; roles=&#34;admin,metrics&#34; /&amp;gt;
        &amp;lt;role name=&#34;admin&#34; permissions=&#34;ADMIN&#34; /&amp;gt;
        &amp;lt;role name=&#34;metrics&#34; permissions=&#34;METRICS&#34;/&amp;gt;
&amp;lt;/azkaban-users&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;启动&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;web服务器
    在azkaban web服务器目录下执行启动命令
        bin/azkaban-web-start.sh
    注:在web服务器根目录运行
或者启动到后台
    nohup  bin/azkaban-web-start.sh  1&amp;gt;/tmp/azstd.out  2&amp;gt;/tmp/azerr.out &amp;amp;
执行服务器
    在执行服务器目录下执行启动命令
        bin/azkaban-executor-start.sh
    注:只能要执行服务器根目录运行

启动完成后,在浏览器(建议使用谷歌浏览器)中输入
https://服务器IP地址:8443 ,即可访问azkaban服务了.
在登录中输入刚才新的户用名及密码,点击 login.&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_azkaban实战&#34;&gt;3.4.3. Azkaban实战&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Azkaba内置的任务类型支持command、java&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Command类型单一job示例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建job描述文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vi command.job
#command.job
type=command
command=echo &#39;hello&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将job资源文件打包成zip文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;zip command.job&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过azkaban的web管理平台创建project并上传job压缩包
首先创建project&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/flume/image011.png&#34; alt=&#34;image011&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;上传zip包&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/flume/image013.png&#34; alt=&#34;image013&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动执行该job
image::/src/img/hadoop/flume/image015.png[]
---&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_command类型多job工作流flow&#34;&gt;Command类型多job工作流flow&lt;/h5&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建有依赖关系的多个job描述
第一个job：foo.job&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# foo.job
type=command
command=echo foo&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;第二个job：bar.job依赖foo.job&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# bar.job
type=command
dependencies=foo
command=echo bar&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将所有job资源文件打到一个zip包中&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/flume/image017.png&#34; alt=&#34;image017&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在azkaban的web管理界面创建工程并上传zip包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动工作流flow&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_hdfs操作任务&#34;&gt;HDFS操作任务&lt;/h5&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建job描述文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# fs.job
type=command
command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop fs -mkdir /azaz&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将job资源文件打包成zip文件&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/flume/image019.png&#34; alt=&#34;image019&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过azkaban的web管理平台创建project并上传job压缩包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动执行该job&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_mapreduce任务&#34;&gt;MAPREDUCE任务&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Mr任务依然可以使用command的job类型来执行&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建job描述文件，及mr程序jar包（示例中直接使用hadoop自带的example jar）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# mrwc.job
type=command
command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop  jar hadoop-mapreduce-examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将所有job资源文件打到一个zip包中&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/flume/image021.png&#34; alt=&#34;image021&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在azkaban的web管理界面创建工程并上传zip包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动job&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_hive脚本任务&#34;&gt;HIVE脚本任务&lt;/h5&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建job描述文件和hive脚本
Hive脚本： test.sql&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;use default;
drop table aztest;
create table aztest(id int,name string) row format delimited fields terminated by &#39;,&#39;;
load data inpath &#39;/aztest/hiveinput&#39; into table aztest;
create table azres as select * from aztest;
insert overwrite directory &#39;/aztest/hiveoutput&#39; select count(1) from aztest;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Job描述文件：hivef.job&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# hivef.job
type=command
command=/home/hadoop/apps/hive/bin/hive -f &#39;test.sql&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将所有job资源文件打到一个zip包中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在azkaban的web管理界面创建工程并上传zip包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动job&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_sqoop数据迁移&#34;&gt;4. sqoop数据迁移&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。
导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统；
导出数据：从Hadoop的文件系统中导出数据到关系数据库&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sqoop安装&#34;&gt;4.1. sqoop安装&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;安装sqoop的前提是已经具备java和hadoop的环境
. 下载并解压&lt;br&gt;
最新版下载地址http://ftp.wayne.edu/apache/sqoop/1.4.6/&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;修改配置文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ cd $SQOOP_HOME/conf
$ mv sqoop-env-template.sh sqoop-env.sh
#打开sqoop-env.sh并编辑下面几行：
export HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.6.1/
export HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.6.1/
export HIVE_HOME=/home/hadoop/apps/hive-1.2.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加入mysql的jdbc驱动包&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cp  ~/app/hive/lib/mysql-connector-java-5.1.28.jar   $SQOOP_HOME/lib/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;验证启动&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ cd $SQOOP_HOME/bin
$ sqoop-version
# 预期的输出：
15/12/17 14:52:32 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6
Sqoop 1.4.6 git commit id 5b34accaca7de251fc91161733f906af2eddbe83
Compiled by abe on Fri Aug 1 11:19:26 PDT 2015&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;到这里，整个Sqoop安装工作完成。&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sqoop的数据导入&#34;&gt;4.2. Sqoop的数据导入&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;“导入工具”导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据）&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;下面的语法用于将数据导入HDFS。&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ sqoop import (generic-args) (import-args)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_示例&#34;&gt;4.2.1. 示例&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;表数据 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在mysql中有一个库userdb中三个表：emp, emp_add和emp_contact&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;表emp&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;id      name    deg     salary  dept
1201    gopal   manager 50,000  TP
1202    manisha Proof reader    50,000  TP
1203    khalil  php dev 30,000  AC
1204    prasanth    php dev 30,000  AC
1205    kranthi admin   20,000  TP&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;表emp_add:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;id      hno     street  city
1201    288A    vgiri   jublee
1202    108I    aoc sec-bad
1203    144Z    pgutta  hyd
1204    78B old city    sec-bad
1205    720X    hitec   sec-bad&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;表emp_conn:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;id      phno    email
1201    2356742 gopal@tp.com
1202    1661663 manisha@tp.com
1203    8887776 khalil@ac.com
1204    9988774 prasanth@ac.com
1205    1231231 kranthi@tp.com&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_导入表表数据到hdfs&#34;&gt;导入表表数据到HDFS&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;下面的命令用于从MySQL数据库服务器中的emp表导入HDFS。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$bin/sqoop import \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--table emp   \
--m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;为了验证在HDFS导入的数据，请使用以下命令查看导入的数据&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-00000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;emp表的数据和字段之间用逗号(,)表示&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1201, gopal,    manager, 50000, TP
1202, manisha,  preader, 50000, TP
1203, kalil,    php dev, 30000, AC
1204, prasanth, php dev, 30000, AC
1205, kranthi,  admin,   20000, TP&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_导入关系表到hive&#34;&gt;导入关系表到HIVE&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --hive-import --m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_导入到hdfs指定目录&#34;&gt;导入到HDFS指定目录&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。
以下是指定目标目录选项的Sqoop导入命令的语法。::&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;--target-dir &amp;lt;new or exist directory in HDFS&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令是用来导入emp_add表数据到&#39;/queryresult&amp;#8217;目录。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop import \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--target-dir /queryresult \
--table emp --m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令是用来验证 /queryresult 目录中 emp_add表导入的数据形式。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$HADOOP_HOME/bin/hadoop fs -cat /queryresult/part-m-*&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;它会用逗号（，）分隔emp_add表的数据和字段。 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1201, 288A, vgiri,   jublee
1202, 108I, aoc,     sec-bad
1203, 144Z, pgutta,  hyd
1204, 78B,  oldcity, sec-bad
1205, 720C, hitech,  sec-bad&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_导入表数据子集&#34;&gt;导入表数据子集&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;我们可以导入表的使用Sqoop导入工具，&#34;where&#34;子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。
where子句的语法如下。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;--where &amp;lt;condition&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令用来导入emp_add表数据的子集。子集查询检索员工ID和地址，居住城市为：Secunderabad &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop import \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--where &#34;city =&#39;sec-bad&#39;&#34; \
--target-dir /wherequery \
--table emp_add --m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;按需导入&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop import \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--target-dir /wherequery2 \
--query &#39;select id,name,deg from emp WHERE  id&amp;gt;1207 and $CONDITIONS&#39; \
--split-by id \
--fields-terminated-by &#39;\t&#39; \
--m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令用来验证数据从emp_add表导入/wherequery目录&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$HADOOP_HOME/bin/hadoop fs -cat /wherequery/part-m-*&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;它用逗号（，）分隔 emp_add表数据和字段。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1202, 108I, aoc, sec-bad
1204, 78B, oldcity, sec-bad
1205, 720C, hitech, sec-bad&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_增量导入&#34;&gt;增量导入&lt;/h5&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;增量导入是仅导入新添加的表中的行的技术。
它需要添加‘incremental’, ‘check-column’, 和 ‘last-value’选项来执行增量导入。
下面的语法用于Sqoop导入命令增量选项。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;--incremental &amp;lt;mode&amp;gt;
--check-column &amp;lt;column name&amp;gt;
--last value &amp;lt;last check column value&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;假设新添加的数据转换成emp表如下：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;1206, satish p, grp des, 20000, GR&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令用于在EMP表执行增量导入。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop import \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--table emp --m 1 \
--incremental append \
--check-column id \
--last-value 1208&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;以下命令用于从emp表导入HDFS emp/ 目录的数据验证。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-*&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;它用逗号（，）分隔 emp_add表数据和字段。&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;1201, gopal,    manager, 50000, TP
1202, manisha,  preader, 50000, TP
1203, kalil,    php dev, 30000, AC
1204, prasanth, php dev, 30000, AC
1205, kranthi,  admin,   20000, TP
1206, satish p, grp des, 20000, GR&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;下面的命令是从表emp 用来查看修改或新添加的行&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ $HADOOP_HOME/bin/hadoop fs -cat /emp/part-m-*1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;这表示新添加的行用逗号（，）分隔emp表的字段。
1206, satish p, grp des, 20000, GR&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sqoop的数据导出&#34;&gt;4.3. Sqoop的数据导出&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;将数据从HDFS导出到RDBMS数据库
导出前，目标表必须存在于目标数据库中。
   默认操作是从将文件中的数据使用INSERT语句插入到表中
   更新模式下，是生成UPDATE语句更新表数据
语法
以下是export命令语法。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ sqoop export (generic-args) (export-args)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;示例
数据是在HDFS 中“EMP/”目录的emp_data文件中。所述emp_data如下：
1201, gopal,     manager, 50000, TP
1202, manisha,   preader, 50000, TP
1203, kalil,     php dev, 30000, AC
1204, prasanth,  php dev, 30000, AC
1205, kranthi,   admin,   20000, TP
1206, satish p,  grp des, 20000, GR&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;首先需要手动创建mysql中的目标表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ mysql
mysql&amp;gt; USE db;
mysql&amp;gt; CREATE TABLE employee (
   id INT NOT NULL PRIMARY KEY,
   name VARCHAR(20),
   deg VARCHAR(20),
   salary INT,
   dept VARCHAR(10));&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;然后执行导出命令&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop export \
--connect jdbc:mysql://hdp-node-01:3306/test \
--username root \
--password root \
--table employee \
--export-dir /user/hadoop/emp/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;验证表mysql命令行。&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mysql&amp;gt;select * from employee;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果给定的数据存储成功，那么可以找到数据在如下的employee表。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;+------+--------------+-------------+-------------------+--------+
| Id   | Name         | Designation | Salary            | Dept   |
+------+--------------+-------------+-------------------+--------+
| 1201 | gopal        | manager     | 50000             | TP     |
| 1202 | manisha      | preader     | 50000             | TP     |
| 1203 | kalil        | php dev     | 30000             | AC     |
| 1204 | prasanth     | php dev     | 30000             | AC     |
| 1205 | kranthi      | admin       | 20000             | TP     |
| 1206 | satish p     | grp des     | 20000             | GR     |
+------+--------------+-------------+-------------------+--------+&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_sqoop作业&#34;&gt;4.4. Sqoop作业&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注：Sqoop作业——将事先定义好的数据导入导出任务按照指定流程运行
语法
以下是创建Sqoop作业的语法。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;+&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;$ sqoop job (generic-args) (job-args)
   [-- [subtool-name] (subtool-args)]

$ sqoop-job (generic-args) (job-args)
   [-- [subtool-name] (subtool-args)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;创建作业(--create) &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;在这里，我们创建一个名为myjob，这可以从RDBMS表的数据导入到HDFS作业。&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/sqoop job --create myimportjob -- import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --m 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;该命令创建了一个从db库的employee表导入到HDFS文件的作业。&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;验证作业 (--list)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;‘--list’ 参数是用来验证保存的作业。下面的命令用来验证保存Sqoop作业的列表。
$ sqoop job --list
它显示了保存作业列表。
Available jobs:
myimportjob
检查作业(--show)
‘--show’ 参数用于检查或验证特定的工作，及其详细信息。以下命令和样本输出用来验证一个名为myjob的作业。
$ sqoop job --show myjob
它显示了工具和它们的选择，这是使用在myjob中作业情况。
Job: myjob
 Tool: import Options:
 ----------------------------
 direct.import = true
 codegen.input.delimiters.record = 0
 hdfs.append.dir = false
 db.table = employee
 ...
 incremental.last.value = 1206
 ...&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;执行作业 (--exec)
‘--exec’ 选项用于执行保存的作业。下面的命令用于执行保存的作业称为myjob。
$ sqoop job --exec myjob
它会显示下面的输出。
10/08/19 13:08:45 INFO tool.CodeGenTool: Beginning code generation
&amp;#8230;&amp;#8203;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Sqoop的原理
概述
Sqoop的原理其实就是将导入导出命令转化为mapreduce程序来执行，sqoop在接收到命令后，都要生成mapreduce程序&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;使用sqoop的代码生成工具可以方便查看到sqoop所生成的java代码，并可在此基础之上进行深入定制开发&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;代码定制
以下是Sqoop代码生成命令的语法：
$ sqoop-codegen (generic-args) (codegen-args)
$ sqoop-codegen (generic-args) (codegen-args)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;示例：以USERDB数据库中的表emp来生成Java代码为例。
下面的命令用来生成导入
$ sqoop-codegen \
--import
--connect jdbc:mysql://localhost/userdb \
--username root \
--table emp&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果命令成功执行，那么它就会产生如下的输出。
14/12/23 02:34:40 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5
14/12/23 02:34:41 INFO tool.CodeGenTool: Beginning code generation
……………….
14/12/23 02:34:42 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop
Note: /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/emp.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
14/12/23 02:34:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/emp.jar&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;验证: 查看输出目录下的文件
$ cd /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/
$ ls
emp.class
emp.jar
emp.java&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果想做深入定制导出，则可修改上述代码文件&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hive详解</title>
      <link>/post/bigdata/hadoop/hive%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/hadoop/hive%E8%AF%A6%E8%A7%A3/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hive详解&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hive简介&#34;&gt;1. Hive简介&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_什么是hive&#34;&gt;1.1. 什么是Hive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_为什么使用hive&#34;&gt;1.2. 为什么使用Hive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive的特点&#34;&gt;1.3. Hive的特点&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#__strong_hive_strong_架构&#34;&gt;2. &lt;strong&gt;Hive&lt;/strong&gt; 架构&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_基本组成&#34;&gt;2.1. 基本组成&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_各组件的基本功能&#34;&gt;2.2. 各组件的基本功能&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive与hadoop的关系&#34;&gt;2.3. Hive与Hadoop的关系&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive与传统数据库对比&#34;&gt;2.4. Hive与传统数据库对比&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_对比&#34;&gt;2.5. 对比&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive的数据存储&#34;&gt;2.6. Hive的数据存储&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#__strong_hive_strong_的安装部署&#34;&gt;2.7. &lt;strong&gt;HIVE&lt;/strong&gt; 的安装部署&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_使用方式&#34;&gt;2.7.1. 使用方式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive基本操作&#34;&gt;3. Hive基本操作&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_ddl操作&#34;&gt;3.1. DDL操作&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_创建表&#34;&gt;3.1.1. 创建表&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_具体实例&#34;&gt;具体实例&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_修改表&#34;&gt;3.1.2. 修改表&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_增加_删除分区&#34;&gt;增加/删除分区&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_重命名表&#34;&gt;重命名表&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_增加_更新列&#34;&gt;增加/更新列&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_显示命令&#34;&gt;显示命令&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_dml操作&#34;&gt;3.1.3. DML操作&lt;/a&gt;
&lt;ul class=&#34;sectlevel4&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_load&#34;&gt;Load&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_insert&#34;&gt;Insert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_select&#34;&gt;SELECT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive_join&#34;&gt;3.1.4. Hive Join&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive_shell参数&#34;&gt;4. Hive Shell参数&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hive命令行&#34;&gt;4.1. Hive命令行&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive参数配置方式&#34;&gt;4.2. Hive参数配置方式&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive函数&#34;&gt;5. Hive函数&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_内置运算符&#34;&gt;5.1. 内置运算符&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_内置函数&#34;&gt;5.2. 内置函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive自定义函数和transform&#34;&gt;5.3. Hive自定义函数和Transform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_自定义函数类别&#34;&gt;5.4. 自定义函数类别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_udf开发实例&#34;&gt;5.5. UDF开发实例&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_transform实现&#34;&gt;5.6. Transform实现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hive实战&#34;&gt;6. Hive实战&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hive_实战案例1_数据etl&#34;&gt;6.1. Hive 实战案例1——数据ETL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_实战案例2_访问时长统计&#34;&gt;6.2. 实战案例2——访问时长统计&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_实战案例3_级联求和&#34;&gt;6.3. 实战案例3——级联求和&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive简介&#34;&gt;1. Hive简介&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_什么是hive&#34;&gt;1.1. 什么是Hive&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_为什么使用hive&#34;&gt;1.2. 为什么使用Hive&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;直接使用hadoop所面临的问题 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;人员学习成本太高&lt;br&gt;
项目周期要求太短&lt;br&gt;
MapReduce实现复杂查询逻辑开发难度太大&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;为什么要使用Hive &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;避免了去写MapReduce，减少开发人员的学习成本。+
扩展功能很方便。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_hive的特点&#34;&gt;1.3. Hive的特点&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;可扩展 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Hive可以自由的扩展集群的规模，一般情况下不需要重启服务。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;延展性 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;容错 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;良好的容错性，节点出现问题SQL仍可完成执行&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;__strong_hive_strong_架构&#34;&gt;2. &lt;strong&gt;Hive&lt;/strong&gt; 架构&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/2017-03-17_155621.png&#34; alt=&#34;2017 03 17 155621&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Jobtracker &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;hadoop1.x中的组件，它的功能相当于： Resourcemanager+AppMaster&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;TaskTracker &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;相当于：  Nodemanager  +  yarnchild&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_基本组成&#34;&gt;2.1. 基本组成&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;用户接口&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;包括 CLI、JDBC/ODBC、WebGUI。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;元数据存储&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;通常是存储在关系数据库如 mysql,derby中。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;解释器、编译器、优化器、执行器&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_各组件的基本功能&#34;&gt;2.2. 各组件的基本功能&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;用户接口主要由三个&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;CLI、JDBC/ODBC和WebGUI。其中，CLI为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;元数据存储 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;解释器、编译器、优化器 &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行。&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_hive与hadoop的关系&#34;&gt;2.3. Hive与Hadoop的关系&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Hive利用HDFS存储数据，利用MapReduce查询数据&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/2017-03-17_160404.png&#34; alt=&#34;2017 03 17 160404&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_hive与传统数据库对比&#34;&gt;2.4. Hive与传统数据库对比&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/2017-03-17_160454.png&#34; alt=&#34;2017 03 17 160454&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_对比&#34;&gt;2.5. 对比&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;查询语言。由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据存储位置。Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据格式。Hive 中没有定义专门的数据格式，数据格式可以由用户指定，用户定义数据格式需要指定三个属性：列分隔符（通常为空格、”\t”、”\x001″）、行分隔符（”\n”）以及读取文件数据的方法（Hive 中默认有三个文件格式 TextFile，SequenceFile 以及 RCFile）。由于在加载数据的过程中，不需要从用户数据格式到 Hive 定义的数据格式的转换，因此，Hive 在加载的过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的 HDFS 目录中。而在数据库中，不同的数据库有不同的存储引擎，定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据更新。由于 Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive 中不支持对数据的改写和添加，所有的数据都是在加载的时候中确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO &amp;#8230;&amp;#8203;  VALUES 添加数据，使用 UPDATE &amp;#8230;&amp;#8203; SET 修改数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;索引。之前已经说过，Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 Key&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_hive的数据存储&#34;&gt;2.6. Hive的数据存储&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hive&lt;/strong&gt; 中所有的数据都存储在 &lt;strong&gt;HDFS&lt;/strong&gt;  中，没有专门的数据存储格式（可支持*Text* ，&lt;strong&gt;SequenceFile&lt;/strong&gt; ，&lt;strong&gt;ParquetFile&lt;/strong&gt; ，&lt;strong&gt;RCFILE&lt;/strong&gt; 等）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;只需要在创建表的时候告诉 &lt;strong&gt;Hive&lt;/strong&gt;  数据中的列分隔符和行分隔符，&lt;strong&gt;Hive&lt;/strong&gt;  就可以解析数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hive&lt;/strong&gt;  中包含以下数据模型：&lt;strong&gt;DB&lt;/strong&gt; 、&lt;strong&gt;Table&lt;/strong&gt; ，&lt;strong&gt;External&lt;/strong&gt;  &lt;strong&gt;Table&lt;/strong&gt; ，&lt;strong&gt;Partition&lt;/strong&gt; ，&lt;strong&gt;Bucket&lt;/strong&gt; 。&lt;/p&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;db&lt;/strong&gt; ：在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现为${&lt;strong&gt;hive&lt;/strong&gt; .&lt;strong&gt;metastore&lt;/strong&gt; .&lt;strong&gt;warehouse&lt;/strong&gt; .&lt;strong&gt;dir&lt;/strong&gt; }目录下一个文件夹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;table&lt;/strong&gt; ：在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现所属*db* 目录下一个文件夹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;external table&lt;/strong&gt; ：外部表, 与*table* 类似，不过其数据存放位置可以在任意指定路径
普通表: 删除表后, &lt;strong&gt;hdfs&lt;/strong&gt; 上的文件都删了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;External&lt;/strong&gt; 外部表删除后, &lt;strong&gt;hdfs&lt;/strong&gt; 上的文件没有删除, 只是把文件删除了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;partition&lt;/strong&gt; ：在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现为 &lt;strong&gt;table&lt;/strong&gt; 目录下的子目录&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;bucket&lt;/strong&gt; ：桶, 在 &lt;strong&gt;hdfs&lt;/strong&gt; 中表现为同一个表目录下根据 &lt;strong&gt;hash&lt;/strong&gt; 散列之后的多个文件, 会根据不同的文件把数据放到不同的文件中&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;__strong_hive_strong_的安装部署&#34;&gt;2.7. &lt;strong&gt;HIVE&lt;/strong&gt; 的安装部署&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;/post/bigdata/hadoop/hive&#34;&gt;hive 安装&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_使用方式&#34;&gt;2.7.1. 使用方式&lt;/h4&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Hive交互shell&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hive&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hive thrift服务&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;启动方式，（假如是在hadoop01上）&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;启动为前台 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hiveserver2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;启动为后台 &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;nohup bin/hiveserver2 1&amp;gt;/var/log/hiveserver.log 2&amp;gt;/var/log/hiveserver.err &amp;amp;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;启动成功后，可以在别的节点上用beeline去连接&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;方式（1）&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;code&gt;hive/bin/beeline&lt;/code&gt;  回车，进入beeline的命令界面&lt;br&gt;
输入命令连接 &lt;code&gt;hiveserver2&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;beeline&amp;gt; !connect jdbc:hive2//mini1:10000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;（hadoop01是hiveserver2所启动的那台主机名，端口默认是10000）&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;方式（2） &lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;或者启动就连接：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/beeline -u jdbc:hive2://mini1:10000 -n hadoop&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;接下来就可以做正常sql查询了&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive基本操作&#34;&gt;3. Hive基本操作&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_ddl操作&#34;&gt;3.1. DDL操作&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_创建表&#34;&gt;3.1.1. 创建表&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;建表语法&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name
   [(col_name data_type [COMMENT col_comment], ...)]
   [COMMENT table_comment]
   [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
   [CLUSTERED BY (col_name, col_name, ...)
   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
   [ROW FORMAT row_format]
   [STORED AS file_format]
   [LOCATION hdfs_path]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CREATE&lt;/strong&gt; &lt;strong&gt;TABLE&lt;/strong&gt; 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 &lt;strong&gt;IF&lt;/strong&gt; &lt;strong&gt;NOT&lt;/strong&gt; &lt;strong&gt;EXISTS&lt;/strong&gt; 选项来忽略这个异常。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;EXTERNAL&lt;/strong&gt; 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（&lt;strong&gt;LOCATION&lt;/strong&gt;），&lt;strong&gt;Hive&lt;/strong&gt; 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LIKE&lt;/strong&gt; 允许用户复制现有的表结构，但是不复制数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ROW&lt;/strong&gt; &lt;strong&gt;FORMAT&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]
| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;用户在建表的时候可以自定义 &lt;strong&gt;SerDe&lt;/strong&gt; 或者使用自带的 &lt;strong&gt;SerDe&lt;/strong&gt;。如果没有指定 &lt;strong&gt;ROW&lt;/strong&gt; &lt;strong&gt;FORMAT&lt;/strong&gt; 或者 &lt;strong&gt;ROW&lt;/strong&gt; &lt;strong&gt;FORMAT&lt;/strong&gt; &lt;strong&gt;DELIMITED&lt;/strong&gt;，将会使用自带的 &lt;strong&gt;SerDe&lt;/strong&gt;。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 &lt;strong&gt;SerDe&lt;/strong&gt;，&lt;strong&gt;Hive*通过 *SerDe&lt;/strong&gt; 确定表的具体的列的数据。&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;STORED&lt;/strong&gt; &lt;strong&gt;AS&lt;/strong&gt;
&lt;strong&gt;SEQUENCEFILE&lt;/strong&gt;|&lt;strong&gt;TEXTFILE&lt;/strong&gt;|&lt;strong&gt;RCFILE&lt;/strong&gt;
如果文件数据是纯文本，可以使用 &lt;strong&gt;STORED&lt;/strong&gt; &lt;strong&gt;AS&lt;/strong&gt; &lt;strong&gt;TEXTFILE&lt;/strong&gt;。如果数据需要压缩，使用 &lt;strong&gt;STORED&lt;/strong&gt; &lt;strong&gt;AS&lt;/strong&gt; &lt;strong&gt;SEQUENCEFILE&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CLUSTERED&lt;/strong&gt; &lt;strong&gt;BY&lt;/strong&gt;
对于每一个表（&lt;strong&gt;table&lt;/strong&gt;）或者分区， &lt;strong&gt;Hive&lt;/strong&gt; 可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。&lt;strong&gt;Hive&lt;/strong&gt; 也是 针对某一列进行桶的组织。&lt;strong&gt;Hive&lt;/strong&gt; 采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。
把表（或者分区）组织成桶（&lt;strong&gt;Bucket&lt;/strong&gt;）有两个理由：&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;获得更高的查询处理效率。桶为表加上了额外的结构，&lt;strong&gt;Hive&lt;/strong&gt; 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 &lt;strong&gt;Map&lt;/strong&gt; 端连接 （&lt;strong&gt;Map&lt;/strong&gt;-&lt;strong&gt;side&lt;/strong&gt; &lt;strong&gt;join&lt;/strong&gt;）高效的实现。比如 &lt;strong&gt;JOIN&lt;/strong&gt; 操作。对于 &lt;strong&gt;JOIN&lt;/strong&gt; 操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行 &lt;strong&gt;JOIN&lt;/strong&gt; 操作就可以，可以大大较少 &lt;strong&gt;JOIN&lt;/strong&gt; 的数据量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使取样（&lt;strong&gt;sampling&lt;/strong&gt;）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_具体实例&#34;&gt;具体实例&lt;/h5&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;创建内部表mytable&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image010.png&#34; alt=&#34;image010&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建外部表pageview&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image012.png&#34; alt=&#34;image012&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建分区表invites&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;create table student_p(Sno int,Sname string,Sex string,Sage int,Sdept string) partitioned by(part string) row format delimited fields terminated by &#39;,&#39;stored as textfile;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image014.png&#34; alt=&#34;image014&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建带桶的表student&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image016.png&#34; alt=&#34;image016&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_修改表&#34;&gt;3.1.2. 修改表&lt;/h4&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_增加_删除分区&#34;&gt;增加/删除分区&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION &#39;location1&#39; ] partition_spec [ LOCATION &#39;location2&#39; ] ...
partition_spec:
: PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)

ALTER TABLE table_name DROP partition_spec, partition_spec,...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;alter table student_p add partition(part=&#39;a&#39;) partition(part=&#39;b&#39;);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image018.png&#34; alt=&#34;image018&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image020.png&#34; alt=&#34;image020&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_重命名表&#34;&gt;重命名表&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name RENAME TO new_table_name&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image022.png&#34; alt=&#34;image022&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_增加_更新列&#34;&gt;增加/更新列&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image024.png&#34; alt=&#34;image024&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_显示命令&#34;&gt;显示命令&lt;/h5&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;show tables
show databases
show partitions
show functions
desc extended t_name;
desc formatted table_name;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_dml操作&#34;&gt;3.1.3. DML操作&lt;/h4&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_load&#34;&gt;Load&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO
TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;说明&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;filepath&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;相对路径，例如：&lt;strong&gt;project/data1&lt;/strong&gt;&lt;br&gt;
绝对路径，例如：&lt;strong&gt;/user/hive/project/data1&lt;/strong&gt;&lt;br&gt;
包含模式的完整 URI，列如：+
&lt;strong&gt;hdfs://namenode:9000/user/hive/project/data1&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LOCAL关键字&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。&lt;br&gt;
如果没有指定 LOCAL 关键字，则根据inpath中的uri 查找文件&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OVERWRITE 关键字&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。&lt;br&gt;
如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;加载相对路径数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image026.png&#34; alt=&#34;image026&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加载绝对路径数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image028.png&#34; alt=&#34;image028&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;加载包含模式数据&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image030.png&#34; alt=&#34;image030&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OVERWRITE关键字使用&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image032.png&#34; alt=&#34;image032&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_insert&#34;&gt;Insert&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement

Multiple inserts:
FROM from_statement
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1
[INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...

Dynamic partition inserts:
INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;基本模式插入&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image034.png&#34; alt=&#34;image034&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多插入模式&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image036.png&#34; alt=&#34;image036&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自动分区模式&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image038.png&#34; alt=&#34;image038&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;sect5&#34;&gt;
&lt;h6 id=&#34;_导出表数据&#34;&gt;导出表数据&lt;/h6&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...


multiple inserts:
FROM from_statement
INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1
[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;导出文件到本地&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image040.png&#34; alt=&#34;image040&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：
数据写入到文件系统时进行文本序列化，且每列用^A来区分，\n为换行符。用more命令查看时不容易看出分割符，可以使用:sed -e &#39;s/\x01/|/g&#39; filename 来查看。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;导出数据到HDFS&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image042.png&#34; alt=&#34;image042&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect4&#34;&gt;
&lt;h5 id=&#34;_select&#34;&gt;SELECT&lt;/h5&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list [HAVING condition]]
[CLUSTER BY col_list
  | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]
]
[LIMIT number]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;注：
. order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。
. sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&amp;gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。
. distribute by根据distribute by指定的内容将数据分到同一个reducer。
. Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;获取年龄大的3个学生&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image044.png&#34; alt=&#34;image044&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查询学生信息按年龄，降序排序&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image046.png&#34; alt=&#34;image046&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image048.png&#34; alt=&#34;image048&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image050.png&#34; alt=&#34;image050&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image052.png&#34; alt=&#34;image052&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;按学生名称汇总学生年龄&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image054.png&#34; alt=&#34;image054&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_hive_join&#34;&gt;3.1.4. Hive Join&lt;/h4&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;join_table:
  table_reference JOIN table_factor [join_condition]
  | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition
  | table_reference LEFT SEMI JOIN table_reference join_condition&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;Hive 支持等值连接（&lt;strong&gt;equality joins&lt;/strong&gt;）、外连接（&lt;strong&gt;outer joins&lt;/strong&gt;）和（&lt;strong&gt;left/right joins&lt;/strong&gt;）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。
另外，Hive 支持多于 2 个表的连接。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;写 join 查询时，需要注意几个关键点：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;只支持等值join
例如：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;  SELECT a.* FROM a JOIN b ON (a.id = b.id)
  SELECT a.* FROM a JOIN b
    ON (a.id = b.id AND a.department = b.department)
是正确的，然而:
  SELECT a.* FROM a JOIN b ON (a.id&amp;gt;b.id)
是错误的。&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以 join 多于 2 个表。&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;例如
  SELECT a.val, b.val, c.val FROM a JOIN b
    ON (a.key = b.key1) JOIN c ON (c.key = b.key2)
如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，例如：
  SELECT a.val, b.val, c.val FROM a JOIN b
    ON (a.key = b.key1) JOIN c
    ON (c.key = b.key1)
被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key。
SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1)
  JOIN c ON (c.key = b.key2)
而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 join。&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;join 时&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;每次 map/reduce 任务的逻辑
reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如：&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;SELECT a.val, b.val, c.val FROM a
    JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;所有表都使用同一个 &lt;strong&gt;join key&lt;/strong&gt;（使用 1 次 &lt;strong&gt;map/reduce&lt;/strong&gt; 任务计算）。
&lt;strong&gt;Reduce&lt;/strong&gt; 端会缓存 &lt;strong&gt;a&lt;/strong&gt; 表和 &lt;strong&gt;b&lt;/strong&gt; 表的记录，然后每次取得一个 c 表的记录就计算一次 &lt;strong&gt;join&lt;/strong&gt; 结果，类似的还有：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;  SELECT a.val, b.val, c.val FROM a
    JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;这里用了 2 次 &lt;strong&gt;map/reduce&lt;/strong&gt; 任务。第一次缓存 &lt;strong&gt;a&lt;/strong&gt; 表，用 &lt;strong&gt;b&lt;/strong&gt; 表序列化；第二次缓存第一次 &lt;strong&gt;map/reduce&lt;/strong&gt; 任务的结果，然后用 c 表序列化。&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LEFT&lt;/strong&gt;，&lt;strong&gt;RIGHT&lt;/strong&gt; 和 &lt;strong&gt;FULL&lt;/strong&gt; &lt;strong&gt;OUTER&lt;/strong&gt; 关键字用于处理 &lt;strong&gt;join&lt;/strong&gt; 中空记录的情况
例如：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;  SELECT a.val, b.val FROM
    a LEFT OUTER JOIN b ON (a.key=b.key)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;对应所有 &lt;strong&gt;a&lt;/strong&gt; 表中的记录都有一条记录输出。&lt;br&gt;
输出的结果应该是 &lt;strong&gt;a.val&lt;/strong&gt;, &lt;strong&gt;b.val&lt;/strong&gt;，当 &lt;strong&gt;a.key&lt;/strong&gt;=&lt;strong&gt;b.key&lt;/strong&gt; 时，而当 &lt;strong&gt;b.key&lt;/strong&gt; 中找不到等值的 &lt;strong&gt;a.key&lt;/strong&gt; 记录时也会输出:
&lt;strong&gt;a.val&lt;/strong&gt;, &lt;strong&gt;NULL&lt;/strong&gt;
所以 &lt;strong&gt;a&lt;/strong&gt; 表中的所有记录都被保留了；
&lt;strong&gt;a&lt;/strong&gt; &lt;strong&gt;RIGHT&lt;/strong&gt; &lt;strong&gt;OUTER&lt;/strong&gt; &lt;strong&gt;JOIN&lt;/strong&gt; &lt;strong&gt;b&lt;/strong&gt; 会保留所有 &lt;strong&gt;b&lt;/strong&gt; 表的记录。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Join&lt;/strong&gt; 发生在 &lt;strong&gt;WHERE&lt;/strong&gt; 子句之前。如果你想限制 &lt;strong&gt;join&lt;/strong&gt; 的输出，应该在 &lt;strong&gt;WHERE&lt;/strong&gt; 子句中写过滤条件——或是在 &lt;strong&gt;join&lt;/strong&gt; 子句中写。这里面一个容易混淆的问题是表分区的情况：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;  SELECT a.val, b.val FROM a
  LEFT OUTER JOIN b ON (a.key=b.key)
  WHERE a.ds=&#39;2009-07-07&#39; AND b.ds=&#39;2009-07-07&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;会 &lt;strong&gt;join&lt;/strong&gt; &lt;strong&gt;a&lt;/strong&gt; 表到 &lt;strong&gt;b&lt;/strong&gt; 表（&lt;strong&gt;OUTER&lt;/strong&gt; &lt;strong&gt;JOIN&lt;/strong&gt;），列出 &lt;strong&gt;a.val&lt;/strong&gt; 和 &lt;strong&gt;b.val&lt;/strong&gt; 的记录。&lt;strong&gt;WHERE&lt;/strong&gt; 从句中可以使用其他列作为过滤条件。但是，如前所述，如果 &lt;strong&gt;b&lt;/strong&gt; 表中找不到对应 &lt;strong&gt;a&lt;/strong&gt; 表的记录，&lt;strong&gt;b&lt;/strong&gt; 表的所有列都会列出 &lt;strong&gt;NULL&lt;/strong&gt;，包括 &lt;strong&gt;ds&lt;/strong&gt; 列。也就是说，&lt;strong&gt;join&lt;/strong&gt; 会过滤 &lt;strong&gt;b&lt;/strong&gt; 表中不能找到匹配 &lt;strong&gt;a&lt;/strong&gt; 表 &lt;strong&gt;join&lt;/strong&gt; &lt;strong&gt;key&lt;/strong&gt; 的所有记录。这样的话，&lt;strong&gt;LEFT&lt;/strong&gt; &lt;strong&gt;OUTER&lt;/strong&gt; 就使得查询结果与 &lt;strong&gt;WHERE&lt;/strong&gt; 子句无关了。解决的办法是在 &lt;strong&gt;OUTER&lt;/strong&gt; &lt;strong&gt;JOIN&lt;/strong&gt; 时使用以下语法：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;  SELECT a.val, b.val FROM a LEFT OUTER JOIN b
  ON (a.key=b.key AND
      b.ds=&#39;2009-07-07&#39; AND
      a.ds=&#39;2009-07-07&#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;这一查询的结果是预先在 &lt;strong&gt;join&lt;/strong&gt; 阶段过滤过的，所以不会存在上述问题。这一逻辑也可以应用于 &lt;strong&gt;RIGHT&lt;/strong&gt; 和 &lt;strong&gt;FULL&lt;/strong&gt; 类型的 &lt;strong&gt;join&lt;/strong&gt; 中。&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Join&lt;/strong&gt; 是不能交换位置的。无论是 &lt;strong&gt;LEFT&lt;/strong&gt; 还是 &lt;strong&gt;RIGHT&lt;/strong&gt; &lt;strong&gt;join&lt;/strong&gt;，都是左连接的。&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;SELECT a.val1, a.val2, b.val, c.val
  FROM a
  JOIN b ON (a.key = b.key)
  LEFT OUTER JOIN c ON (a.key = c.key)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;先 &lt;strong&gt;join&lt;/strong&gt; &lt;strong&gt;a&lt;/strong&gt; 表到 &lt;strong&gt;b&lt;/strong&gt; 表，丢弃掉所有 &lt;strong&gt;join&lt;/strong&gt; &lt;strong&gt;key&lt;/strong&gt; 中不匹配的记录，然后用这一中间结果和 &lt;strong&gt;c&lt;/strong&gt; 表做 &lt;strong&gt;join&lt;/strong&gt;。这一表述有一个不太明显的问题，就是当一个 &lt;strong&gt;key&lt;/strong&gt; 在 &lt;strong&gt;a&lt;/strong&gt; 表和 &lt;strong&gt;c&lt;/strong&gt; 表都存在，但是 &lt;strong&gt;b&lt;/strong&gt; 表中不存在的时候：整个记录在第一次 &lt;strong&gt;join&lt;/strong&gt;，即 &lt;strong&gt;a&lt;/strong&gt; &lt;strong&gt;JOIN&lt;/strong&gt; &lt;strong&gt;b&lt;/strong&gt; 的时候都被丢掉了（包括*a.val*1，&lt;strong&gt;a.val*2和*a.key&lt;/strong&gt;），然后我们再和 &lt;strong&gt;c&lt;/strong&gt; 表 &lt;strong&gt;join&lt;/strong&gt; 的时候，如果 &lt;strong&gt;c.key&lt;/strong&gt; 与 &lt;strong&gt;a.key&lt;/strong&gt; 或 &lt;strong&gt;b.key&lt;/strong&gt; 相等，就会得到这样的结果：&lt;strong&gt;NULL&lt;/strong&gt;, &lt;strong&gt;NULL&lt;/strong&gt;, &lt;strong&gt;NULL&lt;/strong&gt;, &lt;strong&gt;c.val&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;获取已经分配班级的学生姓名&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image056.png&#34; alt=&#34;image056&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;获取尚未分配班级的学生姓名&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image058.png&#34; alt=&#34;image058&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LEFT  SEMI  JOIN是IN/EXISTS的高效实现&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image060.png&#34; alt=&#34;image060&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive_shell参数&#34;&gt;4. Hive Shell参数&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_hive命令行&#34;&gt;4.1. Hive命令行&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;语法结构&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hive [-hiveconf x=y]* [&amp;lt;-i filename&amp;gt;]* [&amp;lt;-f filename&amp;gt;|&amp;lt;-e query-string&amp;gt;] [-S]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;说明：&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;-i&lt;/strong&gt; 从文件初始化HQL。&lt;br&gt;
&lt;strong&gt;-e&lt;/strong&gt; 从命令行执行指定的HQL&lt;br&gt;
&lt;strong&gt;-f&lt;/strong&gt; 执行HQL脚本&lt;br&gt;
&lt;strong&gt;-v&lt;/strong&gt; 输出执行的HQL语句到控制台&lt;br&gt;
&lt;strong&gt;-p&lt;/strong&gt; &amp;lt;port&amp;gt; connect to Hive Server on port number&lt;br&gt;
&lt;strong&gt;-hiveconf&lt;/strong&gt; x=y Use this to set hive/hadoop configuration variables.&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;具体实例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;运行一个查询&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image062.png&#34; alt=&#34;image062&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;运行一个文件&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image064.png&#34; alt=&#34;image064&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;运行参数文件&lt;/p&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/hadoop/image066.png&#34; alt=&#34;image066&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_hive参数配置方式&#34;&gt;4.2. Hive参数配置方式&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;Hive参数大全&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;a href=&#34;https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties&#34; class=&#34;bare&#34;&gt;https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;literalblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有起作用？这通常是错误的设定方式导致的。&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;对于一般参数，有以下三种设定方式&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;配置文件&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;命令行参数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参数声明&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;配置文件&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Hive的配置文件包括&lt;/p&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;用户自定义配置文件：&lt;code&gt;$HIVE_CONF_DIR/hive-site.xml&lt;/code&gt;&lt;br&gt;
默认配置文件：&lt;code&gt;$HIVE_CONF_DIR/hive-default.xml&lt;/code&gt;&lt;br&gt;
用户自定义配置会覆盖默认配置。
另外，&lt;strong&gt;Hive&lt;/strong&gt; 也会读入 &lt;strong&gt;Hadoop&lt;/strong&gt; 的配置，因为*Hive*是作为*Hadoop*的客户端启动的， &lt;strong&gt;Hive&lt;/strong&gt; 的配置会覆盖 &lt;strong&gt;Hadoop&lt;/strong&gt; 的配置。
配置文件的设定对本机启动的所有Hive进程都有效。&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;命令行参数&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;启动 &lt;strong&gt;Hive&lt;/strong&gt;（客户端或Server方式）时，可以在命令行添加 &lt;strong&gt;-hiveconf param=value&lt;/strong&gt; 来设定参数，&lt;br&gt;
例如：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hive -hiveconf hive.root.logger=INFO,console&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;这一设定对本次启动的 &lt;strong&gt;Session&lt;/strong&gt;（对于 &lt;strong&gt;Server&lt;/strong&gt; 方式启动，则是所有请求的 &lt;strong&gt;Sessions&lt;/strong&gt; ）有效。&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;参数声明&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;可以在 &lt;strong&gt;HQL&lt;/strong&gt; 中使用 &lt;strong&gt;SET&lt;/strong&gt; 关键字设定参数，例如：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;set mapred.reduce.tasks=100;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;这一设定的作用域也是 &lt;strong&gt;session&lt;/strong&gt; 级的。&lt;br&gt;
上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。&lt;br&gt;
注意某些系统级的参数，例如 &lt;strong&gt;log4j&lt;/strong&gt; 相关的设定，必须用前两种方式设定，因为那些参数的读取在 &lt;strong&gt;Session&lt;/strong&gt; 建立以前已经完成了。&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive函数&#34;&gt;5. Hive函数&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_内置运算符&#34;&gt;5.1. 内置运算符&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;内容较多，见《Hive官方文档》&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_内置函数&#34;&gt;5.2. 内置函数&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;内容较多，见《Hive官方文档》&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_hive自定义函数和transform&#34;&gt;5.3. Hive自定义函数和Transform&lt;/h3&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;当 &lt;strong&gt;Hive&lt;/strong&gt; 提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（&lt;strong&gt;UDF：user-defined function&lt;/strong&gt;）。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_自定义函数类别&#34;&gt;5.4. 自定义函数类别&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;UDF&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数）
&lt;strong&gt;UDAF&lt;/strong&gt;（用户定义聚集函数）：接收多个输入数据行，并产生一个输出数据行。（&lt;strong&gt;count&lt;/strong&gt;，&lt;strong&gt;max&lt;/strong&gt;）&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_udf开发实例&#34;&gt;5.5. UDF开发实例&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;先开发一个 &lt;strong&gt;java&lt;/strong&gt; 类，继承 &lt;strong&gt;UDF&lt;/strong&gt; ，并重载 &lt;strong&gt;evaluate&lt;/strong&gt; 方法&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;package cn.itcast.bigdata.udf
import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.Text;

public final class Lower extends UDF{
  public Text evaluate(final Text s){
    if(s==null){return null;}
    return new Text(s.toString().toLowerCase());
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;打成 &lt;strong&gt;jar&lt;/strong&gt; 包上传到服务器&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将 &lt;strong&gt;jar&lt;/strong&gt; 包添加到 &lt;strong&gt;hive&lt;/strong&gt; 的 &lt;strong&gt;classpath&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hive&amp;gt;add JAR /home/hadoop/udf.jar;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建临时函数与开发好的 &lt;strong&gt;java class&lt;/strong&gt; 关联&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hive&amp;gt;create temporary function toprovince as &#39;cn.itcast.bigdata.udf.ToProvince&#39;;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;即可在 &lt;strong&gt;hql&lt;/strong&gt; 中使用自定义的函数 &lt;strong&gt;strip&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;select strip(name),age from t_test;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_transform实现&#34;&gt;5.6. Transform实现&lt;/h3&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;strong&gt;Hive&lt;/strong&gt; 的 &lt;strong&gt;TRANSFORM&lt;/strong&gt; 关键字提供了在SQL中调用自写脚本的功能
适合实现 &lt;strong&gt;Hive&lt;/strong&gt; 中没有的功能又不想写UDF的情况&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;使用示例1&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;下面这句sql就是借用了 &lt;strong&gt;weekday_mapper.py&lt;/strong&gt; 对数据进行了处理&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;CREATE TABLE u_data_new (
  movieid INT,
  rating INT,
  weekday INT,
  userid INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY &#39;\t&#39;;

add FILE weekday_mapper.py;

INSERT OVERWRITE TABLE u_data_new
SELECT
  TRANSFORM (movieid, rating, unixtime,userid)
  USING &#39;python weekday_mapper.py&#39;
  AS (movieid, rating, weekday,userid)
FROM u_data;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;weekday_mapper.py&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#!/bin/python
import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  movieid, rating, unixtime,userid = line.split(&#39;\t&#39;)
  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
  print &#39;\t&#39;.join([movieid, rating, str(weekday),userid])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;使用示例2&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;下面的例子则是使用了 &lt;strong&gt;shell&lt;/strong&gt; 的 &lt;strong&gt;cat&lt;/strong&gt; 命令来处理数据&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;FROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING &#39;/bin/cat&#39; WHERE a.ds &amp;gt; &#39;2008-08-09&#39;;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive实战&#34;&gt;6. Hive实战&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_hive_实战案例1_数据etl&#34;&gt;6.1. Hive 实战案例1——数据ETL&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;需求&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;对web点击流日志基础数据表进行etl（按照仓库模型设计）&lt;br&gt;
按各时间维度统计来源域名top10&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;已有数据表 t_orgin_weblog &lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;+------------------+------------+----------+--+
|     col_name     | data_type  | comment  |
+------------------+------------+----------+--+
| valid            | string     |          |
| remote_addr      | string     |          |
| remote_user      | string     |          |
| time_local       | string     |          |
| request          | string     |          |
| status           | string     |          |
| body_bytes_sent  | string     |          |
| http_referer     | string     |          |
| http_user_agent  | string     |          |
+------------------+------------+----------+--+&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;数据示例&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;| true|1.162.203.134| - | 18/Sep/2013:13:47:35| /images/my.jpg                        | 200| 19939 | &#34;http://www.angularjs.cn/A0d9&#34;                      | &#34;Mozilla/5.0 (Windows   |

| true|1.202.186.37 | - | 18/Sep/2013:15:39:11| /wp-content/uploads/2013/08/windjs.png| 200| 34613 | &#34;http://cnodejs.org/topic/521a30d4bee8d3cb1272ac0f&#34; | &#34;Mozilla/5.0 (Macintosh;|&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;实现步骤&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;对原始数据进行抽取转换
将来访 &lt;strong&gt;url&lt;/strong&gt; 分离出 &lt;strong&gt;host&lt;/strong&gt;  &lt;strong&gt;path&lt;/strong&gt;  &lt;strong&gt;query&lt;/strong&gt;  &lt;strong&gt;query&lt;/strong&gt; &lt;strong&gt;id&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;drop table if exists t_etl_referurl;
create table t_etl_referurl as
SELECT a.*,b.*
FROM t_orgin_weblog a LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, &#34;\&#34;&#34;, &#34;&#34;), &#39;HOST&#39;, &#39;PATH&#39;,&#39;QUERY&#39;, &#39;QUERY:id&#39;) b as host, path, query, query_id&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从前述步骤进一步分离出日期时间形成 &lt;strong&gt;ETL&lt;/strong&gt; 明细表 &lt;strong&gt;t_etl_detail&lt;/strong&gt;    day tm&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;drop table if exists t_etl_detail;
create table t_etl_detail as
select b.*,substring(time_local,0,11) as daystr,
substring(time_local,13) as tmstr,
substring(time_local,4,3) as month,
substring(time_local,0,2) as day,
substring(time_local,13,2) as hour
from t_etl_referurl b;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对 &lt;strong&gt;etl&lt;/strong&gt; 数据进行分区(包含所有数据的结构化信息)&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;drop table t_etl_detail_prt;
create table t_etl_detail_prt(
valid                   string,
remote_addr            string,
remote_user            string,
time_local               string,
request                 string,
status                  string,
body_bytes_sent         string,
http_referer             string,
http_user_agent         string,
host                   string,
path                   string,
query                  string,
query_id               string,
daystr                 string,
tmstr                  string,
month                  string,
day                    string,
hour                   string)
partitioned by (mm string,dd string);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;导入数据&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;insert into table t_etl_detail_prt partition(mm=&#39;Sep&#39;,dd=&#39;18&#39;)
select * from t_etl_detail where daystr=&#39;18/Sep/2013&#39;;

insert into table t_etl_detail_prt partition(mm=&#39;Sep&#39;,dd=&#39;19&#39;)
select * from t_etl_detail where daystr=&#39;19/Sep/2013&#39;;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分个时间维度统计各 &lt;strong&gt;referer_host&lt;/strong&gt; 的访问次数并排序&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;create table t_refer_host_visit_top_tmp as
select referer_host,count(*) as counts,mm,dd,hh from t_display_referer_counts group by hh,dd,mm,referer_host order by hh asc,dd asc,mm asc,counts desc;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;来源访问次数 &lt;strong&gt;topn&lt;/strong&gt; 各时间维度 &lt;strong&gt;URL&lt;/strong&gt;
取各时间维度的 &lt;strong&gt;referer_host&lt;/strong&gt; 访问次数 &lt;strong&gt;topn&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;select * from (select referer_host,counts,concat(hh,dd),row_number() over (partition by concat(hh,dd) order by concat(hh,dd) asc) as od from t_refer_host_visit_top_tmp) t where od&amp;lt;=3;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_实战案例2_访问时长统计&#34;&gt;6.2. 实战案例2——访问时长统计&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;需求&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;从web日志中统计每日访客平均停留时间&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;实现步骤&lt;/dt&gt;
&lt;dd&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;由于要从大量请求中分辨出用户的各次访问，逻辑相对复杂，通过 &lt;strong&gt;hive&lt;/strong&gt; 直接实现有困难，因此编写一个 &lt;strong&gt;mr&lt;/strong&gt; 程序来求出访客访问信息（详见代码）
启动 &lt;strong&gt;mr&lt;/strong&gt; 程序获取结果：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;[hadoop@hdp-node-01 ~]$ hadoop jar weblog.jar cn.itcast.bigdata.hive.mr.UserStayTime /weblog/input /weblog/stayout&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将 &lt;strong&gt;mr&lt;/strong&gt; 的处理结果导入 &lt;strong&gt;hive&lt;/strong&gt; 表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;drop table t_display_access_info_tmp;
create table t_display_access_info_tmp(remote_addr string,firt_req_time string,last_req_time string,stay_long bigint)
row format delimited fields terminated by &#39;\t&#39;;

load data inpath &#39;/weblog/stayout4&#39; into table t_display_access_info_tmp;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;得出访客访问信息表 &lt;strong&gt;t_display_access_info&lt;/strong&gt;
由于有一些访问记录是单条记录，mr程序处理处的结果给的时长是0，所以考虑给单次请求的停留时间一个默认市场30秒&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;drop table t_display_access_info;
create table t_display_access_info as
select remote_addr,firt_req_time,last_req_time,
case stay_long
when 0 then 30000
else stay_long
end as stay_long
from t_display_access_info_tmp;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;统计所有用户停留时间平均值&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;select avg(stay_long) from t_display_access_info;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_实战案例3_级联求和&#34;&gt;6.3. 实战案例3——级联求和&lt;/h3&gt;
&lt;div class=&#34;dlist&#34;&gt;
&lt;dl&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;需求&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;有如下访客访问次数统计表 &lt;strong&gt;t_access_times&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;访客    月份       访问次数
A     2015-01-02      5
A     2015-01-03      15
B     2015-01-01      5
A     2015-01-04      8
B     2015-01-05      25
A     2015-01-06      5
A     2015-02-02      4
A     2015-02-06      6
B     2015-02-06      10
B     2015-02-07      5
……    ……              ……&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;需要输出报表：&lt;strong&gt;t_access_times_accumulate&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre&gt;访客  月份     月访问总计    累计访问总计
A     2015-01     33              33
A     2015-02     10              43
…….   …….         …….             …….
B     2015-01     30              30
B     2015-02     15              45
…….   …….         …….             …….&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&#34;hdlist1&#34;&gt;实现步骤&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;可以用一个hql语句即可实现：&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;select A.username,A.month,max(A.salary) as salary,sum(B.salary) as accumulate
from
(select username,month,sum(salary) as salary from t_access_times group by username,month) A
inner join
(select username,month,sum(salary) as salary from t_access_times group by username,month) B
on
A.username=B.username
where B.month &amp;lt;= A.month
group by A.username,A.month
order by A.username,A.month;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hive</title>
      <link>/post/bigdata/hadoop/hive/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/hadoop/hive/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hive&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hive只在一个节点上安装即可&#34;&gt;1. Hive只在一个节点上安装即可&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hive只在一个节点上安装即可&#34;&gt;1. Hive只在一个节点上安装即可&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;上传tar包&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;tar -zxvf hive-1.2.1.tar.gz -C /hadoop/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装mysql数据库（切换到root用户）（装在哪里没有限制，只有能联通hadoop集群的节点）&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;mysql.yml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;version: &#39;2&#39;
services:
  mysql:
    image: dishui.io:5000/mysql:5.5.52
    container_name: mysql
    environment:
      - &#34;MYSQL_ROOT_PASSWORD=111111&#34;
    ports:
      - &#34;3306:3306&#34;
    networks:
      - hadoop
networks:
  hadoop:
    external: true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置 &lt;strong&gt;hive&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;olist loweralpha&#34;&gt;
&lt;ol class=&#34;loweralpha&#34; type=&#34;a&#34;&gt;
&lt;li&gt;
&lt;p&gt;配置 &lt;strong&gt;HIVE_HOME&lt;/strong&gt; 环境变量  &lt;strong&gt;vi conf/hive-env.sh&lt;/strong&gt; 配置其中的 &lt;strong&gt;$hadoop_home&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置元数据库信息   &lt;strong&gt;vi hive-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;jdbc:mysql://mysql:3306/hive?createDatabaseIfNotExist=true&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;JDBC connect string for a JDBC metastore&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;Driver class name for a JDBC metastore&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;root&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;username to use against metastore database&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;

	&amp;lt;property&amp;gt;
		&amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt;
		&amp;lt;value&amp;gt;111111&amp;lt;/value&amp;gt;
		&amp;lt;description&amp;gt;password to use against metastore database&amp;lt;/description&amp;gt;
	&amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装 &lt;strong&gt;hive&lt;/strong&gt; 和 &lt;strong&gt;mysql&lt;/strong&gt; 完成后，将 &lt;strong&gt;mysql&lt;/strong&gt; 的连接 &lt;strong&gt;jar&lt;/strong&gt; 包拷贝到 &lt;strong&gt;$HIVE_HOME/lib&lt;/strong&gt; 目录下&lt;/p&gt;
&lt;div class=&#34;openblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;如果出现没有权限的问题，在 &lt;strong&gt;mysql&lt;/strong&gt; 授权(在安装 &lt;strong&gt;mysql&lt;/strong&gt; 的机器上执行)&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mysql -uroot -p&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;执行下面的语句  &lt;strong&gt;.&lt;/strong&gt;:所有库下的所有表   %：任何IP地址或主机都可以连接&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;111111&#39; WITH GRANT OPTION;
FLUSH PRIVILEGES;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Jline&lt;/strong&gt; 包版本不一致的问题，需要拷贝 &lt;strong&gt;hive&lt;/strong&gt; 的 &lt;strong&gt;lib&lt;/strong&gt; 目录中 &lt;strong&gt;jline.2.12.jar&lt;/strong&gt; 的 &lt;strong&gt;jar&lt;/strong&gt; 包替换掉 &lt;strong&gt;hadoop&lt;/strong&gt; 中的
&lt;strong&gt;/home/hadoop/app/hadoop-2.6.4/share/hadoop/yarn/lib/jline-0.9.94.jar&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动 &lt;strong&gt;hive&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;bin/hive&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;建表(默认是内部表)&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;create table trade_detail(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39;;
# 建分区表
create table td_part(id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by &#39;\t&#39;;
# 建外部表
create external table td_ext(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &#39;\t&#39; location &#39;/td_ext&#39;;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建分区表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 普通表和分区表区别：有大量数据增加的需要建分区表
create table book (id bigint, name string) partitioned by (pubdate string) row format delimited fields terminated by &#39;\t&#39;;

# 分区表加载数据
load data local inpath &#39;./book.txt&#39; overwrite into table book partition (pubdate=&#39;2010-08-22&#39;);

load data local inpath &#39;/root/data.am&#39; into table beauty partition (nation=&#34;USA&#34;);

select nation, avg(size) from beauties group by nation order by avg(size);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hadoop-shell</title>
      <link>/post/bigdata/hadoop/hadoop-shell/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/hadoop/hadoop-shell/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hadoop-shell&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hdfs&#34;&gt;1. hdfs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hdfs&#34;&gt;1. hdfs&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;列表&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -ls /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -put /test.ee /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看文件内容&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -cat /test.ee&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;下载文件&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -get /test.ee /&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建目录&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop fs -mkdir -p /wordcount/input&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hadoop2.4.1集群搭建</title>
      <link>/post/bigdata/hadoop/hadoop2/</link>
      <pubDate>Sun, 12 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/bigdata/hadoop/hadoop2/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hadoop&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#__strong_centos_strong_安装&#34;&gt;1. &lt;strong&gt;CentOS&lt;/strong&gt; 安装&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_准备linux环境&#34;&gt;1.1. 准备Linux环境&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_安装_strong_jdk_strong&#34;&gt;1.2. 安装 &lt;strong&gt;JDK&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_安装_strong_hadoop2_4_1_strong&#34;&gt;1.3. 安装 &lt;strong&gt;hadoop2.4.1&lt;/strong&gt;&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_将_strong_hadoop_strong_添加到环境变量&#34;&gt;1.3.1. 将 &lt;strong&gt;hadoop&lt;/strong&gt; 添加到环境变量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_格式化_strong_namenode_strong_是对_strong_namenode_strong_进行初始化&#34;&gt;1.3.2. 格式化 &lt;strong&gt;namenode&lt;/strong&gt;（是对 &lt;strong&gt;namenode&lt;/strong&gt; 进行初始化）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_启动_strong_hadoop_strong&#34;&gt;1.3.3. 启动 &lt;strong&gt;hadoop&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_验证是否启动成功&#34;&gt;1.3.4. 验证是否启动成功&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_配置ssh免登陆&#34;&gt;1.4. 配置ssh免登陆&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_docker_方式&#34;&gt;2. docker 方式&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_compose&#34;&gt;2.1. compose&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_example&#34;&gt;3. Example&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_wordcount&#34;&gt;3.1. wordcount&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_hdfs_文件上传流程&#34;&gt;4. hdfs 文件上传流程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_参考&#34;&gt;5. 参考&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;__strong_centos_strong_安装&#34;&gt;1. &lt;strong&gt;CentOS&lt;/strong&gt; 安装&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_准备linux环境&#34;&gt;1.1. 准备Linux环境&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;先将虚拟机的网络模式选为NAT&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改主机名&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vi /etc/sysconfig/network

NETWORKING=yes
HOSTNAME=mini1    ###&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改IP&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/sysconfig/network-scripts/ifcfg-eth0

DEVICE=&#34;eth0&#34;
BOOTPROTO=&#34;static&#34;               ###
HWADDR=&#34;00:0C:29:3C:BF:E7&#34;
IPV6INIT=&#34;yes&#34;
NM_CONTROLLED=&#34;yes&#34;
ONBOOT=&#34;yes&#34;
TYPE=&#34;Ethernet&#34;
UUID=&#34;ce22eeca-ecde-4536-8cc2-ef0dc36d4a8c&#34;
IPADDR=&#34;192.168.1.101&#34;           ###
NETMASK=&#34;255.255.255.0&#34;          ###
GATEWAY=&#34;192.168.1.1&#34;            ###&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改主机名和IP的映射关系&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/hosts

192.168.1.101   itcast&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;关闭防火墙&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#查看防火墙状态
service iptables status
#关闭防火墙
service iptables stop
#查看防火墙开机启动状态
chkconfig iptables --list
#关闭防火墙开机启动
chkconfig iptables off&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;修改sudo&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;su root
vim /etc/sudoers
给hadoop用户添加执行的权限&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装_strong_jdk_strong&#34;&gt;1.2. 安装 &lt;strong&gt;JDK&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;上传 &lt;strong&gt;jdk-7u_65-i585.tar.gz&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;解压 &lt;strong&gt;jdk&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#创建文件夹
mkdir /home/hadoop/app
#解压
tar -zxvf jdk-7u55-linux-i586.tar.gz -C /home/hadoop/app&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将 &lt;strong&gt;java&lt;/strong&gt; 添加到环境变量中&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/profile
#在文件最后添加
export JAVA_HOME=/home/hadoop/app/jdk-7u_65-i585
export PATH=$PATH:$JAVA_HOME/bin

#刷新配置
source /etc/profile&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_安装_strong_hadoop2_4_1_strong&#34;&gt;1.3. 安装 &lt;strong&gt;hadoop2.4.1&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&#34;admonitionblock tip&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&#34;icon&#34;&gt;
&lt;i class=&#34;fa icon-tip&#34; title=&#34;Tip&#34;&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class=&#34;content&#34;&gt;
先上传 hadoop 的安装包到服务器上去 /home/hadoop/&lt;br&gt;
注意：hadoop2.x 的配置文件 $HADOOP_HOME/etc/hadoop&lt;br&gt;
伪分布式需要修改5个配置文件
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;配置 hadoop&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hadoop-env.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim hadoop-env.sh
#第27行
export JAVA_HOME=/usr/java/jdk1.7.0_65&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;core-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://weekend-1206-01:9000&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;!-- 指定hadoop运行时产生文件的存储目录 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/home/hadoop/hadoop-2.4.1/tmp&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;hdfs-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定HDFS副本的数量 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.secondary.http.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;192.168.1.152:50090&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;mapred-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;mv mapred-site.xml.template mapred-site.xml
vim mapred-site.xml
&amp;lt;!-- 指定mr运行在yarn上 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;yarn-site.xml&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;!-- 指定YARN的老大（ResourceManager）的地址 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.resourcemanager.hostname&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;weekend-1206-01&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;!-- reducer获取数据的方式 --&amp;gt;
&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_将_strong_hadoop_strong_添加到环境变量&#34;&gt;1.3.1. 将 &lt;strong&gt;hadoop&lt;/strong&gt; 添加到环境变量&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;vim /etc/proflie

export JAVA_HOME=/usr/java/jdk1.7.0_65
export HADOOP_HOME=/itcast/hadoop-2.4.1
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

source /etc/profile&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_格式化_strong_namenode_strong_是对_strong_namenode_strong_进行初始化&#34;&gt;1.3.2. 格式化 &lt;strong&gt;namenode&lt;/strong&gt;（是对 &lt;strong&gt;namenode&lt;/strong&gt; 进行初始化）&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hdfs namenode -format
或
hadoop namenode -format&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_启动_strong_hadoop_strong&#34;&gt;1.3.3. 启动 &lt;strong&gt;hadoop&lt;/strong&gt;&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 先启动HDFS
sbin/start-dfs.sh

# 再启动YARN
sbin/start-yarn.sh

# 启动 namenode
sbin/hadoop-daemon.sh start namenode

# 启动 dataNode
sbin/hadoop-daemon.sh start datanode&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_验证是否启动成功&#34;&gt;1.3.4. 验证是否启动成功&lt;/h4&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# jps
27408 NameNode
28218 Jps
27643 SecondaryNameNode
28066 NodeManager
27803 ResourceManager
27512 DataNode&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;http://192.168.1.101:50070 （HDFS管理界面）
http://192.168.1.101:8088 （MR管理界面）&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_配置ssh免登陆&#34;&gt;1.4. 配置ssh免登陆&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#生成ssh免登陆密钥
#进入到我的home目录
cd ~/.ssh

ssh-keygen -t rsa （四个回车）
# 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）
# 将公钥拷贝到要免密登陆的目标机器上
ssh-copy-id localhost
# ssh免登陆：
# 生成key:
ssh-keygen
# 复制从A复制到B上:
ssh-copy-id B
# 验证：
ssh localhost/exit，ps -e|grep ssh
ssh A  #在B中执行&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_docker_方式&#34;&gt;2. docker 方式&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_compose&#34;&gt;2.1. compose&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hadoop.yml&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;version: &#39;2&#39;
services:
  mini1:
    image: dishui.io:5000/sequenceiq/hadoop-docker:2.4.1
    container_name: mini1
    environment:
      - &#34;HOSTNAME=mini1&#34;
    ports:
      - &#34;50070:50070&#34;
      - &#34;8088:8088&#34;
    volumes:
      - ./core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml
      - ./yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop.sh:/etc/profile.d/hadoop.sh
    command: /etc/bootstrap.sh -d
    networks:
      - hadoop
  mini2:
    image: dishui.io:5000/sequenceiq/hadoop-docker:2.4.1
    container_name: mini2
    environment:
      - &#34;HOSTNAME=mini1&#34;
    volumes:
      - ./core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml
      - ./yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop.sh:/etc/profile.d/hadoop.sh
    command: /usr/sbin/sshd -d
    networks:
      - hadoop

networks:
  hadoop:
    driver: bridge&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;core-site.xml&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;hdfs://mini1:9000&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
      &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
      &amp;lt;value&amp;gt;/hadoop-2.4.1/tmp&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;ps: &lt;code&gt;cd $HADOOP_PREFIX&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;ulist&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;hadoop.sh&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;export PATH=$HADOOP_PREFIX/bin:HADOOP_PREFIX/sbin:$PATH&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_example&#34;&gt;3. Example&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_wordcount&#34;&gt;3.1. wordcount&lt;/h3&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;input&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;a.txt&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;a
b
abc
ef
efg
abc
ef
h
aakk
ef
h&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上传到 &lt;strong&gt;HDFS&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 目录不存在,创建目录 ( `hadoop fs -mkdir -p /wordcount/input` )
hadoop fs -put /a.txt /wordcount/input&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hdfs_文件上传流程&#34;&gt;4. hdfs 文件上传流程&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/post/bigdata/hadoop/hadoop-upload.svg&#34; alt=&#34;hadoop upload&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_参考&#34;&gt;5. 参考&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kiwenlau/hadoop-cluster-docker&#34;&gt;hadoop-cluster-docker&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>多元函数</title>
      <link>/post/mathematics/xuxiaozhan/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B0/</link>
      <pubDate>Thu, 09 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/mathematics/xuxiaozhan/%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B0/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;多元函数&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_多元函数&#34;&gt;1. 多元函数&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_二元函数&#34;&gt;1.1. 二元函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_二元函数图形&#34;&gt;1.2. 二元函数图形&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_复杂的二元函数&#34;&gt;1.3. 复杂的二元函数&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_多元函数的极限和连续性&#34;&gt;2. 多元函数的极限和连续性&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_一元函数的极限&#34;&gt;2.1. 一元函数的极限&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_二元函数的极限&#34;&gt;2.2. 二元函数的极限&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_多元函数&#34;&gt;1. 多元函数&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_二元函数&#34;&gt;1.1. 二元函数&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_084154.png&#34; alt=&#34;2017 03 09 084154&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_084307.png&#34; alt=&#34;2017 03 09 084307&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_084436.png&#34; alt=&#34;2017 03 09 084436&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_085405.png&#34; alt=&#34;2017 03 09 085405&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_085445.png&#34; alt=&#34;2017 03 09 085445&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_二元函数图形&#34;&gt;1.2. 二元函数图形&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_085648.png&#34; alt=&#34;2017 03 09 085648&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_085710.png&#34; alt=&#34;2017 03 09 085710&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_085736.png&#34; alt=&#34;2017 03 09 085736&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_085810.png&#34; alt=&#34;2017 03 09 085810&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_085829.png&#34; alt=&#34;2017 03 09 085829&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_复杂的二元函数&#34;&gt;1.3. 复杂的二元函数&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_085844.png&#34; alt=&#34;2017 03 09 085844&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_085922.png&#34; alt=&#34;2017 03 09 085922&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_085943.png&#34; alt=&#34;2017 03 09 085943&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_090004.png&#34; alt=&#34;2017 03 09 090004&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_090044.png&#34; alt=&#34;2017 03 09 090044&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_090320.png&#34; alt=&#34;2017 03 09 090320&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_多元函数的极限和连续性&#34;&gt;2. 多元函数的极限和连续性&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_一元函数的极限&#34;&gt;2.1. 一元函数的极限&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_090540.png&#34; alt=&#34;2017 03 09 090540&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_二元函数的极限&#34;&gt;2.2. 二元函数的极限&lt;/h3&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_090750.png&#34; alt=&#34;2017 03 09 090750&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_090911.png&#34; alt=&#34;2017 03 09 090911&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_090935.png&#34; alt=&#34;2017 03 09 090935&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;imageblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;img src=&#34;/src/img/gaoshu/2017-03-09_091037.png&#34; alt=&#34;2017 03 09 091037&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>hugo</title>
      <link>/post/hugo/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/hugo/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;
&lt;div id=&#34;toctitle&#34;&gt;hugo&lt;/div&gt;
&lt;ul class=&#34;sectlevel1&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_hugo&#34;&gt;1. hugo&lt;/a&gt;
&lt;ul class=&#34;sectlevel2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_设置发布目录&#34;&gt;1.1. 设置发布目录&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_部署&#34;&gt;1.2. 部署&lt;/a&gt;
&lt;ul class=&#34;sectlevel3&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#_带项目名&#34;&gt;1.2.1. 带项目名&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_不带项目名&#34;&gt;1.2.2. 不带项目名&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_生成_strong_lunr_strong_索引&#34;&gt;2. 生成 &lt;strong&gt;lunr&lt;/strong&gt; 索引&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#_参考&#34;&gt;3. 参考&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_hugo&#34;&gt;1. hugo&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_设置发布目录&#34;&gt;1.1. 设置发布目录&lt;/h3&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;# 创建 gh-pages 分支
git checkout --orphan gh-pages
#
git worktree add public gh-pages&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect2&#34;&gt;
&lt;h3 id=&#34;_部署&#34;&gt;1.2. 部署&lt;/h3&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_带项目名&#34;&gt;1.2.1. 带项目名&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://dishui.oschina.io/note-hugo/&#34; class=&#34;bare&#34;&gt;http://dishui.oschina.io/note-hugo/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hugo-local.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#!/bin/sh

cd /e/note-hugo/

# 替换域名
echo &#34;替换域名&#34;
git checkout -- themes/mainroad/static/js/app.js content/post/base.adoc config.toml themes/mainroad/static/js/jquery.bigautocomplete.js
sed -i &#39;s/dishui.oschina.io/localhost:1313/g&#39; themes/mainroad/static/js/app.js content/post/base.adoc config.toml themes/mainroad/static/js/jquery.bigautocomplete.js
echo &#34;启动 hugo server&#34;
hugo server&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hugo.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#!/bin/sh

cd /e/note-hugo/

# 还原
echo &#34;恢复域名&#34;
git checkout -- themes/mainroad/static/js/app.js content/post/base.adoc config.toml themes/mainroad/static/js/jquery.bigautocomplete.js

echo &#34;Generating site&#34;
hugo

echo &#34;Updating gh-pages branch&#34;
cd public &amp;amp;&amp;amp; git add --all &amp;amp;&amp;amp; git commit -m &#34;$(date +%Y-%m-%d)&#34; &amp;amp;&amp;amp; git push

echo &#34;Commit note-hugo&#34;
cd .. &amp;amp;&amp;amp; git add --all &amp;amp;&amp;amp; git commit -m &#34;$(date +%Y-%m-%d)&#34; &amp;amp;&amp;amp; git push&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect3&#34;&gt;
&lt;h4 id=&#34;_不带项目名&#34;&gt;1.2.2. 不带项目名&lt;/h4&gt;
&lt;div class=&#34;quoteblock&#34;&gt;
&lt;blockquote&gt;
&lt;div class=&#34;paragraph&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://dishui.oschina.io&#34; class=&#34;bare&#34;&gt;http://dishui.oschina.io&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hugo-local.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#!/bin/sh

cd /e/dishui/

echo &#34;启动 hugo server&#34;
hugo server&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;title&#34;&gt;hugo-oschina.sh&lt;/div&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;#!/bin/sh

cd /e/dishui/

hugo

echo &#34;Updating gh-pages branch&#34;
cd public &amp;amp;&amp;amp; git add --all &amp;amp;&amp;amp; git commit -m &#34;$(date +%Y-%m-%d)&#34; &amp;amp;&amp;amp; git push

echo &#34;Commit note&#34;
cd .. &amp;amp;&amp;amp; git add --all &amp;amp;&amp;amp; git commit -m &#34;$(date +%Y-%m-%d)&#34; &amp;amp;&amp;amp; git push&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_生成_strong_lunr_strong_索引&#34;&gt;2. 生成 &lt;strong&gt;lunr&lt;/strong&gt; 索引&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;启动 &lt;strong&gt;nodejieba&lt;/strong&gt;&lt;br&gt;
启动 &lt;strong&gt;docker-volume-netshare&lt;/strong&gt; ( &lt;a href=&#34;/post/docker/docker-base/#_mount_aws_efs_nfs_or_cifs_samba_volumes_in_docker&#34;&gt;docker-volume-netshare&lt;/a&gt; )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动 &lt;strong&gt;nodejieba&lt;/strong&gt; 容器&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;docker run -it --name=lunr --volume-driver=cifs -v 192.168.137.2/note-hugo:/hugo dishui.io:5000/nodejieba:1.1 /bin/bash&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;构建索引&lt;/p&gt;
&lt;div class=&#34;listingblock&#34;&gt;
&lt;div class=&#34;content&#34;&gt;
&lt;pre class=&#34;highlightjs highlight&#34;&gt;&lt;code&gt;cd /hugo/content &amp;amp;&amp;amp; \
nodejs hugo-lunr.js &amp;amp;&amp;amp; \ # 生成原始数据
nodejs index_builder.js # 生成索引&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;sect1&#34;&gt;
&lt;h2 id=&#34;_参考&#34;&gt;3. 参考&lt;/h2&gt;
&lt;div class=&#34;sectionbody&#34;&gt;
&lt;div class=&#34;olist arabic&#34;&gt;
&lt;ol class=&#34;arabic&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://gohugo.io/tutorials/github-pages-blog/&#34;&gt;Deployment via gh-pages branch&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>